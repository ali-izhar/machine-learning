{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torchtext==0.10.0 --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language='en_core_web_sm', batch_first=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Saeed Ahmad Work\\Github Work\\machine_learning\\mini_projects\\movies_reviews_classification\\.data\\imdb\\aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:05<00:00, 15.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "## import data\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples   : 17500\n",
      "Number of validation examples : 7500\n",
      "Number of testing examples    : 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split(split_ratio=0.7, stratified=True, random_state = random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples   : {len(train_data)}')\n",
    "print(f'Number of validation examples : {len(valid_data)}')\n",
    "print(f'Number of testing examples    : {len(test_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary : 20002\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "[('the', 202689), (',', 192447), ('.', 165048), ('and', 109188), ('a', 109040), ('of', 100559), ('to', 93907), ('is', 75853), ('in', 61324), ('I', 53964), ('it', 53467), ('that', 49110), ('\"', 43651), (\"'s\", 43059), ('this', 42281), ('-', 36976), ('/><br', 35313), ('was', 35289), ('as', 30401), ('with', 29831)]\n",
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(f\"Unique tokens in TEXT vocabulary : {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "print(TEXT.vocab.itos[:10])\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minibatch for training dataset   : 274\n",
      "Number of minibatch for validation dataset : 118\n",
      "Number of minibatch for testing dataset    : 391\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "print('Number of minibatch for training dataset   : {}'.format(len(train_iterator)))\n",
    "print('Number of minibatch for validation dataset : {}'.format(len(valid_iterator)))\n",
    "print('Number of minibatch for testing dataset    : {}'.format(len(test_iterator)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\root\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(20002, 300, padding_idx=0)\n",
       "  (rnn): LSTM(300, 128, batch_first=True, dropout=0.2)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "embedding_dim = 256\n",
    "hidden_units = 128\n",
    "EPOCHS = 50\n",
    "learning_rate = 5e-4\n",
    "\n",
    "\n",
    "# Build NN model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim, embedding_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.linear(output[:, -1, :])\n",
    "        return output\n",
    "  \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "\n",
    "model = LSTM(len(TEXT.vocab), 128, len(LABEL.vocab)-1, 300, 0.2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,220,889 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Optimizer and Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "## change model and criterion to device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "M7. Define Accuracy Function\n",
    "'''\n",
    "def binary_accuracy(preds, target):\n",
    "    '''\n",
    "    from https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
    "    '''\n",
    "    # round predictions to the closest integer (0 or 1)\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "\n",
    "    #convert into float for division\n",
    "    correct = (rounded_preds == target).float()\n",
    "\n",
    "    # rounded_preds = [ 1   0   0   1   1   1   0   1   1   1]\n",
    "    # targets       = [ 1   0   1   1   1   1   0   1   1   0]\n",
    "    # correct       = [1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # We initialize the gradient to 0 for every batch.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # batch of sentences \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        # Calculate the loss value by comparing the prediction result with batch.label \n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        # Accuracy calculation\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        # Backpropagation using backward()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters using the optimization algorithm\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # \"evaluation mode\" : turn off \"dropout\" or \"batch nomalizaation\"\n",
    "    model.eval()\n",
    "\n",
    "    # Use less memory and speed up computation by preventing gradients from being computed in pytorch\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            \n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 26m 45s\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.74%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 49.74%\n",
      "Epoch: 02 | Epoch Time: 25m 42s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.13%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 56.06%\n",
      "Epoch: 03 | Epoch Time: 25m 42s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.46%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 54.88%\n",
      "Epoch: 04 | Epoch Time: 26m 12s\n",
      "\tTrain Loss: 0.692 | Train Acc: 49.86%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 54.36%\n",
      "Epoch: 05 | Epoch Time: 20m 53s\n",
      "\tTrain Loss: 0.689 | Train Acc: 51.09%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.67%\n",
      "Epoch: 06 | Epoch Time: 18m 31s\n",
      "\tTrain Loss: 0.689 | Train Acc: 50.21%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 57.19%\n",
      "Epoch: 07 | Epoch Time: 18m 6s\n",
      "\tTrain Loss: 0.688 | Train Acc: 50.64%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 53.18%\n",
      "Epoch: 08 | Epoch Time: 16m 58s\n",
      "\tTrain Loss: 0.688 | Train Acc: 49.87%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 55.88%\n",
      "Epoch: 09 | Epoch Time: 17m 12s\n",
      "\tTrain Loss: 0.687 | Train Acc: 50.16%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 56.69%\n",
      "Epoch: 10 | Epoch Time: 15m 53s\n",
      "\tTrain Loss: 0.687 | Train Acc: 50.19%\n",
      "\t Val. Loss: 0.709 |  Val. Acc: 56.19%\n",
      "Epoch: 11 | Epoch Time: 17m 39s\n",
      "\tTrain Loss: 0.687 | Train Acc: 50.08%\n",
      "\t Val. Loss: 0.724 |  Val. Acc: 56.10%\n",
      "Epoch: 12 | Epoch Time: 14m 7s\n",
      "\tTrain Loss: 0.677 | Train Acc: 56.03%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 64.14%\n",
      "Epoch: 13 | Epoch Time: 9m 43s\n",
      "\tTrain Loss: 0.649 | Train Acc: 63.30%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 61.09%\n",
      "Epoch: 14 | Epoch Time: 10m 11s\n",
      "\tTrain Loss: 0.601 | Train Acc: 69.93%\n",
      "\t Val. Loss: 0.556 |  Val. Acc: 72.97%\n",
      "Epoch: 15 | Epoch Time: 9m 30s\n",
      "\tTrain Loss: 0.517 | Train Acc: 77.62%\n",
      "\t Val. Loss: 0.531 |  Val. Acc: 76.60%\n",
      "Epoch: 16 | Epoch Time: 9m 4s\n",
      "\tTrain Loss: 0.485 | Train Acc: 79.55%\n",
      "\t Val. Loss: 0.529 |  Val. Acc: 76.82%\n",
      "Epoch: 17 | Epoch Time: 9m 0s\n",
      "\tTrain Loss: 0.429 | Train Acc: 83.24%\n",
      "\t Val. Loss: 0.576 |  Val. Acc: 76.58%\n",
      "Epoch: 18 | Epoch Time: 9m 5s\n",
      "\tTrain Loss: 0.408 | Train Acc: 84.36%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 74.22%\n",
      "Epoch: 19 | Epoch Time: 9m 9s\n",
      "\tTrain Loss: 0.359 | Train Acc: 86.62%\n",
      "\t Val. Loss: 0.434 |  Val. Acc: 83.52%\n",
      "Epoch: 20 | Epoch Time: 8m 57s\n",
      "\tTrain Loss: 0.338 | Train Acc: 87.59%\n",
      "\t Val. Loss: 0.454 |  Val. Acc: 82.05%\n",
      "Epoch: 21 | Epoch Time: 8m 57s\n",
      "\tTrain Loss: 0.308 | Train Acc: 88.90%\n",
      "\t Val. Loss: 0.460 |  Val. Acc: 81.07%\n",
      "Epoch: 22 | Epoch Time: 9m 3s\n",
      "\tTrain Loss: 0.275 | Train Acc: 90.36%\n",
      "\t Val. Loss: 0.463 |  Val. Acc: 82.15%\n",
      "Epoch: 23 | Epoch Time: 9m 6s\n",
      "\tTrain Loss: 0.276 | Train Acc: 90.26%\n",
      "\t Val. Loss: 0.460 |  Val. Acc: 81.29%\n",
      "Epoch: 24 | Epoch Time: 9m 3s\n",
      "\tTrain Loss: 0.240 | Train Acc: 91.78%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 85.52%\n",
      "Epoch: 25 | Epoch Time: 9m 1s\n",
      "\tTrain Loss: 0.225 | Train Acc: 92.49%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 85.75%\n",
      "Epoch: 26 | Epoch Time: 8m 59s\n",
      "\tTrain Loss: 0.206 | Train Acc: 93.12%\n",
      "\t Val. Loss: 0.405 |  Val. Acc: 86.26%\n",
      "Epoch: 27 | Epoch Time: 9m 3s\n",
      "\tTrain Loss: 0.185 | Train Acc: 93.90%\n",
      "\t Val. Loss: 0.420 |  Val. Acc: 84.93%\n",
      "Epoch: 28 | Epoch Time: 9m 11s\n",
      "\tTrain Loss: 0.185 | Train Acc: 93.99%\n",
      "\t Val. Loss: 0.396 |  Val. Acc: 86.98%\n",
      "Epoch: 29 | Epoch Time: 9m 1s\n",
      "\tTrain Loss: 0.169 | Train Acc: 94.70%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 86.65%\n",
      "Epoch: 30 | Epoch Time: 9m 3s\n",
      "\tTrain Loss: 0.166 | Train Acc: 94.80%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 86.48%\n",
      "Epoch: 31 | Epoch Time: 9m 7s\n",
      "\tTrain Loss: 0.142 | Train Acc: 95.62%\n",
      "\t Val. Loss: 0.388 |  Val. Acc: 86.98%\n",
      "Epoch: 32 | Epoch Time: 9m 2s\n",
      "\tTrain Loss: 0.132 | Train Acc: 96.04%\n",
      "\t Val. Loss: 0.401 |  Val. Acc: 87.46%\n",
      "Epoch: 33 | Epoch Time: 9m 9s\n",
      "\tTrain Loss: 0.129 | Train Acc: 96.24%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 87.14%\n",
      "Epoch: 34 | Epoch Time: 9m 3s\n",
      "\tTrain Loss: 0.119 | Train Acc: 96.48%\n",
      "\t Val. Loss: 0.419 |  Val. Acc: 86.75%\n",
      "Epoch: 35 | Epoch Time: 9m 4s\n",
      "\tTrain Loss: 0.106 | Train Acc: 96.90%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 87.40%\n",
      "Epoch: 36 | Epoch Time: 9m 4s\n",
      "\tTrain Loss: 0.110 | Train Acc: 96.80%\n",
      "\t Val. Loss: 0.406 |  Val. Acc: 87.14%\n",
      "Epoch: 37 | Epoch Time: 9m 10s\n",
      "\tTrain Loss: 0.094 | Train Acc: 97.37%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 87.51%\n",
      "Epoch: 38 | Epoch Time: 9m 18s\n",
      "\tTrain Loss: 0.081 | Train Acc: 97.79%\n",
      "\t Val. Loss: 0.523 |  Val. Acc: 85.50%\n",
      "Epoch: 39 | Epoch Time: 9m 3s\n",
      "\tTrain Loss: 0.085 | Train Acc: 97.66%\n",
      "\t Val. Loss: 0.449 |  Val. Acc: 87.12%\n",
      "Epoch: 40 | Epoch Time: 9m 10s\n",
      "\tTrain Loss: 0.156 | Train Acc: 95.27%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 86.14%\n",
      "Epoch: 41 | Epoch Time: 9m 4s\n",
      "\tTrain Loss: 0.089 | Train Acc: 97.68%\n",
      "\t Val. Loss: 0.429 |  Val. Acc: 86.63%\n",
      "Epoch: 42 | Epoch Time: 9m 7s\n",
      "\tTrain Loss: 0.077 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.426 |  Val. Acc: 87.24%\n",
      "Epoch: 43 | Epoch Time: 9m 7s\n",
      "\tTrain Loss: 0.072 | Train Acc: 98.12%\n",
      "\t Val. Loss: 0.449 |  Val. Acc: 86.75%\n",
      "Epoch: 44 | Epoch Time: 8m 55s\n",
      "\tTrain Loss: 0.067 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.472 |  Val. Acc: 86.77%\n",
      "Epoch: 45 | Epoch Time: 9m 10s\n",
      "\tTrain Loss: 0.065 | Train Acc: 98.31%\n",
      "\t Val. Loss: 0.461 |  Val. Acc: 87.47%\n",
      "Epoch: 46 | Epoch Time: 9m 7s\n",
      "\tTrain Loss: 0.070 | Train Acc: 98.19%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 87.44%\n",
      "Epoch: 47 | Epoch Time: 9m 19s\n",
      "\tTrain Loss: 0.067 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 87.30%\n",
      "Epoch: 48 | Epoch Time: 9m 7s\n",
      "\tTrain Loss: 0.055 | Train Acc: 98.60%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 87.63%\n",
      "Epoch: 49 | Epoch Time: 9m 2s\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.64%\n",
      "\t Val. Loss: 0.571 |  Val. Acc: 77.62%\n",
      "Epoch: 50 | Epoch Time: 9m 5s\n",
      "\tTrain Loss: 0.124 | Train Acc: 96.20%\n",
      "\t Val. Loss: 0.491 |  Val. Acc: 85.69%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "'''\n",
    "Episode / each step Process\n",
    "'''\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), os.path.join('.models', f'model_{epoch}.pt'))\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.424 | Test Acc: 84.14%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join('.models', 'model_23.pt')))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This film is terrible\n",
      ">>>This is a negative review.( 0.43)\n",
      "\n",
      " This film is great\n",
      ">>>This is a positive review. ( 0.93)\n",
      "\n",
      " This movie is fantastic\n",
      ">>>This is a positive review. ( 0.86)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training result test for Code Engineering\n",
    "'''\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n",
    "\n",
    "examples = [\n",
    "  \"This film is terrible\",\n",
    "  \"This film is great\",\n",
    "  \"This movie is fantastic\"\n",
    "]\n",
    "\n",
    "for idx in range(len(examples)) :\n",
    "\n",
    "    sentence = examples[idx]\n",
    "    pred = predict_sentiment(model,sentence)\n",
    "    print(\"\\n\",sentence)\n",
    "    if pred >= 0.5 :\n",
    "        print(f\">>>This is a positive review. ({pred : .2f})\")\n",
    "    else:\n",
    "        print(f\">>>This is a negative review.({pred : .2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
