{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31346ec",
   "metadata": {},
   "source": [
    "# Building GPT\n",
    "\n",
    "> Recreating [nanoGPT](https://github.com/karpathy/nanoGPT) from Andrej Karpathy\n",
    "\n",
    "<font color=\"purple\">We'll train a character-level GPT on the works of Shakespeare.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aaca1e",
   "metadata": {},
   "source": [
    "# 1 - Shakespeare Dataset\n",
    "\n",
    "<hr>\n",
    "\n",
    "This section deals with preparing the dataset that will be used for training our gpt model. The dataset is the \"tiny shakespeare dataset,\" a relatively small file of about 1MB, containing approximately ~ 1.1M characters from various works of William Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f2678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open('data/tinyshakespeare.txt', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the file. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e736ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "--------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "with open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "print('-'*50)\n",
    "print(text[:500]) # let's look at the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd113c5",
   "metadata": {},
   "source": [
    "### 1.1 - Unique Characters\n",
    "\n",
    "To preprocess this text for a machine learning model, it's crucial to understand the unique characters present in the dataset because these characters will form the vocabulary that our model will learn to generate text.\n",
    "\n",
    "- `set(text)` converts the text into a set, thereby removing any duplicate characters\n",
    "- `list(set)` then converts this set into a list, which doesn't have a specific order\n",
    "- `sorted(list)` sorts this list in a standard alphabetical order, making it easier to index each character\n",
    "- `vocab_size` refers to the total count of these unique characters, which is crucial for defining the input layer of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff155a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars)) # note the space character at the start\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186dce3c",
   "metadata": {},
   "source": [
    "## 2 - Tokenization\n",
    "\n",
    "<hr>\n",
    "\n",
    "Tokenization is the process of converting text into a format that can be understood by machine learning models. This involves breaking down text into \"tokens,\" which can represent individual characters, sub-words, or entire words.\n",
    "\n",
    "\n",
    "### 2.1 - Character-level vs. Sub-word Tokenization\n",
    "\n",
    "- **Character-level Encoding:** This is a simple form of tokenization where each unique character in the text is assigned a unique integer. This method is particularly useful for languages with a small set of characters (like English) or for applications like character-level language models where the nuances of individual characters are important.\n",
    "    - For the Shakespeare dataset, a character-level tokenizer would encode each of the ~65 unique characters into a unique integer.\n",
    "\n",
    "- **Sub-word Tokenization:** This method breaks text into pieces that are larger than individual characters but smaller than entire words. It strikes a balance by reducing the vocabulary size compared to character-level encoding without losing the granularity of meaning that word-level encoding might miss.\n",
    "    - Tokenizers like OpenAI's `tiktoken` and Google's `SentencePiece` are examples of sub-word tokenization systems. They analyze the corpus to find the optimal way to break down words into sub-word units.\n",
    "    - This approach is beneficial when dealing with large vocabularies or languages with complex morphology.\n",
    "\n",
    "For instance, the phrase \"hi there\" (8 characters) could be encoded into:\n",
    "- 8 integers (character-level encoding)\n",
    "- 2 integers (word-level encoding)\n",
    "- 3 integers (sub-word encoding)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example of Tokenization with tiktoken**\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.n_vocab)\n",
    "# Output: 50257 (the size of the vocabulary)\n",
    "print(enc.encode(\"hi there\"))\n",
    "# Output: [71, 4178, 612] (sub-word level encoding)\n",
    "print(enc.decode([71, 4178, 612]))\n",
    "# Output: 'hi there' (decoding back to text)\n",
    "```\n",
    "\n",
    "- This example shows how `tiktoken` with a GPT-2 encoding model breaks down \"hi there\" into three integers, each representing a sub-word unit. This is compared to what would have been eight integers for character-level encoding or two for word-level encoding.\n",
    "\n",
    "- The `n_vocab` value of 50257 indicates the size of the vocabulary that `tiktoken` uses, which is significantly larger than the vocabulary size for character-level encoding in our Shakespeare dataset example. This large vocabulary allows for encoding a vast array of words and sub-words into a relatively small number of integers, making the model more efficient in processing and generating text.\n",
    "\n",
    "\n",
    "In summary, you can have:\n",
    "- A large sequence of integers with a very small vocabulary\n",
    "- A short sequence of integers with a very large vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc22ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]          # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747270c8",
   "metadata": {},
   "source": [
    "### 2.2 - Encoding Data into a Torch Tensor\n",
    "\n",
    "Now that we've our character-level tokenizer, encode the entire Shakespeare dataset into a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3901a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50907303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b1000a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "--------------------------------------------------\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)\n",
    "print('-'*50)\n",
    "print(data[:500]) # the 500 characters we looked at earier will look like this to the GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1120c",
   "metadata": {},
   "source": [
    "## 3 - Train/Val Split & Data Loader\n",
    "\n",
    "<hr>\n",
    "\n",
    "This section focuses on preparing the dataset for training by splitting it into training and validation sets and then setting up a data loader to feed data into the model efficiently.\n",
    "\n",
    "**Train/Val Split:**\n",
    "\n",
    "The dataset is divided into two parts: 90% for training and 10% for validation. This split allows the model to learn from the majority of the data while also having a separate set to evaluate its performance on unseen data. Here's how the split is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83aac1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # Calculate the splitting point\n",
    "train_data = data[:n]     # Assign the first 90% of data to training\n",
    "val_data = data[n:]       # Assign the remaining 10% to validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9bced",
   "metadata": {},
   "source": [
    "### 3.1 - Data Loader\n",
    "\n",
    "Due to computational constraints, data is not fed into the model all at once. Instead, it's broken into manageable chunks that can be processed efficiently.\n",
    "\n",
    "\n",
    "#### 3.1.1 - Block Size / Context Length\n",
    "\n",
    "- The `block_size` or `context_length` sets the maximum length for these data chunks, influencing how much context the model considers when making predictions.\n",
    "- Each chunk results in several input-output pairs, where each input (except the last one) is paired with the next character/token as the target output. For a block size of 9, there would be 8 such pairs.\n",
    "\n",
    "\n",
    "#### 3.1.2 - Dimensions in Training\n",
    "\n",
    "- **Time Dimension:** This represents the sequence of tokens in the text, showing the progression over time.\n",
    "- **Batch Dimension:** Data is organized in batches to allow parallel processing, enhancing efficiency.\n",
    "\n",
    "**Example of Creating Training Pairs:**\n",
    "\n",
    "Given a tensor representing a chunk of text:  \n",
    "\n",
    "```python\n",
    "tensor([18, 47, 56, 57, 58, 1, 15, 47, 58])\n",
    "```\n",
    "\n",
    "This tensor can generate training pairs where each context is paired with the next token as the target, simulating real-time prediction as the model processes the text.\n",
    "\n",
    "- in the context of [18], 47 comes next.\n",
    "- in the context of [18, 47], 56 comes next.\n",
    "- in the context of [18, 47, 56], 57 comes next, and so on.\n",
    "\n",
    "**Why do we do this?**\n",
    "\n",
    "- This approach ensures that the transformer is exposed to contexts ranging from very small (a single integer) to the full length of the `block_size`, allowing it to learn and understand text in varying lengths effectively.\n",
    "- If the input text exceeds the set `block_size`, the transformer model truncates the excess, focusing only on the text within the defined limit. This process ensures computational efficiency and relevance in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf1baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8bc6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]      # inputs to the transformer: first block_size characters\n",
    "y = train_data[1:block_size+1]   # targets for each input position: off-set by 1\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]            # all chars up to and including t\n",
    "    target = y[t]                # t-th char in the y array\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91c033",
   "metadata": {},
   "source": [
    "### 3.2 - Data Batches\n",
    "\n",
    "The data loader creates batches of data for training or validation. The batch size and block size are key parameters:\n",
    "\n",
    "- `batch_size`: Determines how many sequences are processed in parallel.\n",
    "- `block_size`: Limits the maximum context length the model will use for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb9e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4            # how many independent sequences will we process in parallel?\n",
    "block_size = 8            # what is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a768985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"This function generates batches by randomly selecting starting points in the data, \n",
    "    then extracting sequences of length block_size for inputs and their corresponding next \n",
    "    tokens as targets.\"\"\"\n",
    "    \n",
    "    # Choose the dataset based on the split\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Randomly select starting points for each sequence in the batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Extract sequences of length block_size for inputs (x)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # Extract the next token for each input as targets (y)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ca56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37b218",
   "metadata": {},
   "source": [
    "Here, `xb` is a single batch of 4x8 or 32 independent examples sampled from the training dataset and `yb` contains the corresponding target labels (for loss computations later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad248c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "--------------------------------------------------\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "--------------------------------------------------\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('-'*50)\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('-'*50)\n",
    "\n",
    "for b in range(batch_size):     # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae841e",
   "metadata": {},
   "source": [
    "## 4 - Bigram Language Model\n",
    "\n",
    "<hr>\n",
    "\n",
    "The bigram language model is the simplest form of neural network for language modeling. It predicts the next token in a sequence based solely on the current token, without considering any broader context. This model is a foundational concept in language modeling, demonstrating the basic principle of predicting subsequent elements in a sequence.\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "- Vocabulary Size (`vocab_size`): It defines the size of the model's input and output layers. Each unique token in our dataset contributes to the total `vocab_size`.\n",
    "- Embedding Table (`self.token_embedding_table`): Maps each token to a vector of logits representing the probabilities of subsequent tokens. This table has dimensions `[vocab_size, vocab_size]`, enabling each token to have a distinct probability distribution over the next token.\n",
    "\n",
    "<br>\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"images/embedding_matrix.png\" width=400>\n",
    "    <center><caption><font color=\"purple\"><strong><u>Figure 1:</u></strong> Example of a possible embedding matrix</font></caption></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50dbaa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f95aa2ced0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724c7a0",
   "metadata": {},
   "source": [
    "In this model, `idx` and `targets` are tensors of integers representing sequences of tokens. The model directly retrieves a vector of `logits` for the next token based on the current token's index, without accounting for any wider context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e16b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Initialize an embedding table with dimensions [vocab_size, vocab_size]\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        # idx: Input tokens\n",
    "        # targets: Expected subsequent tokens\n",
    "        # Returns logits with shape (B, T, C) indicating token predictions\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b7cc05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb, yb)\n",
    "print(out.shape) # (B,T,C) = (4,8,65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4049d6f",
   "metadata": {},
   "source": [
    "### 4.1 - Loss Calculation\n",
    "\n",
    "To evaluate the model's predictions, we utilize the negative log likelihood, effectively implemented as `cross-entropy` loss in PyTorch. This metric compares the predicted probabilities (logits) with the actual subsequent tokens (targets) to quantify the model's performance.\n",
    "\n",
    "So we'd write something like this:\n",
    "\n",
    "```python\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "However, this will give us an error because torch expects the channels dimension to be the second dimension, i.e., instead of (B, T, C), we want (B, C, T).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Adjusting Tensor Dimensions:**\n",
    "\n",
    "PyTorch expects the logits tensor to be in the shape of (B, C, T) for computing cross-entropy loss. Since our model outputs logits in the shape (B, T, C), we need to reshape them along with the targets tensor to align with PyTorch's requirements.\n",
    "\n",
    "This adjustment flattens the batch and time dimensions, ensuring each prediction is paired with its corresponding target in a one-dimensional array, i.e., (B, T, C) into (B\\*T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e35a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C) # 2D array\n",
    "        targets = targets.view(B*T)  # 2D array\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0f22b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6630, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape) # (B*T, C) = (4*8, 65) = (32, 65)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defc338",
   "metadata": {},
   "source": [
    "We got a loss of 4.6630. Notice that the model is randomly predicting the next token based off of the current token, and since we've 65 tokens in total, the expected loss would be $-ln\\left(\\frac{1}{65}\\right) \\approx 4.1217$ but we're getting 4.49 which is telling us that the initial predictions are not super diffused - they've got a little bit of entropy - and so we're guessing wrong.\n",
    "\n",
    "### 4.2 - Token Generation\n",
    "\n",
    "The process of token generation in a bigram language model is intriguing because it allows the model to produce new text sequences based on a given context. This model, despite its simplicity, can generate sequences one token at a time by leveraging the learned distribution over the vocabulary.\n",
    "\n",
    "**Prediction Process:** In each iteration of token generation, the model:\n",
    "- Predicts logits based on the current sequence.\n",
    "- Focuses on the logits corresponding to the last predicted token.\n",
    "- Applies softmax to these logits to obtain a probability distribution.\n",
    "- Samples a new token from this distribution.\n",
    "- Concatenates the newly predicted token to the existing sequence.\n",
    "\n",
    "This process iterates `max_new_tokens` times, progressively building a longer text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da2bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        targets has to be None by default because in the generate function below, \n",
    "        we call the forward method without targets `self(idx)` \n",
    "        \"\"\"\n",
    "\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Predict the next token's logits given the current sequence\n",
    "            logits, _ = self(idx)  # No need for targets during generation\n",
    "            logits = logits[:, -1, :]  # Focus on the last predicted token's logits\n",
    "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities: (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample a new token: (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # Append the new token to the sequence: (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba4859",
   "metadata": {},
   "source": [
    "**Generating New Tokens:**\n",
    "\n",
    "To initiate token generation, start with a minimal context, typically a single token that the model can \"understand\" as a starting point. For example, using a newline character (often encoded as 0 in many datasets) is a common choice for initiating text generation as it's akin to starting a new sentence or line.\n",
    "\n",
    "```python\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)  # Starting token\n",
    "```\n",
    "\n",
    "Following this initialization, we can call the `generate` function to extend this sequence by a specified number of tokens `(max_new_tokens)`, in this case, 100 tokens.\n",
    "\n",
    "\n",
    "**Converting Tokens to Text:**\n",
    "\n",
    "After generating a sequence of tokens, the final step involves converting these numeric tokens back into human-readable text. This requires a decoding function that maps each token ID back to its corresponding character or word in the vocabulary.\n",
    "\n",
    "```python\n",
    "generated_sequence = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
    "\n",
    "# unplug single batch dimension to retrieve time steps as python list\n",
    "decoded_text = decode(generated_sequence[0].tolist())  \n",
    "print(decoded_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c7b779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8430, grad_fn=<NllLossBackward0>)\n",
      "--------------------------------------------------\n",
      "Here's some generated text:\n",
      "\n",
      "\n",
      "eKugsuRNC!T3b,jqDNMhsHAJSOWYvkZlA'wjtw3IzUltSG:rX;UOIp:RQ!:KU\n",
      "eRyE-\n",
      "QZtjcOaCx qUOM.pq?kTTtjACpKJ.EHB\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "print('-'*50)\n",
    "print(\"Here's some generated text:\\n\")\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe37790",
   "metadata": {},
   "source": [
    "Initially, the model's predictions are based on a randomly initialized embedding table, leading to nonsensical text. The goal of training the model is to refine these embeddings so that the model can learn meaningful patterns and dependencies between tokens, thereby improving the quality of text generation.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "- The bigram model, in its current form, uses all previous tokens to predict the next one but only leverages the immediately preceding token due to its design. This results in inefficient computation and is not ideal for a simple bigram approach.\n",
    "- The decision to feed the entire sequence into the model, even though only the last token is used for prediction, is made with future scalability in mind. As our model evolves to consider more context, this approach will allow for seamless integration of broader contextual understanding without significant architectural changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20631e66",
   "metadata": {},
   "source": [
    "### 4.3 - Model Training\n",
    "\n",
    "Training the model involves adjusting the embeddings in the `token_embedding_table` to minimize prediction errors. This process helps the model learn the probability distribution of the dataset, enabling it to make more accurate predictions based on the preceding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07a2c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5270330905914307\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # Zero-out all gradients from previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Getting gradients from all parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters using gradients\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4fd43a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shyecer, INoosider hengh frn, obe rewougRxime.\n",
      "I.\n",
      "ARe st'LBEx uin, thertr bl.\n",
      "Cave OMESinod INROMBut.\n",
      "CONINGeves, having, Bulouar t ma, s s meero t kevy ero wonoud Bu is h l,\n",
      "Whaimed sounddes, myor l\n",
      "LAndease Wht ppa thareshot,\n",
      "WeLfthen?\n",
      "Hur\n",
      "Owism. tit eisee e Whentan myo stheerive fand a ts t.\n",
      "Thooryothan\n",
      "ATI'd il,\n",
      "\n",
      "TI:\n",
      "\n",
      "CAROnonghisil aun,\n",
      "tou, y.\n",
      "\n",
      "\n",
      "tthe; re, FOr hen fe,\n",
      "BEO,'tore ssentonknthes; \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87065a4",
   "metadata": {},
   "source": [
    "Although not Shakespearean, we observe an improvement in loss, raising hopes for more reasonable outcomes. This basic model operates on individual tokens without interaction. Moving forward, the aim is to enhance this simplicity by enabling tokens to communicate and consider context beyond just the preceding character. By creating an inter-token communication, the model can better predict subsequent elements, marking the transition towards implementing a transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6803852",
   "metadata": {},
   "source": [
    "## 5 - Self Attention\n",
    "\n",
    "<hr>\n",
    "\n",
    "Before diving into self-attention, here is a mathematical trick that's super useful in understanding the concept of attention.\n",
    "\n",
    "Imagine a data structure of dimensions (B, T, C) where B, T, and C represent batches, time steps, and channels, respectively. In this scenario, each time step (or token) within a batch holds information but initially doesn't interact with others.\n",
    "\n",
    "The goal is to enable these tokens to \"communicate\" with each other, specifically allowing a token to be influenced by tokens from previous time steps while ignoring future ones (preventing data leak from future). The simplest form of such communication could be averaging the information (channels) of all preceding tokens, including the current one, to create a feature vector that captures the essence of a token in the context of its history.\n",
    "\n",
    "This averaging method, while straightforward, is a rudimentary way to enable interaction among tokens, as it might lose detailed spatial information. However, it sets the stage for more sophisticated mechanisms, like self-attention in transformers, which refine and enhance this concept of inter-token communication, ensuring that a token can effectively integrate and leverage past information to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "636588c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Assume x is our input matrix of shape (B, T, C)\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64226f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1: here is an inefficient double for-loop implementation of the idea\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # row-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c01732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "324d0463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac0ef3",
   "metadata": {},
   "source": [
    "Notice that the first row of `x[0]` and `xbow[0]` are the same because the average of a single row is the row itself. The second row of `xbow[0]` is the average of the first two rows of `x[0]`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "202ecc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "----------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# VERSION 2: efficient implementation using matrix multiplication\n",
    "\n",
    "# consider the following matrix multiplication\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-'*10)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-'*10)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea132439",
   "metadata": {},
   "source": [
    "We want the $i^{\\text{th}}$ row of matrix C to be the average of rows from 0 to $i$ in matrix B. Pytorch has a function called `tril` which returns a lower triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f39dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "----------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-'*10)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-'*10)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3b3fd",
   "metadata": {},
   "source": [
    "Notice that the $i^{\\text{th}}$ row in matrix C is the sum of rows 0 to $i$ in matrix B. Since we achieved the addition, now we can do the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f028828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "----------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)   # sum across columns\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('-'*10)\n",
    "print('b=')\n",
    "print(b)\n",
    "print('-'*10)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793eada",
   "metadata": {},
   "source": [
    "Here, the $i^{\\text{th}}$ row of matrix C is the average of 0 to $i$ rows in matrix B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a931afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VERSION 2: using matrix multiply for a weighted aggregation\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2, atol=1e-6, rtol=1e-4)  # tolerance to resist floating point arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fdb0356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c170471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VERSION 3: using Softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "\n",
    "# masked fill replaces zeros with -inf in the wei matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "027de40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax normalizes wei across rows\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00189b1",
   "metadata": {},
   "source": [
    "This is the same exact matrix: the $i^{\\text{th}}$ row is the average of rows from 0 to $i$. \n",
    "\n",
    "**Why is this the case?**\n",
    "\n",
    "When we apply softmax, the 0 entries become $e^{0}=1$ and the $e^{-\\infty} = 0$. Therefore, the software operation translates the matrix into a normalized lower-triangular matrix. Interesting!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51b652b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3, atol=1e-6, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b47259db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe2215",
   "metadata": {},
   "source": [
    "<font color=\"purple\">Why is the softmax method (verion 3) important?</font>\n",
    "\n",
    "This method introduces a foundational concept for understanding self-attention mechanisms in neural networks. \n",
    "- It begins with assigning initial zero weights to token interactions, representing the strength or affinity between tokens. \n",
    "- The use of masking ensures that future tokens do not influence past tokens, maintaining the chronological integrity of the sequence. \n",
    "- The application of softmax normalizes these weights, allowing for a contextual aggregation that considers the relevance of each token up to the current point. \n",
    "- By multiplying the normalized weights with the data $(x)$, it computes affinities that reflect how tokens influence each other based on their positions. \n",
    "\n",
    "However, this method only averages these interactions; the goal is to evolve these interactions into data-dependent relationships. This is achieved through the use of **keys, queries, and values,** which enable the model to dynamically adjust affinities based on the content and context of the tokens, moving beyond simple averaging to a more nuanced, content-aware interaction mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbf848",
   "metadata": {},
   "source": [
    "### 5.1 - Key, Query, Value in Self-Attention\n",
    "\n",
    "We don't want all the affinities to be uniform because some tokens will find others more or less interesting, so we want the affinities to be data-dependent. For example, if I'm a vowel, then maybe I'm looking for consonants in my past and maybe I want to know what those consonants are and I want that information to flow to me. So, I want to now gather information from my past, but I want to do it in a data-dependent way - which is exactly the problem that self-attention solves in the following manner.\n",
    "\n",
    "Every node, or every single token, at each position will emit two vectors: the query $(Q)$ and the key $(K)$ vector. The query vector, roughly speaking, is what am I looking for; and the key vector, roughly speaking, is what do I contain. And then the way we get affinities between these tokens in a sequence is a dot product between the queries and keys. Intuitively, if a query matches a key, the product product will be high and thus the two tokens are more likely to have a higher affinity.\n",
    "\n",
    "---\n",
    "\n",
    "- **Key (K):** Each token generates a Key vector that represents what information it holds. This vector can be thought of as the token's \"identity\" in the context of the sequence.\n",
    "\n",
    "- **Query (Q):** Each token also generates a Query vector that represents what information it is seeking from other tokens. This vector signifies the \"question\" or the type of information the token is looking to gather from its context.\n",
    "\n",
    "- **Value (V):** The Value vector represents the actual information that a token can provide to others. This is what will be communicated if the token is deemed relevant by another token's query.\n",
    "\n",
    "The affinity or relevance between two tokens is computed using the dot product of their Query and Key vectors. A higher dot product indicates a stronger relevance or affinity, suggesting that the information held by the tokens is closely related or important to each other.\n",
    "\n",
    "---\n",
    "\n",
    "In the provided code, a simple self-attention mechanism (single-head attention) is implemented, showcasing how Keys, Queries, and Values are generated from the input sequence $x$ using linear transformations.\n",
    "\n",
    "- `head_size`: The head size in self-attention mechanisms refers to the dimensionality of the Key, Query, and Value vectors, determining the size of the subspace they occupy for computing affinities between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3243d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False) # matrix mult with fixed weights, therefore no bias\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # (B, T, 16)\n",
    "\n",
    "# Compute affinities between tokens\n",
    "# Transpose only the last two dimensions and not the batch dimension\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# Mask future tokens to prevent information leakage\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Normalize affinities to probabilities\n",
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff5bbac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18d3f605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27e1d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate information based on computed affinities\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56e41fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4a983",
   "metadata": {},
   "source": [
    "So, you can think of $x$ as kind of like a private information to a token. If I'm a fifth token and I have some identity and my information is kept in vector $X$ and now for the purposes of attention, what i'm interested in is $(Q)$, what I have is $(K)$. If you find me interesting, then what I will communicate to you is $(V)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2308c21",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "```python\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# remove this line to enable future nodes to talk to past nodes as well\n",
    "```\n",
    "\n",
    "- **self-attention** just means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from $x$, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- **Scaled attention** additional divides `wei` by $\\frac{1}{\\sqrt{\\text{head_size}}}$. This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffused and not saturate too much.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V$$\n",
    "\n",
    "The only thing we're missing is the division by $\\sqrt{d_k}$ where $d_k$ is the head size. Why do we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6d2ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naively multiplying keys and queries\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad4dfc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0449)\n",
      "tensor(1.0700)\n",
      "tensor(17.4690)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7602d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale by head_size\n",
    "\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ff1c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9006)\n",
      "tensor(1.0037)\n",
      "tensor(0.9957)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dac13f",
   "metadata": {},
   "source": [
    "Scaling the variance for `wei` in self-attention is crucial because it ensures the initial distribution of affinities is sufficiently diffused. Since `wei` is inputted into softmax, extremely high or low values would lead softmax to produce nearly one-hot vectors, overly concentrating on a single token and neglecting the rest. By controlling the variance, particularly at initialization, we prevent softmax from becoming too peaky, fostering a more balanced and effective aggregation of information across multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d77b845d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "449a9012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6825cfe",
   "metadata": {},
   "source": [
    "### 5.2 - Multi-Head Attention\n",
    "\n",
    "Multi-head attention enhances single-head attention by running it in parallel across multiple \"heads,\" each focusing on different aspects of the information, enabling a more comprehensive and nuanced understanding of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04a466",
   "metadata": {},
   "source": [
    "### 5.3 - Feed Forward Layer\n",
    "\n",
    "The feed-forward layer in the Transformer architecture, positioned right after multi-head attention, acts as a fully connected neural network (MLP) that processes each token independently. It introduces additional computational depth, allowing each token to further analyze and integrate the information gathered from the attention mechanism, enhancing the model's ability to make more informed predictions.\n",
    "\n",
    "> While computing logits for the tokens, we went too fast: the tokens looked at each other but didn't really have a lot of time to think on what they found from the other tokens; the MLP enables the tokens or nodes to think further on the data they've collected so far from the attention mechanism before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b53bee",
   "metadata": {},
   "source": [
    "### 5.4 - Layer Norm\n",
    "\n",
    "Layer normalization, as opposed to batch normalization, normalizes the inputs across the features for each data point in a batch. It is designed for stabilizing and accelerating the training of deep neural networks. This normalization is done by subtracting the mean and dividing by the standard deviation of the features, then scaling and shifting the result with learnable parameters, gamma and beta. Layer normalization is particularly useful in sequence models like Transformers, where it is applied at each sub-layer of the model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79addf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps  # A small value added for numerical stability during division\n",
    "        self.gamma = torch.ones(dim)  # Learnable scale parameters\n",
    "        self.beta = torch.zeros(dim)  # Learnable shift parameters\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Perform normalization for each input\n",
    "        xmean = x.mean(1, keepdim=True) # Calculate the mean of each input\n",
    "        xvar = x.var(1, keepdim=True) # Calculate the variance of each input\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # Normalize inputs to have zero mean and unit variance\n",
    "        self.out = self.gamma * xhat + self.beta # Scale and shift normalized inputs\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Return the learnable parameters of the layer\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d1e7f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69c83110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean, std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da140acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean, std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06fdc17",
   "metadata": {},
   "source": [
    "## 6 - Full Code Review\n",
    "\n",
    "<hr>\n",
    "\n",
    "Using all of this knowledge, we implement below a \"decoder-only\" transformer model adapted from the \"Attention Is All You Need\" paper. \n",
    "\n",
    "**Key components:** \n",
    "- multi-head self-attention\n",
    "- position embeddings\n",
    "- feed-forward layers\n",
    "- layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7b477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a833f61",
   "metadata": {},
   "source": [
    "We want to move beyond the straightforward approach where token embeddings directly produce logits (raw preditions or scores for each vocabulary token). Instead, we want to introduce an intermediate layer or \"level of abstraction\" between the token embeddings and the logits.\n",
    "\n",
    "- **Embedding Dimension (`n_embed`):** : Specifies the size of the embedding vectors for each token and position within the sequence. By increasing the number of dimensions, we provide a richer, more nuanced representation of each token's features and positional information, creating a more detailed \"space\" where relationships between tokens can be learned and exploited by the model\n",
    "- **Number of Layers (`n_layer`):** Determines the depth of the model by specifying how many transformer blocks or layers are stacked. Each layer includes mechanisms like self-attention and feed-forward networks, allowing the model to process information at multiple levels of abstraction. More layers enable the model to capture more complex dependencies and relationships within the data, improving its ability to understand context and generate coherent text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f233ca1",
   "metadata": {},
   "source": [
    "### 6.1 - Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "529701c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64         # how many independent sequences will we process in parallel?\n",
    "block_size = 256        # what is the maximum context length for predictions?\n",
    "n_embd = 384            # size of the embedding dimension (features + positional encoding)\n",
    "n_head = 6              # number of attention heads\n",
    "n_layer = 6             # number of transformer layers\n",
    "dropout = 0.2           # dropout rate\n",
    "\n",
    "max_iters = 5000        # how many training iterations\n",
    "eval_interval = 500     # how often to evaluate the model\n",
    "learning_rate = 3e-4    # learning rate\n",
    "eval_iters = 200        # how many iterations to average the loss over\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9357494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f95aa2ced0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22477341",
   "metadata": {},
   "source": [
    "### 6.2 - Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc9b3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text))) # unique characters\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}      # char : int mapping\n",
    "itos = {i: ch for i, ch in enumerate(chars)}      # int : char mapping\n",
    "encode = lambda s: [stoi[c] for c in s]           # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f117f",
   "metadata": {},
   "source": [
    "### 6.3 - Split Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3cc0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # 90-10 train/test split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d143880",
   "metadata": {},
   "source": [
    "### 6.4 - Data Loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6635f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddbd0b3",
   "metadata": {},
   "source": [
    "### 6.5 - Estimate Model Loss for Evaluation\n",
    "\n",
    "The context manager `torch.no_grad()` is used to prevent the computation graph from storing the gradients of the operations inside of it. This is to tell pytorch that we will not call `loss.backward()` and hence it does not need to store the gradients of the operations inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20fb64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d02f8",
   "metadata": {},
   "source": [
    "### 6.6 - Self-Attention Head\n",
    "\n",
    "`tril` is not a parameter of the module, so in pytorch naming convention, it is called a buffer. Buffers are added to the state dict of the module by `register_buffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4acc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k, q, v = self.key(x), self.query(x), self.value(x) # (B,T,C)\n",
    "        \n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5                       # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)                                 # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        out = wei @ v                                                # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5d15f",
   "metadata": {},
   "source": [
    "### 6.7 - Multi-Head Self-Attention\n",
    "\n",
    "- **Multiple Attention Heads:** The model initializes multiple `Head` instances, allowing it to capture various aspects of the input data in parallel. Each head focuses on different parts of the sequence, potentially learning to attend to unique features or patterns.\n",
    "\n",
    "- **Projection Layer:** After processing the input through multiple heads, the outputs are concatenated. However, this concatenated output has a larger dimension `(num_heads * head_size)` than the input embedding dimension `(n_embd)`. The projection layer (`self.proj`) maps this higher-dimensional space back to the original embedding dimension. This step is crucial for maintaining a consistent dimensionality across the network, allowing the multi-head attention output to be seamlessly integrated into subsequent parts of the Transformer architecture. Additionally, this projection step can mix information from all heads, enabling the model to leverage the diverse perspectives captured by individual heads.\n",
    "\n",
    "- **Use of Dropout:** The dropout layer applied after the projection helps prevent overfitting by randomly zeroing parts of the output, encouraging the model to learn more robust features that do not depend too heavily on specific paths through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6256f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) # Projection layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Concatenate the outputs from all attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  \n",
    "        # Project the concatenated output back to the original embedding dimension\n",
    "        out = self.dropout(self.proj(out))  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a439a",
   "metadata": {},
   "source": [
    "### 6.8 - Feed-Forward Network\n",
    "\n",
    "In the original \"Attention Is All You Need\" paper, the feedforward network within each Transformer block expands the internal representation dimensionality by a factor of 4 before applying a non-linearity (`ReLU` in this case) and then compresses it back to the original dimension.\n",
    "\n",
    "- Expansion by 4x (`4 * n_embd`): The input embeddings are first linearly projected to a higher-dimensional space (4 times the size of the embedding dimension, `n_embd`). This expansion increases the model's capacity and provides more room for the network to generate internal representations that capture complex patterns and relationships within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a70bd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # the GPT-3 model uses 4x the input dimension here\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),  # compress back down to the input dimension\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2d789",
   "metadata": {},
   "source": [
    "### 6.9 - Transformer Block\n",
    "\n",
    "- **Residual Connections:** By adding the output of the self-attention and feed-forward networks back to their respective inputs `(x + self.sa(...)` and `x + self.ffwd(...))`, the model utilizes residual connections. These connections help mitigate the vanishing gradient problem and allow deeper networks by promoting more effective backpropagation of gradients. \n",
    "\n",
    "- **Layer Normalization:** Layer normalization is applied before each sub-layer (self-attention and feed-forward networks) through `self.ln1(x)` and `self.ln2(x)`. It normalizes the inputs across the features for each data point, stabilizing the training process and improving convergence.\n",
    "\n",
    "- **Head Size Calculation (`head_size = n_embd // n_head`):** This formula divides the embedding dimension by the number of attention heads to determine the dimensionality of each head. By distributing the embedding dimension across multiple heads, the model can attend in parallel to different subspace representations of the input, enabling it to capture a wide variety of information from different perspectives.\n",
    "\n",
    "<br>\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"images/decoder.png\" width=200>\n",
    "    <br><center><caption><font color=\"purple\"><strong><u>Figure 2:</u></strong> Decoder-Only Transformer</font></caption></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6637c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head                    # 64 = 384 / 6 (original GPT-3 model uses 64)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # self-attention module\n",
    "        self.ffwd = FeedFoward(n_embd)                  # feedforward module\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                 # layernorms\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                 # layernorms\n",
    "\n",
    "    def forward(self, x):                       # x is the input tensor\n",
    "        x = x + self.sa(self.ln1(x))            # add skip connection & apply self-attention\n",
    "        x = x + self.ffwd(self.ln2(x))          # add skip connection & apply feedforward\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c39e6",
   "metadata": {},
   "source": [
    "### 6.10 - GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b0c6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # Embedding layer for tokens\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Embedding layer for positions\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])  # Transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer normalization\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # Output layer to predict next token\n",
    "\n",
    "        self.apply(self._init_weights)  # Apply custom weights initialization\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        # Initialize weights for linear and embedding layers\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)  # Normal initialization for linear layer weights\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)  # Zero initialization for linear layer biases\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)  # Normal initialization for embedding weights\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape  # Batch size (B) and sequence length (T)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
    "        x = tok_emb + pos_emb  # Combine token and position embeddings: (B,T,C)\n",
    "        x = self.blocks(x)  # Pass through Transformer blocks: (B,T,C)\n",
    "        x = self.ln_f(x)  # Apply final layer normalization: (B,T,C)\n",
    "        logits = self.lm_head(x)  # Generate logits for next token prediction: (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Reshape logits for loss computation\n",
    "            targets = targets.view(B*T)  # Flatten targets to match logits shape\n",
    "            loss = F.cross_entropy(logits, targets) # Cross-entropy loss between logits and targets\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  # Use only the most recent tokens\n",
    "            logits, _ = self(idx_cond)  # Get logits for the current sequence\n",
    "            logits = logits[:, -1, :]  # Use logits for the last token only: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)  # Softmax to get probabilities: (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample next token: (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # Append to sequence: (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f79d4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcd0e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c6a74",
   "metadata": {},
   "source": [
    "Given the model size of approximately 10.79 million parameters and aiming to demo the model on a local setup with an NVIDIA RTX 3060 GPU, I'm reducing the model size to ensure it fits comfortably within the GPU's memory limits while leaving room for other computational overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d22b2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32   # Reduce from 64 to 32\n",
    "block_size = 128  # Reduce from 256 to 128\n",
    "n_embd = 64       # Reduce from 384 to 64\n",
    "n_head = 6        # Keep the same\n",
    "n_layer = 6       # Keep the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "098ffdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7cbd083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.309313 M parameters\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f57e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1958, val loss 4.1954\n",
      "step 500: train loss 2.4412, val loss 2.4419\n",
      "step 1000: train loss 2.2352, val loss 2.2550\n",
      "step 1500: train loss 2.0054, val loss 2.0670\n",
      "step 2000: train loss 1.8422, val loss 1.9513\n",
      "step 2500: train loss 1.7431, val loss 1.8774\n",
      "step 3000: train loss 1.6803, val loss 1.8480\n",
      "step 3500: train loss 1.6343, val loss 1.8068\n",
      "step 4000: train loss 1.5905, val loss 1.7738\n",
      "step 4500: train loss 1.5708, val loss 1.7501\n",
      "step 4999: train loss 1.5446, val loss 1.7215\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3daf366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TYBERY:\n",
      "Annd Was'rwicher I twich them good. Heer be this slist,\n",
      "Frears and runt you I not all wint\n",
      "We lie beger,' good husal though went and Cateived peies,\n",
      "Now teell would eath, I should weep,\n",
      "For an eat of soreld them unwain's to law his\n",
      "Meath'd leattugh in ont, and theu now, deat to Psoddue us.\n",
      "Which I of no' merch.\n",
      "\n",
      "CESCINLUS:\n",
      "An odour at of trunge.\n",
      "Go to go? this out need rabuse in, netter as those\n",
      "timpe his wearn, and youghnar layst upa with I willl.\n",
      "Lord beilfess contoumser and me rathing\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "# open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
