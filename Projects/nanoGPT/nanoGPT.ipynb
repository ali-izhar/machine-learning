{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31346ec",
   "metadata": {},
   "source": [
    "# Building GPT\n",
    "\n",
    "> Recreating [nanoGPT](https://github.com/karpathy/nanoGPT) from Andrej Karpathy\n",
    "\n",
    "<font color=\"purple\">We'll train a character-level GPT on the works of Shakespeare.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aaca1e",
   "metadata": {},
   "source": [
    "## 1 - Shakespeare Dataset\n",
    "\n",
    "<hr>\n",
    "\n",
    "Let's download the tiny shakespeare dataset. It is ~ 1MB file and contains ~ 1115394 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f2678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open('data/tinyshakespeare.txt', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download the file. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e736ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text to inspect it\n",
    "with open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c3099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f03b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 500 characters\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd113c5",
   "metadata": {},
   "source": [
    "### 1.1 - Unique Characters\n",
    "\n",
    "Extract the unique characters in the dataset.\n",
    "- `set(text)` will create a set of the characters that occur in the text\n",
    "- `list(.)` will create an arbitrary ordering from the set\n",
    "- `sorted(.)` will create a sorted ordering from the list\n",
    "- `vocab_size` is the number of all the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff155a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars)) # note the space character at the start\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186dce3c",
   "metadata": {},
   "source": [
    "## 2 - Tokenization\n",
    "\n",
    "<hr>\n",
    "\n",
    "A tokenizer functions as an encoder-decoder system, translating human-readable text into a format understandable by machines, and vice versa. Its primary role is to break down text into manageable pieces, known as tokens, which can represent individual characters, parts of words (sub-words), or whole words.\n",
    "\n",
    "### 2.1 - Character-level vs. Sub-word Tokenization\n",
    "\n",
    "- **Character-level Encoding:** This approach encodes each character of the text as a unique integer. Since we're building a character-level language model, we'll use this tokenizer.\n",
    "\n",
    "- **Sub-word Tokenization:** Advanced tokenizers like OpenAI's `tiktoken` and Google's `SentencePiece` operate at the sub-word level. This method finds a balance between not encoding entire words and not going down to individual characters. \n",
    "\n",
    "For instance, the phrase \"hi there\" (8 characters) could be encoded into:\n",
    "- 8 integers using character-level encoding\n",
    "- 2 integers using word-level encoding\n",
    "- 3 integers, for instance, using sub-word level encoding\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.n_vocab)\n",
    "# 50257\n",
    "print(enc.encode(\"hii there\"))\n",
    "# [71, 4178, 612]\n",
    "print(enc.decode([71, 4178, 612]))\n",
    "# 'hii there'\n",
    "```\n",
    "\n",
    "Notice that `tiktoken` uses a vocabular size of 50257 instead of 65 (in our case). And thus the encoding for the string \"hi there\" is just 3 integers.\n",
    "\n",
    "In essence, you can have:\n",
    "- A very large sequence of integers with a small vocabulary\n",
    "- A very short sequence of integers with a large vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dc22ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]          # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747270c8",
   "metadata": {},
   "source": [
    "### 2.2 - Encoding Data into a Torch Tensor\n",
    "\n",
    "Now that we've our character-level tokenizer, encode the entire Shakespeare dataset into a `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50907303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b1000a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)\n",
    "print(data[:500]) # the 500 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1120c",
   "metadata": {},
   "source": [
    "## 3 - Train / Val Split & Data Loader\n",
    "\n",
    "<hr>\n",
    "\n",
    "Split the dataset into 90/10 train/val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4468fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9bced",
   "metadata": {},
   "source": [
    "### 3.1 - Data Loader\n",
    "\n",
    "To train transformer models like NanoGPT efficiently, data is not fed into the system all at once due to computational constraints. Instead, text data is broken into manageable chunks.\n",
    "\n",
    "#### 3.1.1 - Block Size / Context Length\n",
    "\n",
    "- The `block_size` or `context_length` determines the maximum length of these data chunks.\n",
    "- When processing a chunk, the model is trained to predict the next character (or token) in the sequence. For example, in a block with a size of 9, there are effectively 8 training pairs, each input 'x' paired with its subsequent character 'y'.\n",
    "\n",
    "#### 3.1.2 - Dimensions in Training\n",
    "\n",
    "- **Time Dimension:** This refers to the sequence of tokens fed into the model, reflecting the linear progression of text.\n",
    "- **Batch Dimension:** Training data is also organized in batches. This dimension allows multiple chunks of data to be processed simultaneously, enhancing the efficiency of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd4ce69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f2777",
   "metadata": {},
   "source": [
    "In a chunk of 9 characters, there are 8 individual examples packed in there. For instance, given this chunk:\n",
    "\n",
    "```python\n",
    "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
    "```\n",
    "\n",
    "- in the context of [18], 47 comes next.\n",
    "- in the context of [18, 47], 56 comes next.\n",
    "- in the context of [18, 47, 56], 57 comes next, and so on.\n",
    "\n",
    "Why do we do this?\n",
    "\n",
    "- This approach ensures the transformer is exposed to contexts ranging from very small (a single integer) to the full length of the `block_size`, allowing it to learn and understand text in varying lengths effectively.\n",
    "- If the input text exceeds the set `block_size`, the transformer model truncates the excess, focusing only on the text within the defined limit. This process ensures computational efficiency and relevance in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8bc6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]      # inputs to the transformer: first block_size characters\n",
    "y = train_data[1:block_size+1]   # targets for each input position: off-set by 1\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]            # all chars up to and including t\n",
    "    target = y[t]                # t-th char in the y array\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91c033",
   "metadata": {},
   "source": [
    "### 3.2 - Data Batches\n",
    "\n",
    "- `batch_size = 4` specifies that four sequences (or chunks of data) will be processed in parallel during each training iteration. It enhances efficiency by utilizing the GPU's ability to handle parallel computations.\n",
    "- `block_size = 8` indicates the maximum length of context the model will consider for making predictions. Each sequence in a batch will have up to 8 elements (e.g., characters or tokens).\n",
    "\n",
    "- `get_batch(split)` creates batches of input $(x)$ and target $(y)$ data for training or validation. It randomly selects starting points in the data and then extracts sequences of length `block_size` for $x$ and the corresponding next elements as targets $y$. It selects `batch_size` number of random starting points $(ix)$ in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1735d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4            # how many independent sequences will we process in parallel?\n",
    "block_size = 8            # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad248c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):     # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d010a1",
   "metadata": {},
   "source": [
    "Above, we see that `xb` is a single batch of 32 independent examples sampled from the training dataset and `yb` is the corresponding target labels (for loss computations later on). This batch `xb` of 32 examples in going to feed into a transformer which will simutaneously process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5136d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331cb359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d324f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9ceff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097aac5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb32baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
