{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdca6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\github\\data-mining\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f595fcd",
   "metadata": {},
   "source": [
    "<img src=\"images/transformers.png\" width=500>\n",
    "\n",
    "The Transformer architecture has two main blocks: the **encoder** and the **decoder.**\n",
    "\n",
    "- **Encoder:** It has a Multi-Head Attention mechanism and a fully connected Feed-Forward network. There are also residual connections around the two sub-layers, plus layer normalization for the output of each sub-layer. All sub-layers in the model and the embedding layers produce outputs of dimension $d_{\\text{model}} = 512$\n",
    "\n",
    "- **Decoder:** The decoder follows a similar structure, but it inserts a third sub-layer that performs multi-head attention over the output of the encoder block. There is also a modification of the self-attention sub-layer in the decoder block to avoid positions from attending to subsequent positions. This masking ensures that the predictions for position $i$ depend solely on the known outputs at positions less than $i$.\n",
    "\n",
    "Both the encoder and decode blocks are repeated $N$ times. In the original paper, they defined $N=6$, and we will define a similar value in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce1f83",
   "metadata": {},
   "source": [
    "## Input Embeddings\n",
    "\n",
    "When we observe the Transformer architecture image above, we can see that the Embeddings represent the first step of both blocks.\n",
    "\n",
    "The `InputEmbedding` class below is responsible for converting the input text into numerical vectors of `d_model` dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{text{model}}}$.\n",
    "\n",
    "In the image below, we can see how the embeddings are created. First, we have a sentence that gets split into tokens. Then, the token IDs are transformed into the embeddings, which are high-dimensional vectors.\n",
    "\n",
    "<img src=\"images/transformer-tokenization.svg\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcb13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model       # Dimension of vectors (512)\n",
    "        self.vocab_size = vocab_size # Size of the vocabulary\n",
    "        \n",
    "        # PyTorch layer that converts integer indices to dense embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Normalizing the variance of the embeddings\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d61691",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder blocks so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.\n",
    "\n",
    "In the `PositionalEncoding` class below, we will create a matrix of positional encodings pe with dimensions `(seq_len, d_model)`. We will start by filling it with 0s. We will then apply the `sine` function to even indices of the positional encoding matrix while the `cosine` function is applied to the odd ones.\n",
    "\n",
    "$$\\text{Even Indices}(2i): \\quad \\text{PE}(pos, 2i) = \\sin\\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right)\\tag{1}$$\n",
    "\n",
    "$$\\text{Odd Indices}(2i + 1): \\quad \\text{PE}(pos, 2i + 1) = \\cos\\left( \\frac{pos}{10000^{2i / d_{\\text{model}}}} \\right)\\tag{2}$$\n",
    "\n",
    "We apply the `sine` and `cosine` functions because it allows the model to determine the position of a word based on the position of other words in the sequence, since for any fixed offset $k, PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of `sine` and `cosine` functions, where a shift in the input results in a predictable change in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf4dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model     # Dimensionality of the model\n",
    "        self.seq_len = seq_len     # Maximum sequence length\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
    "        \n",
    "        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
    "        pe = torch.zeros(seq_len, d_model) \n",
    "        \n",
    "        # Creating a tensor representing positions (0 to seq_len - 1)\n",
    "        # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices in pe\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in pe\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Adding an extra dimension at the beginning of pe matrix for batch handling\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Addind positional encoding to the input tensor X\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x) # Dropout for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd90a9",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "When we look at the encoder and decoder blocks, we see several normalization layers called `Add & Norm`.\n",
    "\n",
    "The `LayerNormalization` class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called `epsilon` to avoid any divisions by zero. This process results in a normalized output with a mean 0 and a standard deviation 1.\n",
    "\n",
    "We will then scale the normalized output by a learnable parameter `alpha` and add a learnable parameter called `bias`. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4d1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        # One-dimensional tensor that will be used to scale the input data\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # One-dimensional tensor that will be added to the input data\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6666f",
   "metadata": {},
   "source": [
    "## Feed-Forward Network\n",
    "\n",
    "In the fully connected feed-forward network, we apply two linear transformations with a `ReLU` activation in between. We can mathematically represent this operation as:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max{(0, x W_1 + b_1)W_2 + b_2}\\tag{3}$$\n",
    "\n",
    "$W_1$ and $W_2$ are the weights, while $b_1$ and $b_2$ are the biases of the two linear transformations.\n",
    "\n",
    "In the `FeedForwardBlock` below, we will define the two linear transformations `self.linear_1` and `self.linear_2` and the inner-layer `d_ff`. The input data will first pass through the `self.linear_1` transformation, which increases its dimensionality from `d_model` to `d_ff`. The output of this operation passes through the ReLU activation function, which introduces non-linearity so the network can learn more complex patterns, and the `self.dropout` layer is applied to mitigate overfitting. The final operation is the `self.linear_2` transformation to the dropout-modified tensor, which transforms it back to the original `d_model` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be631ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # First linear transformation\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        \n",
    "        # Second linear transformation\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd91cdb",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "The Multi-Head Attention is the most crucial component of the Transformer. It is responsible for helping the model to understand complex relationships and patterns in the data.\n",
    "\n",
    "<img src=\"images/mhattention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e77c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        \n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
    "        \n",
    "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
    "        \n",
    "        # Defining the weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        # mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "        \n",
    "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "        \n",
    "        # We calculate the Attention(Q,K,V) as in the formula in the image above \n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "        if mask is not None: # If a mask IS defined...\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "            \n",
    "        # Multiply the output matrix by the V matrix, as in the formula\n",
    "        return (attention_scores @ value), attention_scores\n",
    "        \n",
    "    def forward(self, q, k, v, mask): \n",
    "        \n",
    "        query = self.w_q(q) # Q' matrix\n",
    "        key = self.w_k(k) # K' matrix\n",
    "        value = self.w_v(v) # V' matrix\n",
    "        \n",
    "        \n",
    "        # Splitting results into smaller matrices for the different heads\n",
    "        # Splitting embeddings (third dimension) into h parts\n",
    "        \n",
    "        # Transpose => bring the head to the second dimension\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Transpose => bring the head to the second dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Transpose => bring the head to the second dimension\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # Obtaining the output and the attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Obtaining the H matrix\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee290fc1",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "When we look at the architecture of the Transformer, we see that each sub-layer, including the self-attention and Feed Forward blocks, adds its output to its input before passing it to the Add & Norm layer. This approach integrates the output with the original input in the Add & Norm layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e97b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "        self.norm = LayerNormalization() # We use a normalization layer \n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "        return x + self.dropout(sublayer(self.norm(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3a37f",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "We will now build the encoder. We create the EncoderBlock class, consisting of the Multi-Head Attention and Feed Forward layers, plus the residual connections.\n",
    "\n",
    "<img src=\"images/encoder.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf244473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, \n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        \n",
    "        # 2 Residual Connections with dropout\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "        # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        \n",
    "        # Applying the second residual connection with the feed-forward block \n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49eb140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Encoder can have several Encoder Blocks\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers             # Storing the EncoderBlocks\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)   # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x)      # Normalizing output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2e396",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Similarly, the Decoder also consists of several `DecoderBlocks` that repeat six times in the original paper. The main difference is that it has an additional sub-layer that performs multi-head attention with a cross-attention component that uses the output of the Encoder as its keys and values while using the Decoder's input as queries.\n",
    "\n",
    "<img src=\"images/decoder.png\" width=300>\n",
    "\n",
    "For the Output Embedding, we can use the same `InputEmbeddings` class we use for the Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e55c2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "    # It also takes in the feed-forward block and the dropout rate\n",
    "    def __init__(self,  \n",
    "                 self_attention_block: MultiHeadAttentionBlock, \n",
    "                 cross_attention_block: MultiHeadAttentionBlock, \n",
    "                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([\n",
    "            ResidualConnection(dropout) for _ in range(3)\n",
    "        ]) # List of three Residual Connections with dropout rate\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        \n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        \n",
    "        # Feed-forward block with residual connections\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91ca6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Decoder can have several Decoder Blocks\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Returns normalized output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acbf2d0",
   "metadata": {},
   "source": [
    "You can see in the Decoder image that after running a stack of DecoderBlocks, we have a Linear Layer and a Softmax function to the output of probabilities. The ProjectionLayer class below is responsible for converting the output of the model into a probability distribution over the vocabulary, where we select each output token from a vocabulary of possible tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "675987d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buiding Linear Layer\n",
    "class ProjectionLayer(nn.Module):\n",
    "    # Model dimension and the size of the output vocabulary\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3b186",
   "metadata": {},
   "source": [
    "## Building the Transformer\n",
    "\n",
    "We finally have every component of the Transformer architecture ready. We may now construct the Transformer by putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dededeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "    def __init__(self, \n",
    "                 encoder: Encoder, decoder: Decoder, \n",
    "                 src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, \n",
    "                 src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, \n",
    "                 projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        \n",
    "    # Encoder     \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "        return self.encoder(src, src_mask) \n",
    "    \n",
    "    # Decoder\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "        \n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398e359",
   "metadata": {},
   "source": [
    "The architecture is finally ready. We now define a function called build_transformer, in which we define the parameters and everything we need to have a fully operational Transformer model for the task of **machine translation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19487b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, \n",
    "                      d_model: int = 512, N: int = 6, h: int = 8, \n",
    "                      dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    \n",
    "    # Creating Embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "    \n",
    "    # Creating Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "    \n",
    "    # Creating EncoderBlocks\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "        \n",
    "        # Combine layers into an EncoderBlock\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "        \n",
    "    # Creating DecoderBlocks\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "        \n",
    "        # Combining layers into a DecoderBlock\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, \n",
    "                                     feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "        \n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Creating projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "    \n",
    "    # Creating the transformer by combining everything above\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13efde57",
   "metadata": {},
   "source": [
    "### Reference: [Kaggle](https://www.kaggle.com/code/lusfernandotorres/transformer-from-scratch-with-pytorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
