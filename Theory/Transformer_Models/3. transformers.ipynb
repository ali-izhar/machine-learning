{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebc992a",
   "metadata": {},
   "source": [
    "# 1 - Transformer Networks\n",
    "<hr>\n",
    "\n",
    "Transformers have revolutionized the field of Natural Language Processing (NLP). This advanced neural network architecture surpasses previous models in handling complex NLP tasks, offering unparalleled effectiveness.\n",
    "\n",
    "### 1.1 - Evolution of Sequence Models in NLP\n",
    "\n",
    "- **From RNN to LSTM:** The journey in sequence modeling began with Recurrent Neural Networks (RNNs), which faced challenges like vanishing gradients, hindering their ability to capture long-range dependencies in sequences. This led to the development of Gated Recurrent Units (GRUs) and Long Short-Term Memory networks (LSTMs), which introduced gates to control information flow, addressing the vanishing gradient problem to an extent.\n",
    "- **Complexity and Sequential Processing:** With each advancement (RNN → GRU → LSTM), models grew more complex. However, they remained inherently sequential - processing inputs one token at a time, thus creating a bottleneck in information flow.\n",
    "\n",
    "\n",
    "### 1.2 - The Transformer Breakthrough\n",
    "\n",
    "- **Parallel Processing:** The Transformer revolutionized this approach by enabling the parallel processing of entire sequences. Unlike the sequential processing of RNNs and LSTMs, Transformers ingest and process entire sentences simultaneously.\n",
    "- **Combining Attention Mechanisms and CNN-style Processing:** The Transformer's core innovation lies in its use of attention-based mechanisms combined with a processing style akin to Convolutional Neural Networks (CNNs). This allows for more efficient and richer representation of data.\n",
    "- **Self-Attention:** Computes representations for each word in a sentence in parallel, capturing different aspects of semantic information.\n",
    "- **Multi-Headed Attention:** Iterates over the self-attention process, creating multiple sets of word representations, enriching the contextual understanding of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1d65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec16ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1dbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
