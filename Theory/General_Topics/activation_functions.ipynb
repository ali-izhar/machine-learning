{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf9e4b3",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "Activation functions are mathematical functions used in neural networks to introduce non-linearity into the model. In the following image, the function $g$ would be the activation function:\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/activation_intro.png\" width=800>\n",
    "</div>\n",
    "\n",
    "We need to use activation functions because linear transformations of the input data (i.e. $g$ would be the identity function, $f(x) = x$) in a neural network results in a linear output, regardless of the number of layers or neurons used in the model. This limits the model's ability to learn complex patterns in the data. For example, in a binary classification problem, a model without non-linearity would be limited to fitting a linear decision boundary between the two classes.\n",
    "\n",
    "## Need for Activation Functions\n",
    "Activation functions are crucial in neural networks to introduce non-linearity, enabling them to learn from complex data patterns. If every neuron in a neural network were to use a linear activation function, the network would function like linear regression. Regardless of the network's depth, it could only fit linear relationships in data, limiting its utility. \n",
    "\n",
    "Let's simplify this concept with a one-hidden-unit network example. If a linear function is used everywhere, the output becomes a linear function of the input, equivalent to using a simple linear regression model.\n",
    "\n",
    "This limitation arises from the fact that the composition of linear functions is also a linear function. Therefore, a multilayer neural network employing linear activation functions would equate to linear or logistic regression, depending on the output layer function. This would prevent the network from learning complex features and diminish the benefit of multiple layers. Therefore, it's advised not to use linear activation functions in hidden layers. The Rectified Linear Unit (ReLU) is a commonly recommended alternative for hidden layers. Activation functions other than linear ones enable neural networks to tackle a wider range of problems, including binary classification, regression, and multi-category classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269905f",
   "metadata": {},
   "source": [
    "## Linear\n",
    "A straight line function where activation is proportional to input (which is the weighted sum from neuron).\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/linear_act.png\" width=600>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- It gives a range of activations, so it is not binary activation.\n",
    "- We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.\n",
    "\n",
    "**Cons:**\n",
    "- For this function, derivative is a constant. That means, the gradient has no relationship with X.\n",
    "- It is a constant gradient and the descent is going to be on constant gradient.\n",
    "- If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381d3e5",
   "metadata": {},
   "source": [
    "## ELU\n",
    "**Exponential Linear Unit** or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.\n",
    "\n",
    "ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/elu_act.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "- ELU is a strong alternative to ReLU.\n",
    "- Unlike to ReLU, ELU can produce negative outputs.\n",
    "\n",
    "**Cons:**\n",
    "- For x > 0, it can blow up the activation with the output range of $[0, \\infty]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96340919",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "ReLU (Rectified Linear Unit) is most commonly used activation function in deep learning neural networks. It works by mapping any negative input to zero and any positive input to its own value. Mathematically, ReLU is defined as: $F(x) = \\text{max} (0, x)$. Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/relu_act.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- It avoids and rectifies vanishing gradient problem.\n",
    "- ReLu is less computationally expensive than `tanh` and `sigmoid` because it involves simpler mathematical operations.\n",
    "\n",
    "**Cons:**\n",
    "- One of its limitations is that it should only be used within hidden layers of a neural network model.\n",
    "- Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. In other words, ReLu can result in dead neurons.\n",
    "- In another words, For activations in the region $(x<0)$ of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input (simply because gradient is 0, nothing changes). This is called the dying ReLu problem.\n",
    "- The range of ReLu is $[0,\\infty)$. This means it can blow up the activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c0332",
   "metadata": {},
   "source": [
    "## LeakyReLU\n",
    "LeakyRelu is a variant of ReLU. Instead of being $0$ when $z<0$, a leaky ReLU allows a small, non-zero, constant gradient $\\alpha$ (Normally, $\\alpha=0.01$).\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/leaky_relu_act.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- Leaky ReLUs are one attempt to fix the \"dying ReLU\" problem by having a small negative slope (of 0.01, or so).\n",
    "\n",
    "**Cons:**\n",
    "- As it possess linearity, it can't be used for the complex Classification. It lags behind the Sigmoid and Tanh for some of the use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ccfa2",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "Sigmoid takes a real value as input and outputs another value between 0 and 1. It's easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/sigmoid_act.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- It is nonlinear in nature. Combinations of this function are also nonlinear!\n",
    "- It will give an analog activation unlike step function.\n",
    "- It has a smooth gradient too.\n",
    "- It’s good for a classifier.\n",
    "- The output of the activation function is always going to be in range (0,1) compared to $(-\\infty, \\infty)$ of linear function. So we have our activations bound in a range. Nice, it won’t blow up the activations then.\n",
    "\n",
    "**Cons:**\n",
    "- Towards either end of the sigmoid function, the $Y$ values tend to respond very less to changes in $X$.\n",
    "- It gives rise to a problem of \"vanishing gradients\".\n",
    "- Its output isn't zero centered. It makes the gradient updates go too far in different directions. $0 < \\text{output} < 1$, and it makes optimization harder.\n",
    "- Sigmoids saturate and kill gradients.\n",
    "- The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd7555",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "Tanh squashes a real-valued number to the range $[-1, 1].$ It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/tanh_act.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- The gradient is stronger for tanh than sigmoid (derivatives are steeper).\n",
    "\n",
    "**Cons:**\n",
    "- Tanh also has the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4075a",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "The Softplus activation function is a smooth and continuous function that is a variation of the ReLU activation function. It maps any input value to a value between 0 and infinity. The math behind the Softplus function is:\n",
    "\n",
    "$$f(x) = \\log (1 + \\exp(x))$$\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/softplus_act.png\" width=500>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- It has a range of output values between 0 and infinity, which can be useful in some cases.\n",
    "- It is computationally efficient to calculate.\n",
    "\n",
    "**Cons:**\n",
    "- It is not zero-centered, which can cause problems with convergence based on the neural network architectures.\n",
    "- It can be sensitive to the initial values of the weights in the network, which can affect the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbacac",
   "metadata": {},
   "source": [
    "## GELU (Gaussian Error Linear Unit)\n",
    "The GELU (Gaussian Error Linear Unit) activation function is a smooth, non-linear function that is used in deep learning models. It is defined as:\n",
    "\n",
    "$$\\text{GELU} = \\frac{1}{2} x (1 + \\text{erf} (\\frac{x}{\\sqrt{2}})$$\n",
    "\n",
    "where $\\text{erf}$ is the error function.\n",
    "\n",
    "<div style=\"align=center\">\n",
    "    <img src=\"media/gelu_act.png\" width=600>\n",
    "</div>\n",
    "\n",
    "**Pros:**\n",
    "- It has shown to perform well in deep neural networks, especially in natural language processing (NLP) tasks.\n",
    "- It is computationally efficient and can be easily implemented in neural network architectures.\n",
    "\n",
    "**Cons:**\n",
    "- It may not perform as well in image recognition tasks compared to ReLU and its variants.\n",
    "- It may not be as stable as other activation functions, especially when using large learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d32db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Choosing the Right Activation Function\n",
    "Selecting the appropriate activation function depends on the nature of the problem and the type of output you're predicting.\n",
    "\n",
    "For the **output layer:**\n",
    "\n",
    "- **Sigmoid** is best for binary classification tasks due to its probability interpretation.\n",
    "- **Linear activation** is suitable for tasks like predicting stock prices, which can take on any real number (positive or negative).\n",
    "- **ReLU** is ideal for predicting values that are always non-negative, such as house prices.\n",
    "- **TanH** is a good choice for multi-class classification tasks.\n",
    "\n",
    "For **hidden layers, ReLU** is the most commonly used due to its ability to speed up computation and learning by enabling sparse activation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
