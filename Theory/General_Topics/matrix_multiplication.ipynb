{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa4d14b",
   "metadata": {},
   "source": [
    "# Understanding the Basic Concept of Matrix Multiplication in ML\n",
    "Matrix multiplication, often represented as $Y = W X + b$, is fundamental to the workings of neural networks and a wide variety of machine learning algorithms. In the realm of machine learning, we frequently deal with an input dataset $X$ and a corresponding output $Y$. The primary objective of a machine learning algorithm is to discern a function $f$ that can effectively map inputs $X$ to their outputs $Y$. This is mathematically captured as $f(X)=Y$. To ascertain $f$, we need to determine the optimal parameters (or weights) $W$ and bias $b$ such that the equation $f(X)=W \\times X + b$ holds true. Here:\n",
    "\n",
    "- $W$ is a matrix, representing the weights.\n",
    "- $b$ is a vector, symbolizing the bias.\n",
    "\n",
    "The dimensions of $W$ are influenced by the number of rows in $X$ (features) and the number of columns in $Y$ (output dimensions). Meanwhile, $b$'s size is set by the number of rows in $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088dc84",
   "metadata": {},
   "source": [
    "## Generalized Representation\n",
    "Given an input data $X$ and output data $Y$, we can represent these matrices in their generalized form:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "| & | & \\cdots & | \\\\\n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n",
    "| & | & \\cdots & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each $x^{(i)}$ is a column vector representing the $i$-th sample in the dataset and $m$ is the total number of samples.\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "-- & w^{(1)} & -- \\\\\n",
    "-- & w^{(2)} & -- \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "-- & w^{(n)} & -- \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Or, $W$ can also be represented as:\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\cdots & w_{1n} \\\\\n",
    "w_{21} & w_{22} & \\cdots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m1} & w_{m2} & \\cdots & w_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $w_{ij}$ represents the weight of the connection between the $i$-th input and the $j$-th output node.\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_n \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each $b_i$ is the bias for the $i$-th output node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06164671",
   "metadata": {},
   "source": [
    "## Matrix Multiplication Unveiled\n",
    "When executing the operation $W \\times X + b$:\n",
    "\n",
    "1. Matrix Multiplication (Dot Product): Each row of matrix $W$ is multiplied with each column of matrix $X$. This results in a new matrix where each entry is the sum of the products of the corresponding row and column entries.\n",
    "2. Adding the Bias: The resulting matrix from the dot product is then added to the bias vector $b$. This operation is done element-wise, i.e., each entry in the matrix is added to its corresponding entry in the bias vector.\n",
    "\n",
    "The outcome of this process is a new matrix, which provides the predicted outputs corresponding to the input data in $X$.\n",
    "\n",
    "> Note: For the matrix multiplication to be valid, the number of columns in $W$ must match the number of rows in $X$. That's why in many scenarios, we see the transpose of $W$ (i.e., $W^T$) being used to align the matrices properly for multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b398fc",
   "metadata": {},
   "source": [
    "## Matrix Dimensions in Neural Networks\n",
    "Consider a neural network with the following architecture:\n",
    "\n",
    "- **Input layer**: 2 nodes\n",
    "- **First hidden layer**: 3 nodes\n",
    "- **Second hidden layer**: 5 nodes\n",
    "- **Third hidden layer**: 4 nodes\n",
    "- **Fourth hidden layer**: 2 nodes\n",
    "- **Output layer**: 1 node\n",
    "\n",
    "To denote the number of nodes in each layer, we use the notation $n^{[l]}$. For example:\n",
    "- $n^{[1]}$ represents the number of nodes in the first hidden layer.\n",
    "- $n^{[2]}$ represents the number of nodes in the second hidden layer, and so on.\n",
    "\n",
    "The output $Z^{[l]}$ of each layer $l$ is calculated using the formula:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]} \\times A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "Where:\n",
    "- $Z^{[l]}$ is the output of the layer.\n",
    "- $W^{[l]}$ is the weight matrix.\n",
    "- $A^{[l-1]}$ is the input matrix.\n",
    "- $b^{[l]}$ is the bias vector.\n",
    "\n",
    "To understand the dimensions of the weight matrices $W$, we can use the following table:\n",
    "\n",
    "> Note: The dimensions of the weight matrices $W$ are influenced by the number of nodes in the current layer $n^{[l]}$ and the number of nodes in the previous layer $n^{[l-1]}$. For example, $W^{[1]}$ is a $3 \\times 2$ matrix because the first hidden layer has 3 nodes and the input layer has 2 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfb408",
   "metadata": {},
   "source": [
    "| Layer Output $Z$ | Calculation | Dimensions | Dimensions of $W$ |\n",
    "| :--------------: | :---------: | :--------: | :---------------: |\n",
    "| $$Z^{[1]}$$        | $$W^{[1]} \\times X$$ | $$(3 \\times 2) \\times (2 \\times 1) = 3 \\times 1$$ | $$3 \\times 2$$ |\n",
    "| $$Z^{[2]}$$        | $$W^{[2]} \\times A^{[1]}$$ | $$(5 \\times 3) \\times (3 \\times 1) = 5 \\times 1$$ | $$5 \\times 3$$ |\n",
    "| $$Z^{[3]}$$        | $$W^{[3]} \\times A^{[2]}$$ | $$(4 \\times 5) \\times (5 \\times 1) = 4 \\times 1$$ | $$4 \\times 5$$ |\n",
    "| $$Z^{[4]}$$        | $$W^{[4]} \\times A^{[3]}$$ | $$(2 \\times 4) \\times (4 \\times 1) = 2 \\times 1$$ | $$2 \\times 4$$ |\n",
    "| $$Z^{[5]}$$        | $$W^{[5]} \\times A^{[4]}$$ | $$(1 \\times 2) \\times (2 \\times 1) = 1 \\times 1$$ | $$1 \\times 2$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a0dbc",
   "metadata": {},
   "source": [
    "Therefore, the general formula for calculating the dimensions of the weight matrices $W$ is:\n",
    "\n",
    "$$W^{[l]} = n^{[l]} \\times n^{[l-1]}$$\n",
    "\n",
    "Where $n^{[l]}$ is the number of nodes in the current layer and $n^{[l-1]}$ is the number of nodes in the previous layer.\n",
    "\n",
    "The dimensions of the bias vectors $b$ for each layer are:\n",
    "- $b^{[1]}$ is a $3 \\times 1$ matrix.\n",
    "- $b^{[2]}$ is a $5 \\times 1$ matrix.\n",
    "- $b^{[3]}$ is a $4 \\times 1$ matrix.\n",
    "- $b^{[4]}$ is a $2 \\times 1$ matrix.\n",
    "- $b^{[5]}$ is a $1 \\times 1$ matrix.\n",
    "\n",
    "Therefore, the general formula for calculating the dimensions of the bias vectors $b$ is:\n",
    "\n",
    "$$b^{[l]} = n^{[l]} \\times 1$$\n",
    "\n",
    "Where $n^{[l]}$ is the number of nodes in the current layer.\n",
    "\n",
    "In calculating backpropagation, we need to calculate the derivative of the weight matrices $W$ and the bias vectors $b$. The dimensions of the derivative matrices $dW$ are the same as the dimensions of the weight matrices $W$. The dimensions of the derivative vectors $db$ are the same as the dimensions of the bias vectors $b$. That is:\n",
    "\n",
    "$$dW^{[l]} = n^{[l]} \\times n^{[l-1]}$$\n",
    "\n",
    "$$db^{[l]} = n^{[l]} \\times 1$$\n",
    "\n",
    "A concrete example of the dimensions of the weight matrices $W$ and the bias vectors $b$ is shown below:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> <b>Shape of W</b> </td> \n",
    "        <td> <b>Shape of b</b>  </td> \n",
    "        <td> <b>Activation</b> </td>\n",
    "        <td> <b>Shape of Activation</b> </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Layer 1</b> </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Layer 2</b> </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>  \n",
    "   <tr>\n",
    "       <td> <b>Layer L-1</b> </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "   <tr>\n",
    "   <tr>\n",
    "       <td> <b>Layer L</b> </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
