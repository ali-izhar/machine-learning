{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9660edac",
   "metadata": {},
   "source": [
    "# Maximum Likelihood\n",
    "<hr>\n",
    "Maximum likelihood is an important principle in probability theory that can be used to estimate the unknown parameters of a probability distribution given a data set. \n",
    "\n",
    "Assume that we have a set of $n$ data points denoted by $X = \\{x_1, x_2, \\cdots, x_n\\}$, which are generated from some probability distribution $P(X; \\theta)$ with unknown parameter $\\theta$. For example, the data points might be drawn from a Gaussian (normal) distribution, where the parameters $\\theta$ are the mean $\\mu$ and the standard deviation $\\sigma$ of the distribution.\n",
    "\n",
    "We also assume that the points are identically and independently distributed ($\\text{iid}$ for short), which means that they are all sampled from the same distribution (identically), and all the points are mutually independent (independently).\n",
    "\n",
    "Our goal is to find a model (represented by $\\theta$) that make the observed data most probable, or in other words a model that maximizes the likelihood of obtaining the data points $X$ if we were sampling them from the distribution $P$. This process is often referred to as **maximum likelihood estimation (MLE).**\n",
    "\n",
    "Formally, the **likelihood** of the model (represented by $\\theta$) is defined as the probability of obtaining the observed data $X$ given the model:\n",
    "\n",
    "$$\\mathcal{L}(\\theta \\mid X) = P(X \\mid \\theta)$$\n",
    "\n",
    "Notice the change in the direction of conditionality here: when the model $\\theta$ is known (fixed), the function $P(X \\mid \\theta)$ is the probability density function (PDF) of the points, but when the points $X$ are known (fixed), then the same function becomes a likelihood function of the model $\\theta$.\n",
    "\n",
    "Since the points in $X$ are identically and independently distributed, we can write the likelihood function as a product of the probabilities of the individual data points in $X$:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\mathcal{L}(\\theta \\mid X) = P(x_1, \\cdots, x_n \\mid \\theta) = \\prod_{i=1}^n P(x_i \\mid \\theta) & \\text{discrete case} \\\\\n",
    "\\mathcal{L}(\\theta \\mid X) = f(x_1, \\cdots, x_n \\mid \\theta) = \\prod_{i=1}^n f(x_i \\mid \\theta) & \\text{continuous case} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Writing an explicit expression for the likelihood function can be quite complex (depending on the probability $P$). To simplify the function, we typically take its logarithm, which allows us to convert the product of probabilities into a sum of logarithms. The resultant function is called the **log likelihood:**\n",
    "\n",
    "$$l(\\theta \\mid X) = \\log \\mathcal{L}(\\theta \\mid X) = \\sum_{i=1}^n \\log{(P(x_i \\mid \\theta))}$$\n",
    "\n",
    "Maximizing the log likelihood is the same as maximizing the likelihood, since the logarithm function is monotonically increasing.\n",
    "\n",
    "To find the parameters $\\theta$ that maximize the (log) likelihood, all we need to do is to compute the derivatives of the (log) likelihood with respect to each parameter in $\\theta$, set it to zero, and then solve the resultant system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f0fdc",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "<hr>\n",
    "There are 10 balls in a bag. Each ball is either red or green. Let $\\theta$ be the number of red balls. In order to estimate $\\theta$, we draw 5 balls with replacement out of the bag, replacing each one before drawing the next. The balls that we get are \"red\", \"red\", \"green\", \"red\" and \"green\" (in that order). What is the maximum likelihood estimate (MLE) for $\\theta$?\n",
    "\n",
    "To answer this question, we first need to write the likelihood function of $\\theta$. According to our previous definitions, the likelihood of $\\theta$ is the probability of getting this specific sequence of colors (our \"data points\"), or in mathematical notations:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = P(\\text{red, red, green, red, green} \\mid \\theta)$$\n",
    "\n",
    "Since we have $\\theta$ red balls out of 10 in the bag, the probability of drawing a red ball from the bag is $\\frac{\\theta}{10}$ and the probability of drawing a green ball from the bag is $\\frac{10-\\theta}{10}$.\n",
    "\n",
    "Therefore, the probability of getting this specific sequence of colors is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\left(\\frac{\\theta}{10}\\right)^3 \\left(\\frac{10-\\theta}{10}\\right)^2 $$\n",
    "\n",
    "To simplify this function, we take its logarithm to obtain the log likelihood:\n",
    "\n",
    "$$l(\\theta) = \\text{log} \\mathcal{L}(\\theta) = 3(\\text{log} \\theta - \\text{log} 10) + 2 (\\text{log}(10 - \\theta) - \\text{log}10) = 3 \\text{log}\\theta + 2 \\text{log}(10-\\theta) - 5 \\text{log}10 $$\n",
    "\n",
    "We now compute the derivative of the log likelihood and set it to $0$:\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\theta}} = \\frac{3}{\\theta} - \\frac{2}{10-\\theta} = 0$$\n",
    "\n",
    "Therefore we get:\n",
    "\n",
    "$$3(10-\\theta) = 2\\theta \\Rightarrow 5\\theta = 30 \\Rightarrow \\theta=6$$\n",
    "\n",
    "The model that best explains our data states that we have 6 red balls in the bag out of 10. This result meets our expectation, since in our experiment we drew from the bag 3 red balls out of 5 random balls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beff88f",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "<hr>\n",
    "Suppose that $Y_1, Y_2,\\cdots,Y_n$ denote a random sample from the Poisson distribution with mean $\\lambda$. Find the MLE $\\hat{\\lambda}$ for $\\lambda$.\n",
    "\n",
    "$$\\mathcal{L}(\\lambda) = P(y_1, y_2, \\cdots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i !} = \\frac{\\lambda^{y_1+y_2+\\cdots+y_n} e^{-n\\lambda}}{y_1! y_2! \\cdots y_n!}$$\n",
    "\n",
    "Taking the natural log,\n",
    "\n",
    "$$\\mathcal{L}(\\lambda) = \\ln{\\frac{\\lambda^{y_1+y_2+\\cdots+y_n} e^{-n\\lambda}}{y_1! y_2! \\cdots y_n!}} = \\ln{\\lambda^{y_1+y_2+\\cdots+y_n} + \\ln{e^{-n\\lambda}} - \\ln{y_1! y_2! \\cdots y_n!}} $$\n",
    "\n",
    "$$\\mathcal{L}(\\lambda) = (y_1 + y_2 + \\cdots + y_n) \\ln{\\lambda} - n\\lambda - \\ln{y_1! y_2! \\cdots y_n!}$$\n",
    "\n",
    "Taking derivative with respect to $\\lambda$ and setting it equal to zero:\n",
    "\n",
    "$$\\frac{d\\mathcal{L}}{d\\lambda} = \\frac{d}{d\\lambda} [(y_1 + y_2 + \\cdots + y_n)] - \\frac{d}{d\\lambda} (n\\lambda) - \\frac{d}{d\\lambda} \\ln{y_1! y_2! \\cdots y_n!}$$\n",
    "\n",
    "$$\\frac{d\\mathcal{L}}{d\\lambda} = \\sum_{i=1}^n y_i \\left(\\frac{1}{\\lambda}\\right) - n$$\n",
    "\n",
    "Setting it equal to zero:\n",
    "\n",
    "$$\\frac{d\\mathcal{L}}{d\\lambda} = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^n y_i \\left(\\frac{1}{\\lambda}\\right) - n = 0 \\quad \\Rightarrow \\sum_{i=1}^n y_i \\left(\\frac{1}{\\lambda}\\right) = n \\quad \\Rightarrow \\frac{1}{n}\\sum_{i=1}^n y_i = \\lambda \\quad \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\bar{y}$$\n",
    "\n",
    "Hence, for a given set of data, the maximum likelihood estimate of $\\lambda$ is the sample mean of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090723",
   "metadata": {},
   "source": [
    "## Estimating the Parameters of a Normal Distribution\n",
    "<hr>\n",
    "Assume that we have $n$ sample points generated from a one-dimensional Gaussian distribution, and we would like to find the parameters of this distribution $\\mu$ and $\\sigma$.\n",
    "\n",
    "In this case, the likelihood of the parameters $\\mu$ and $\\sigma$ is given by the probability density function (PDF) of the normal distribution:\n",
    "\n",
    "$$\\mathcal{L}(\\mu, \\sigma \\mid X) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi} \\sigma} \\text{exp} \\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "Therefore, the log likelihood is:\n",
    "\n",
    "$$l(\\mu, \\sigma \\mid X) = \\sum_{i=1}^n \\text{log} \\left[\\frac{1}{\\sqrt{2\\pi} \\sigma} \\text{exp} \\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right) \\right] = n \\text{log} \\frac{1}{\\sqrt{2\\pi} \\sigma} - \\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{2\\sigma^2} = -\\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{2\\sigma^2} - n \\text{log}\\sigma - \\frac{n}{2} \\text{log}(2\\pi)$$\n",
    "\n",
    "To find the parameters $\\mu$ and $\\sigma$ that yield the maximum likelihood, we now take the partial derivatives of the log likelihood with respect to each one of them and set them to $0$.\n",
    "First, we take the partial derivative of the log likelihood with respect to $\\mu$:\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\mu}} = - \\sum_{i=1}^n \\frac{-2(x_i - \\mu)}{2\\sigma^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) = 0\n",
    "\\\\\n",
    "\\Rightarrow \\sum_{i=1}^n x_i - n\\mu = 0\n",
    "\\\\\n",
    "\\Rightarrow \\mu = \\frac{\\sum_{i=1}^n x_i}{n}$$\n",
    "\n",
    "As expected, we get that the best mean that describes our data is just the sample mean of the given data points!\n",
    "\n",
    "We now do the same for $\\sigma$:\n",
    "\n",
    "$$\\frac{\\partial{l}}{\\partial{\\sigma}} = - \\sum_{i=1}^n \\frac{-2(x_i - \\mu)^2}{2\\sigma^3} -\\frac{n}{\\sigma} = \\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{\\sigma^3} - \\frac{n}{\\sigma} = 0\n",
    "\\\\\n",
    "\\Rightarrow \\frac{1}{\\sigma^3} \\sum_{i=1}^n (x_i - \\mu)^2 = \\frac{n}{\\sigma}\n",
    "\\\\\n",
    "\\Rightarrow \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n}}$$\n",
    "\n",
    "Similarly, we get that the MLE of $\\sigma$ is just the standard deviation of our sample points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
