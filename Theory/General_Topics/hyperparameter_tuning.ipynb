{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac5a52",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Tuning\n",
    "- Purpose: Find the best hyperparameters for the model.\n",
    "- Note: Hyperparameters are not learned during training, but are set before training.\n",
    "\n",
    "### 1.1 Grid Search\n",
    "- Try all possible combinations of hyperparameters and choose the best one.\n",
    "- Challenges:\n",
    "    - Computationally expensive.\n",
    "    - Not suitable for continuous hyperparameters.\n",
    "\n",
    "### 1.2 Random Search\n",
    "- Try random combinations of hyperparameters and choose the best one.\n",
    "- Advantages:\n",
    "    - Computationally efficient.\n",
    "    - Suitable for continuous hyperparameters.\n",
    "\n",
    "### 1.3 Bayesian Optimization\n",
    "- Build a probabilistic model of the loss function and use it to choose the next hyperparameters to try.\n",
    "- Advantages:\n",
    "    - Computationally efficient.\n",
    "    - Suitable for continuous hyperparameters.\n",
    "    - Can handle noisy loss functions.\n",
    "    \n",
    "### 1.4 Tuning Process\n",
    "- **Grid vs. Random Sampling:** Instead of a grid, random sampling of hyperparameters is often more effective.\n",
    "- **Coarse to Fine Sampling:** When you find some hyperparameters values that give you a better performance - zoom into a smaller region around these values and sample more densely within this space.\n",
    "- **Scale for Sampling:** Use a logarithmic scale for searching hyperparameters instead of linear scale. For instance, a uniform random sample from the range $(0.0001, 1)$ might give us 90% of the values between $0.1$ and $1$. Therefore, using log-scale sampling:\n",
    "\n",
    "\\begin{align}\n",
    "0.0001 &= 10^{-4} \\rightarrow a=-4 \\\\\n",
    "1 &= 10^0 \\rightarrow b=0\\\\\n",
    "\\\\\n",
    "r &= (b-a) \\times np.random.rand() + b \\\\\n",
    "\\text{result} &= 10^r \\\\\n",
    "\\end{align}\n",
    "\n",
    "In the example, the range would be $[-4, 0]$. It uniformly samples values in log-scale from $[a,b]$.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6699f",
   "metadata": {},
   "source": [
    "# 2. Batch Normalization\n",
    "- Purpose: Normalize the inputs to each layer, so that the inputs to the activation function are not too large/small. It maintains a consistent distribution of inputs to each layer.\n",
    "\n",
    "$$Z_{norm}^{(i)} = \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "where $\\mu$ is the mean of the inputs to the layer, $\\sigma^2$ is the variance of the inputs to the layer, and $\\epsilon$ is a small number to avoid division by zero.\n",
    "- Placement: After FC (fully connected) or CONV (convolutional) layers, but before the activation function.\n",
    "- Note: For non-standard activations, like tanh, which might not want a unit gaussian input:\n",
    "- Updated formula: \n",
    "\n",
    "$$Z_{norm}^{(i)} = \\gamma \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are learnable parameters of the model.\n",
    "\n",
    "### 2.1 Batch Normalization Algorithm\n",
    "- Given: A mini-batch of $m$ examples.\n",
    "- Calculate: $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} Z^{(i)}$ and $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (Z^{(i)} - \\mu)^2$.\n",
    "- Normalize: $Z_{norm}^{(i)} = \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$.\n",
    "- Scale and shift: $Z_{norm}^{(i)} = \\gamma Z_{norm}^{(i)} + \\beta$, where $\\gamma$ and $\\beta$ are learnable parameters of the model.\n",
    "- Output: $Z^{(i)} = Z_{norm}^{(i)}$.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
