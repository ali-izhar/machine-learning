{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6858d32",
   "metadata": {},
   "source": [
    "# What is the Curse of Dimensionality?\n",
    "The curse of dimensionality refers to various problems that arise when working with high-dimensional data.\n",
    "\n",
    "## Living in a High-Dimensional Space\n",
    "We are so used to live in three dimensions, that our intuition fails when we try to imagine things in high-dimensional spaces. Many things behave very differently in high-dimensional spaces.\n",
    "\n",
    "For example, if we pick a random point in a unit square (a square whose side length is 1), its probability of being located less than $0.01$ from the border of the square is less than 2%. This is because the area of the unit square is 1, whereas the area of the inner square with side length $0.99$ is $0.99 \\times 0.99 = 0.9801$. Therefore, the probability that the point falls inside the area between the inner square and the unit square is $1 - 0.9801 = 0.0199$.\n",
    "\n",
    "On the other hand, if we pick a random point in a unit hypercube with 1,000 dimensions, its probability of being located less than 0.01 from the border of the hypercube is greater than 99.99%!\n",
    "\n",
    "The reason is that the volume of the unit hypercube is $1^{1000} = 1$, while the volume of the inner hypercube with side length of $0.99$ is $0.99^{1000} = 0.00004317$. Therefore, the probability that the point falls inside the volume between the inner hypercube and the unit hypercube is $1 - 0.00004317 = 0.99995683$.\n",
    "\n",
    "This means that high-dimensional spaces are very sparse, i.e., most of the data points lie very close to the border of the surface, and thus are very far from each other.\n",
    "\n",
    "## Nearest Neighbor Search\n",
    "This effect complicates nearest neighbor search in high-dimensional spaces, since the nearest neighbors in these spaces are not very near!\n",
    "\n",
    "Consider $k$-nearest neighbors in a data set of n points that are uniformly distributed inside a $d$-dimensional unit hypercube. Let the $k$-neighborhood of a point be the smallest hypercube that contains its $k$-nearest neighbors, and let $l$ be the average side length of such a neighborhood. Then the volume of the neighborhood (that contains $k$ points) is $l^d$, while the volume of the unit hypercube (that contains $n$ points) is 1. Therefore, on average $l^d = \\frac{k}{n}$, or:\n",
    "\n",
    "$$l = (\\frac{k}{n})^{1/d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274de5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b78e2ec8",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a powerful unsupervised learning algorithm used primarily for dimensionality reduction and visualization. In scenarios where datasets contain many features (dimensions) that make visualization challenging, PCA assists by compressing these features into fewer dimensions, ensuring the original data's essence remains intact.\n",
    "\n",
    "## Why PCA?\n",
    "- **Visualizing High-dimensional Data:** If your dataset comprises numerous features, such as 10, 50, or even thousands, visualizing it becomes a formidable task. PCA provides an elegant solution by reducing these features to just two or three, enabling easy plotting and visualization.\n",
    "\n",
    "- **Understanding Data Structure:** By projecting the data into a lower-dimensional space, PCA can often reveal hidden structures or patterns that might be harder to discern in the higher-dimensional space.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/2to1.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "## PCA Algorithm\n",
    "PCA is a linear dimensionality reduction technique that seeks to maximize the variance of the projected data. The algorithm works by identifying the hyperplane that lies closest to the data and then projecting the data onto it.\n",
    "\n",
    "## Choosing the Hyperplane (Axis of Projection or Principal Component)\n",
    "Consider the following data points in 2D space that we wish to project onto a single dimension. The red line represents the hyperplane that lies closest to the data.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/axis1.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "> This is not a bad choice, but it is not the best choice either. The hyperplane should be chosen such that it preserves the maximum amount of variance.\n",
    "\n",
    "In this case, the `hyperplane` or `principal component` is the line that passes through the origin and bisects the data, maximizing the variance of the projected data. The `principal component` is the direction along which the data varies the most. The `second principal component` is the direction orthogonal to the first principal component that accounts for the next highest variance, and so on.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/axis2.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "> This is the worst choice of hyperplane as it squishes the data points together, resulting in a loss of variance.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/axis2.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "> This is the best choice of hyperplane as it preserves the maximum amount of variance.\n",
    "\n",
    "## Projection onto the Hyperplane\n",
    "Once the hyperplane is chosen, the data is projected onto it. The distance of each data point from the hyperplane is called the `projection` or `loading` of the data point onto the hyperplane. The projection is the dot product of the data point and the unit vector along the hyperplane. The unit vector is the vector that has a magnitude of 1 and points in the direction of the hyperplane. For instance, $(x_1, x_2) = (2, 3)$ and the hyperplane $y = x$, the projection is the dot product of the data point and the unit vector (0.707, 0.707) along the hyperplane.\n",
    "\n",
    "$$ \\text{Projection} = (2, 3) \\cdot (0.707, 0.707) = 3.535 $$\n",
    "\n",
    "\n",
    "## Reconstruction of the Data\n",
    "The projected data can be reconstructed by reversing the process. The projected data is multiplied by the unit vector along the hyperplane and added to the mean of the original data. The result is the reconstructed data point. Notice that the reconstructed data point is not the same as the original data point but a close approximation.\n",
    "\n",
    "$$ \\text{Reconstruction} = \\text{Projection} \\cdot \\text{Unit Vector} + \\text{Mean} $$\n",
    "\n",
    "$$ (2, 3) \\approx 3.535 \\cdot (0.707, 0.707) + (0, 0) = (2.52, 2.52) $$\n",
    "\n",
    "## PCA is not Linear Regression\n",
    "PCA is often confused with linear regression. However, the two are very different. Linear regression is a supervised learning algorithm that seeks to minimize the error between the predicted and actual values. PCA is an unsupervised learning algorithm that seeks to maximize the variance of the projected data. PCA is not used for prediction but for visualization and dimensionality reduction.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/PCA.vs.LR.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## PCA in Scikit-learn\n",
    "1. Optional pre-processing: Perform feature scaling to standardize the dataset's features onto unit scale (mean = 0 and variance = 1) to ensure the PCA algorithm's effectiveness.\n",
    "2. \"fit\" the PCA algorithm to the data to obtain 2 (or 3) new axes (principal components) that maximize the variance of the projected data.\n",
    "3. Optionally examine how much variance (information) each principal component captures. This is often referred to as the explained variance ratio.\n",
    "4. Transform the original data onto the new axes.\n",
    "\n",
    "## Applications of PCA\n",
    "- **Visualizing High-dimensional Data:** PCA is often used to visualize high-dimensional data in 2D or 3D. For instance, PCA is used to visualize the MNIST dataset, which comprises 784 features (dimensions), in 2D or 3D.\n",
    "\n",
    "> **Less Frequently Used Applications**\n",
    "> - PCA is often used to speed up machine learning algorithms by reducing the number of features. For instance, PCA is used to reduce the number of features in the MNIST dataset from 784 to 154 while preserving 95% of the variance.\n",
    "> - PCA can be used to filter out noise from datasets. For instance, PCA is used to remove noise from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7ef7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965ac70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
