{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d503579",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "For a long time it was not clear how to train networks on a given data set. While single-layer perceptrons had a simple learning rule that was guaranteed to converge to a solution, it could not be extended to networks with more than one layer. The AI community has struggled with this problem for more than 30 years (in a period known as the \"AI winter\"), when eventually in 1986 Rumelhart et al. introduced the **backpropagation algorithm** in their groundbreaking [paper](https://www.nature.com/articles/323533a0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378b6af",
   "metadata": {},
   "source": [
    "## The Three Phases of the Algorithm\n",
    "The backpropagation algorithm consists of three phases:\n",
    "- **Forward pass:** In this phase we feed the inputs through the network, make a prediction and measure its error with respect to the true label.\n",
    "- **Backward pass:** We propagate the gradients of the error with respect to each one of the weights backward from the output layer to the input layer.\n",
    "- **Gradient descent step:** We slightly tweak the connection weights in the network by taking a step in the opposite direction of the error gradients.\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/mlp.png\" width=800>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f0cdf",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Forward Pass\n",
    "In the forward pass, we propagate the inputs in a forward direction, layer-by-layer, until the output is generated. The activation of neuron $i$ in layer $l$ is computed using the following equation:\n",
    "\n",
    "$$a_i^{l} = f(z_i^{l}) = f(\\sum_{j} w_{ij}^{l} a_j^{l-1} + b_i^{l})$$\n",
    "\n",
    "where $f$ is the activation function, $z_i^{l}$ is the net input of neuron $i$ in layer $l$, $w_{ij}^{l}$ is the connection weight between neuron $j$ in layer $l - 1$ and neuron $i$ in layer $l$, and $b_i^{l}$ is the bias of neuron $i$ in layer $l$.\n",
    "\n",
    "To simplify the derivation of the learning algorithm, we will treat the bias as if it were the weight $w_0$ of an input neuron $x_0$ that has a constant value of $1$. This enables us to write the above equation as follows:\n",
    "\n",
    "$$a_i^{l} = f(z_i^{l}) = f(\\sum_{j} w_{ij}^{l} a_j^{l-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d10b1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Backward Pass\n",
    "In the backward pass we propagate the gradients of the error from the output layer back to the input layer.\n",
    "\n",
    "### Definition of the Error and Loss Functions\n",
    "We first define the error of the network on the training set with respect to its weights. Let’s denote by $w$ the vector that contains all the weights of the network. Assume that we have $n$ training samples ${(x_i, y_i)}, i = 1, \\cdots,n$, and the output of the network on sample $i$ is $o_i$. Then the error of the network with respect to $w$ is:\n",
    "\n",
    "$$E(w) = \\sum_{i=1}^n J(y_i, o_i)$$\n",
    "\n",
    "where $J(y, o)$ is the loss function. The specific loss function that we use depends on the task the network is trying to accomplish:\n",
    "\n",
    "1. For regression problems, we use the **squared loss** function:\n",
    "\n",
    "$$J(y, o) = (y - o)^2$$\n",
    "\n",
    "2. For binary classification problems, we use **log loss** (also known as the binary cross-entropy loss):\n",
    "\n",
    "$$J(y, o) = -y \\log{o} - (1 - y) \\log{1 - o}$$\n",
    "\n",
    "3. For multi-class classification problems, we use the **cross-entropy** loss function:\n",
    "\n",
    "$$J_{CE} (y, o) = - \\sum_{i=1}^k y_i \\log{o_i}$$\n",
    "\n",
    "where $k$ is the number of classes.\n",
    "\n",
    "Our goal is to find the weights $w$ that minimize $E(w).$ Unfortunately, this function is non-convex because of the non-linear activations of the hidden neurons. This means that it may have multiple local minima:\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/non-convex.png\" width=500>\n",
    "</div>\n",
    "\n",
    "### Finding the Gradients of the Error\n",
    "In order to use gradient descent, we need to compute the partial derivatives of $E(w)$ with respect to each one of the weights in the network:\n",
    "\n",
    "$$\\frac{\\partial{E}}{\\partial{w_{ij}^{l}}}$$\n",
    "\n",
    "To simplify the mathematical derivation, we will assume that we have only one training example and find the partial derivatives of the error with respect to that example:\n",
    "\n",
    "$$\\frac{\\partial{E}}{\\partial{w_{ij}^{l}}} = \\frac{\\partial{J(y, o)}}{\\partial{w_{ij}^{l}}}$$\n",
    "\n",
    "where $y$ is the label of this example and $o$ is the output of the network for that example. The extension to $n$ training samples is straightforward, since the derivative of the sum of functions is just the sum of their derivatives.\n",
    "\n",
    "The computation of the partial derivatives of the weights in the hidden layers is not trivial, since those weights don’t affect directly the output (and hence the error). To address this problem, we will use the chain rule of derivatives to establish a relationship between the gradients of the error in a given layer and the gradients in the subsequent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929feda1",
   "metadata": {},
   "source": [
    "### The Delta Terms\n",
    "We first note that $E$ depends on the weight $w_{ij}^{l}$ only via the net input $z_i^{l}$ of neuron $i$ in layer $l$. Therefore, we can apply the chain rule of derivatives to the gradient of $E$ with respect to this weight:\n",
    "\n",
    "$$\\frac{\\partial{E}}{\\partial{w_{ij}^{l}}} = \\frac{\\partial{J(y, o)}}{\\partial{z_i^{l}}} \\times \\frac{\\partial{z_i^{l}}}{\\partial{w_{ij}^{l}}}$$\n",
    "\n",
    "The second derivative on the right side of the equation is:\n",
    "\n",
    "$$\\frac{\\partial{z_i^{l}}}{\\partial{w_{ij}^{l}}} = \\frac{\\partial{\\sum_{k} w_{ij}^{l} a_k^{l-1}}}{\\partial{w_{ij}^{l}}} = a_j^{l-1}$$\n",
    "\n",
    "Therefore, we can write:\n",
    "\n",
    "$$\\frac{\\partial{E}}{\\partial{w_{ij}^{l}}} = \\frac{\\partial{J(y, o)}}{\\partial{z_i^{l}}} a_j^{l-1} = \\delta_i^{l} a_j^{l-1}$$\n",
    "\n",
    "The variable $\\delta_i$ is called the delta term of neuron $i$ or delta for short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e8a3b",
   "metadata": {},
   "source": [
    "### The Delta Rule\n",
    "The delta rule establishes the relationship between the delta terms in layer $l$ and the delta terms in layer $l + 1$. To derive the delta rule, we again use the chain rule of derivatives. The loss function depends on the net input of neuron $i$ only via the net inputs of all the neurons it is connected to in layer $l + 1$. Therefore we can write:\n",
    "\n",
    "$$\\delta_i^{l} = \\frac{\\partial{J(y,o)}}{\\partial{z_i^{l}}} = \\sum_{j} (\\frac{\\partial{J(y,o)}}{\\partial{z_j^{l+1}}} \\frac{\\partial{z_j^{l+1}}}{\\partial{z_i^{l}}})$$\n",
    "\n",
    "where the index $j$ in the sum goes over all the neurons in layer $l + 1$ that neuron $i$ in layer $l$ is connected to.\n",
    "\n",
    "Once again we use the chain rule to decompose the second partial derivative inside the brackets:\n",
    "\n",
    "$$\\delta_i^{l} = \\sum_{j} (\\frac{\\partial{J(y,o)}}{\\partial{z_j^{l+1}}} \\frac{\\partial{z_j^{l+1}}}{\\partial{a_i^{l}}} \\frac{\\partial{a_i^{l}}}{\\partial{z_i^{l}}})$$\n",
    "\n",
    "The first partial derivative inside the brackets is just the delta of neuron $j$ in layer $l + 1$, therefore we can write:\n",
    "\n",
    "$$\\delta_i^{l} = \\sum_{j} (\\delta_j^{l+1} \\frac{\\partial{z_j^{l+1}}}{\\partial{a_i^{l}}} \\frac{\\partial{a_i^{l}}}{\\partial{z_i^{l}}})$$\n",
    "\n",
    "The second partial derivative is easy to compute:\n",
    "\n",
    "$$\\frac{\\partial{z_j^{l+1}}}{\\partial{a_i^{l}}} = \\frac{\\partial{(\\sum_{k} w_{jk}^{l+1} a_k^{l})}}{\\partial{a_i^{l}}} = w_{ji}^{l+1}$$\n",
    "\n",
    "Therefore, we get:\n",
    "\n",
    "$$\\delta_i^{l} = \\sum_{j} (\\delta_j^{l+1} w_{ji}^{l+1} \\frac{\\partial{a_i^{l}}}{\\partial{z_i^{l}}}) = \\frac{\\partial{a_i^{l}}}{\\partial{z_i^{l}}} \\sum_{j} (w_{ji}^{l+1} \\delta_j^{l+1})$$\n",
    "\n",
    "But $a_i^{l} = f(z_i^{l})$, where $f$ is the activation function. Hence, the partial derivative outside the sum is just the derivative of the activation function $f'(x)$ for $x = z_i^{l}$.\n",
    "\n",
    "Therefore we can write:\n",
    "$$\\delta_i^{l} = f'(z_i^{l}) \\sum_{j} (w_{ji}^{l+1} \\delta_{j}^{l+1})$$\n",
    "\n",
    "This equation, known as the **delta rule,** shows the relationship between the deltas in layer $l$ and the deltas in layer $l + 1.$ More specifically, each delta in layer $l$ is a linear combination of the deltas in layer $l + 1$, where the coefficients of the combination are the connection weights between these layers. The delta rule allows us to compute all the delta terms (and thus all the gradients of the error) recursively, starting from the deltas in the output layer and going back layer-by-layer until we reach the input layer.\n",
    "\n",
    "The following diagram illustrates the flow of the error information:\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/error_flow.png\" width=500>\n",
    "</div>\n",
    "\n",
    "For specific activation functions, we can derive more explicit equations for the delta rule. For example, if we use the sigmoid function then:\n",
    "\n",
    "$$a_i^{l} = \\sigma(z_i^{l}) = \\frac{1}{1+e^{-z_i^{l}}}$$\n",
    "\n",
    "The derivative of the sigmoid function has a simple form:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\\sigma'(z_i^{l}) = \\sigma(z_i^{l})(1-\\sigma(z_i^{l})) = a_i^{l}(1-a_i^{l})$$\n",
    "\n",
    "Then the delta rule for the sigmoid function gets the following form:\n",
    "\n",
    "$$\\delta_i^{l} = a_i^{l}(1-a_i^{l}) \\sum_{j} (w_{ji}^{l+1} \\delta_j^{l+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03feb6",
   "metadata": {},
   "source": [
    "### The Deltas in the Output Layer\n",
    "The final piece of the puzzle are the delta terms in the output layer, which are the first ones that we need to compute.\n",
    "\n",
    "The deltas in the output layer depend both on the loss function and the activation function used in the output neurons:\n",
    "\n",
    "$$\\delta^{L} = \\frac{\\partial{J(y,o)}}{\\partial{z^{L}}} = \\frac{\\partial{J(y,o)}}{\\partial{o}} \\frac{\\partial{o}}{\\partial{z^{L}}} = \\frac{\\partial{J(y,o)}}{\\partial{o}} \\frac{\\partial{f(z^{L}}}{\\partial{z^{L}}} = \\frac{\\partial{J(y,o)}}{\\partial{o}} f'(z^{L})$$\n",
    "\n",
    "where $f$ is the activation function used to compute the output.\n",
    "\n",
    "Let's now derive more specific delta terms for each type of learning task:\n",
    "\n",
    "1. In regression problems, the activation function we use in the output is the identity function $f(x) = x$, whose derivative is $1$, and the loss function is the squared loss. Therefore the delta is:\n",
    "\n",
    "$$\\delta^{L} = \\frac{\\partial{(y-o)^2}}{\\partial{o}} = -2(y-o) = 2(o-y)$$\n",
    "\n",
    "2. In binary classification problems, the activation function we use is sigmoid and the loss function is log loss, therefore we get:\n",
    "\n",
    "$$\\delta^{L} = \\frac{\\partial{-y \\log{o} - (1-y) \\log{1-o}}}{\\partial{o}} \\sigma'(z^{L})$$\n",
    "\n",
    "$$ = (- \\frac{y}{o} + \\frac{1-y}{1-o}) \\sigma(z^{L}) (1-\\sigma(z^{L}))$$\n",
    "\n",
    "$$ = (- \\frac{y}{o} + \\frac{1-y}{1-o}) o(1-o) = -y(1-o)+(1-y)o = o-y$$\n",
    "\n",
    "3. In multiclass classification problems, we have $k$ output neurons (where $k$ is the number of classes) and we use softmax activation and the cross-entropy log loss. Similar to the previous case, the delta term of the $i$th output neuron is surprisingly simple:\n",
    "\n",
    "$$\\delta_i^{L} = o_i - y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaaafeb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Gradient Descent\n",
    "Once we finish computing all the delta terms, we can use gradient descent to update the weights. In gradient descent, we take small steps in the opposite direction of the gradient (i.e., in the direction of the steepest descent) in order to get closer to the minimum error:\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/gradient_descent.png\" width=500>\n",
    "</div>\n",
    "\n",
    "Remember that the partial derivative of the error function with respect to each weight is:\n",
    "\n",
    "$$\\frac{\\partial{E}}{\\partial{w_{ij}^{j}}} = \\delta_i^{l} a_j^{l-1}$$\n",
    "\n",
    "Therefore, we can write the gradient descent update rule as follows:\n",
    "\n",
    "$$w_{ij}^{l} \\leftarrow w{ij}^{l} - \\alpha \\delta_i^{l} a_j^{l-1}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
