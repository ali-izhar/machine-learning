import numpy as np

"""
ExploitationOnlyAgent Module
----------------------------

This module implements an exploitation-only strategy for the Multi-Armed Bandit problem.
The agent sequentially tests each action until finding a success, then exploits (repeats)
that successful action for all remaining trials.

Strategy Overview:
-----------------
Consider 4 slot machines with different winning probabilities:
    Machine 1: 50% (0.5)
    Machine 2: 70% (0.7)
    Machine 3: 30% (0.3)
    Machine 4: 40% (0.4)

The agent:
1. Tests machines in sequence (1→2→3→4→1→2→...)
2. Once a machine wins, keeps using that machine
3. If no machine succeeds, continues the cycle

Key Concepts:
------------
For any trial k:
- m_k = k mod 4: Current position in cycle (which machine we're testing)
- q_k = floor(k/4): Number of complete cycles finished
- k = m_k + 4q_k: Relationship between trial, position, and cycles

Probability Calculation Examples:
------------------------------
Example 1 (Mid-Cycle Case) - For trial k = 7:

1. Position Analysis:
   - q_k = floor(7/4) = 1 complete cycle
   - m_k = 7 mod 4 = 3 (Machine 3)
   - We've tried: M1→M2→M3→M4→M1→M2→M3

2. Failure History:
   - Machine 1: Failed twice (cycle + current sequence)
   - Machine 2: Failed twice (cycle + current sequence)
   - Machine 3: Failed once (in cycle)
   - Machine 4: Failed once (in cycle)

3. Probability Calculation:
   P(first success at k=7) = 
   - P(M1 failed twice)    x (1-0.5)^2 = 0.25
   - P(M2 failed twice)    x (1-0.7)^2 = 0.09
   - P(M3 failed in cycle) x (1-0.3)^1 = 0.70
   - P(M4 failed in cycle) x (1-0.4)^1 = 0.60
   - P(M3 succeeds now)    x 0.3
   = 0.00945 x 0.3 ≈ 0.002835

4. Expected Reward if Success:
   - Immediate reward: 1
   - Future rewards: (remaining_trials) x 0.3
   Because we'll keep using Machine 3 with 30% success rate

Example 2 (Complete Cycles Case) - For trial k = 36:

1. Position Analysis:
   - q_k = floor(36/4) = 9 complete cycles
   - m_k = 36 mod 4 = 0 (Machine 4)
   - We've tried: (M1→M2→M3→M4) x 9 times + current M4

2. Failure History:
   - Machine 1: Failed 9 times (in complete cycles)
   - Machine 2: Failed 9 times (in complete cycles)
   - Machine 3: Failed 9 times (in complete cycles)
   - Machine 4: Failed 8 times (in cycles, currently testing 9th attempt)
   
3. Probability Calculation:
   P(first success at k=36) = 
   - P(M1 failed 9 times)    x (1-0.5)^9  = 0.5^9    ≈ 0.001953125
   - P(M2 failed 9 times)    x (1-0.7)^9  = 0.3^9    ≈ 0.000001968
   - P(M3 failed 9 times)    x (1-0.3)^9  = 0.7^9    ≈ 0.040353607
   - P(M4 failed 8 times)    x (1-0.4)^8  = 0.6^8    ≈ 0.016796160
   - P(M4 succeeds now)      x 0.4
   = (0.001953125 x 0.000001968 x 0.040353607 x 0.016796160) x 0.4
   ≈ 0.00000000000022 x 0.4
   ≈ 8.8e-14

4. Expected Reward if Success:
   - Immediate reward: 1
   - Future rewards: (100-36) x 0.4
   Because we'll keep using Machine 4 with 40% success rate

Key Observations from k=36 Example:
1. With complete cycles, the pattern of failures is more regular
2. Machine 4 has one less failure count (q_k-1) because we're currently testing it
3. The probability becomes extremely small due to many accumulated failures
4. The expected future rewards use Machine 4's probability (40%)

This example illustrates why the adjustment factor a_4 = -1 when m_k = 0,
as we need to account for Machine 4 having one less failure than the other
machines when we're currently testing it at the end of a cycle.

The theoretical analysis shows this strategy achieves better performance
(regret = 13.9936) compared to pure exploration (regret = 22.5).
"""


class ExploitationOnlyAgent:
    """
    An agent that implements an exploitation-only strategy for the Multi-Armed Bandit problem.
    The agent sequentially tests each action until it finds one that yields a success; from that
    point onward, it exploits (repeats) the successful action for all remaining trials.
    """

    def __init__(
        self,
        num_actions: int,
        num_trials: int,
        action_probabilities: list,
        reward_value: float = 1.0,
    ):
        """
        Initialize the exploitation-only agent with the specified parameters.

        Args:
            num_actions (int): Number of available actions (bandit arms).
            num_trials (int): Total number of trials (attempts) in an episode.
            action_probabilities (list): List of success probabilities for each action.
            reward_value (float): Reward received on a successful action (default is 1.0).
        """
        self.num_actions = num_actions
        self.num_trials = num_trials
        self.action_probabilities = action_probabilities
        self.reward_value = reward_value

        # Calculate the maximum achievable reward (optimal strategy)
        # by always choosing the action with the highest probability.
        self.optimal_return = (
            max(self.action_probabilities) * self.reward_value * self.num_trials
        )

    def get_action_probability(self, trial: int) -> float:
        """
        Determine the success probability of the action to be tried at the given trial.

        This function maps the trial number (using 1-based indexing) to a specific action based on
        a fixed cyclic order: actions are tried in a predetermined sequence. The mapping is defined
        such that for remainders [1, 2, 3, 0] when the trial number is divided by the number of actions,
        the corresponding action probabilities are [p1, p2, p3, p4].

        Args:
            trial (int): The current trial number (1-based indexing).

        Returns:
            float: The success probability associated with the action for this trial.
        """
        # Determine the remainder when the trial number is divided by the number of actions.
        # This gives a cyclic ordering.
        r_k = trial % self.num_actions

        # If the remainder is 0, select the fourth action's probability.
        if r_k == 0:
            return self.action_probabilities[3]  # Corresponds to p4
        else:
            # For remainders 1, 2, or 3, select the corresponding action (p1, p2, or p3).
            return self.action_probabilities[r_k - 1]

    def run_episode(self) -> float:
        """
        Execute one episode using the exploitation-only strategy.

        The agent sequentially tests actions until it finds a successful one. Once a success is achieved,
        the agent exploits that action for all subsequent trials.

        Returns:
            float: The total accumulated reward from the episode.
        """
        episode_return = 0.0  # Initialize the accumulated reward for the episode.
        first_success = False  # Flag to indicate if a successful action has been found.

        # Stores the probability of the successful action once found.
        successful_prob = None

        # Loop over each trial in the episode.
        for k in range(self.num_trials):
            if not first_success:
                # Still searching for the first successful action.
                # Use (k+1) to convert 0-based index to 1-based for our mapping.
                p_k = self.get_action_probability(k + 1)

                # Check if the selected action is successful.
                if np.random.random() < p_k:
                    first_success = True  # Mark that a success has been found.
                    # Save the probability of the successful action.
                    successful_prob = p_k
                    # Add reward for this successful trial.
                    episode_return += self.reward_value
            else:
                # Once a successful action is found, continue exploiting it.
                if np.random.random() < successful_prob:
                    episode_return += self.reward_value

        return episode_return

    def calculate_theoretical_return(self) -> float:
        """
        Compute the theoretical expected return using probability theory.

        For each trial, the function estimates the probability of achieving a success at that trial,
        then calculates the expected reward from that trial (accounting for future rewards if the success
        occurs, which triggers exploitation of that action).

        Returns:
            float: The theoretical expected total return for the episode.
        """
        total_return = 0.0  # Accumulator for the total expected return

        # Loop over each trial using 1-based indexing.
        for k in range(1, self.num_trials + 1):
            # Calculate q_k: the number of complete cycles (each cycle covers all actions)
            q_k = (k - 1) // self.num_actions

            # Determine the current trial's position in the cycle (remainder).
            r_k = k % self.num_actions

            # Depending on the remainder, determine the success probability p_k for this trial.
            # Also set adjustment factors (a_1, a_2, a_4) that help calculate the probability
            # that no success occurred in previous trials.
            if r_k == 0:
                p_k = self.action_probabilities[3]  # p4
                a_1, a_2, a_4 = 0, 0, -1
            elif r_k == 1:
                p_k = self.action_probabilities[0]  # p1
                a_1, a_2, a_4 = 0, 0, 0
            elif r_k == 2:
                p_k = self.action_probabilities[1]  # p2
                a_1, a_2, a_4 = 1, 0, 0
            else:  # r_k == 3
                p_k = self.action_probabilities[2]  # p3
                a_1, a_2, a_4 = 1, 1, 0

            # Expected reward if success occurs at trial k:
            # 1 reward at trial k plus rewards for all remaining trials using the successful action.
            expected_reward_k = 1 + (self.num_trials - k) * p_k

            # Extract individual action probabilities for clarity.
            p1, p2, p3, p4 = self.action_probabilities

            # Compute the probability that no success was achieved in all previous trials,
            # using the adjustment factors for each action.
            prob_factors = (
                (1 - p1) ** (q_k + a_1)
                * (1 - p2) ** (q_k + a_2)
                * (1 - p3) ** q_k
                * (1 - p4) ** (q_k + a_4)
            )

            # The expected return contribution from trial k is the probability that the first success
            # occurs exactly at trial k times the expected reward if that success happens.
            return_k = prob_factors * p_k * expected_reward_k

            # Accumulate the expected return.
            total_return += return_k

        return total_return

    def calculate_regret(self, actual_return: float) -> float:
        """
        Compute the regret relative to the optimal strategy.

        Regret is defined as the difference between the optimal achievable return (if the best action was
        used in every trial) and the actual return achieved by the agent.

        Args:
            actual_return (float): The return obtained by the agent.

        Returns:
            float: The regret value.
        """
        return self.optimal_return - actual_return

    def run_multiple_episodes(self, num_episodes: int) -> tuple[float, float]:
        """
        Execute a specified number of episodes and compute the average return and regret.

        Args:
            num_episodes (int): Number of episodes to simulate.

        Returns:
            tuple[float, float]: A tuple containing:
                - average_return: Mean reward obtained over the episodes.
                - average_regret: Regret computed from the average return.
        """
        total_return = 0.0  # Accumulate total return across episodes

        # Run each episode and sum the returns.
        for _ in range(num_episodes):
            episode_return = self.run_episode()
            total_return += episode_return

        # Compute the average return.
        average_return = total_return / num_episodes
        # Compute the corresponding average regret.
        average_regret = self.calculate_regret(average_return)

        return average_return, average_regret


if __name__ == "__main__":
    # Initialize the exploitation-only agent with 4 actions, 100 trials per episode,
    # and the given success probabilities for each action.
    agent = ExploitationOnlyAgent(
        num_actions=4,
        num_trials=100,
        action_probabilities=[0.5, 0.7, 0.3, 0.4],
        reward_value=1.0,
    )

    # Run a single episode and print its return and regret.
    single_return = agent.run_episode()
    single_regret = agent.calculate_regret(single_return)
    print(f"Single Episode - Return: {single_return:.2f}, Regret: {single_regret:.2f}")

    # Calculate and print the theoretical expected return and corresponding regret.
    theo_return = agent.calculate_theoretical_return()
    theo_regret = agent.calculate_regret(theo_return)
    print(f"\nTheoretical values:")
    print(f"Return: {theo_return:.2f}")
    print(f"Regret: {theo_regret:.2f}")

    # Run multiple episodes (e.g., 10,000) and compute average performance metrics.
    num_episodes = 10000
    avg_return, avg_regret = agent.run_multiple_episodes(num_episodes=num_episodes)
    print(f"\nAverage over {num_episodes} episodes:")
    print(f"Expected Return: {avg_return:.2f} (theoretical: {theo_return:.2f})")
    print(f"Average Regret: {avg_regret:.2f} (theoretical: {theo_regret:.2f})")
