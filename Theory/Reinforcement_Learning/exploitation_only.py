import numpy as np


class ExploitationOnlyAgent:
    """
    An agent that implements an exploitation-only strategy for the Multi-Armed Bandit problem.
    The agent sequentially tests each action until it finds one that yields a success; from that
    point onward, it exploits (repeats) the successful action for all remaining trials.
    """

    def __init__(
        self,
        num_actions: int,
        num_trials: int,
        action_probabilities: list,
        reward_value: float = 1.0,
    ):
        """
        Initialize the exploitation-only agent with the specified parameters.

        Args:
            num_actions (int): Number of available actions (bandit arms).
            num_trials (int): Total number of trials (attempts) in an episode.
            action_probabilities (list): List of success probabilities for each action.
            reward_value (float): Reward received on a successful action (default is 1.0).
        """
        self.num_actions = num_actions
        self.num_trials = num_trials
        self.action_probabilities = action_probabilities
        self.reward_value = reward_value

        # Calculate the maximum achievable reward (optimal strategy)
        # by always choosing the action with the highest probability.
        self.optimal_return = (
            max(self.action_probabilities) * self.reward_value * self.num_trials
        )

    def get_action_probability(self, trial: int) -> float:
        """
        Determine the success probability of the action to be tried at the given trial.

        This function maps the trial number (using 1-based indexing) to a specific action based on
        a fixed cyclic order: actions are tried in a predetermined sequence. The mapping is defined
        such that for remainders [1, 2, 3, 0] when the trial number is divided by the number of actions,
        the corresponding action probabilities are [p1, p2, p3, p4].

        Args:
            trial (int): The current trial number (1-based indexing).

        Returns:
            float: The success probability associated with the action for this trial.
        """
        # Determine the remainder when the trial number is divided by the number of actions.
        # This gives a cyclic ordering.
        r_k = trial % self.num_actions

        # If the remainder is 0, select the fourth action's probability.
        if r_k == 0:
            return self.action_probabilities[3]  # Corresponds to p4
        else:
            # For remainders 1, 2, or 3, select the corresponding action (p1, p2, or p3).
            return self.action_probabilities[r_k - 1]

    def run_episode(self) -> float:
        """
        Execute one episode using the exploitation-only strategy.

        The agent sequentially tests actions until it finds a successful one. Once a success is achieved,
        the agent exploits that action for all subsequent trials.

        Returns:
            float: The total accumulated reward from the episode.
        """
        episode_return = 0.0  # Initialize the accumulated reward for the episode.
        first_success = False  # Flag to indicate if a successful action has been found.

        # Stores the probability of the successful action once found.
        successful_prob = None

        # Loop over each trial in the episode.
        for k in range(self.num_trials):
            if not first_success:
                # Still searching for the first successful action.
                # Use (k+1) to convert 0-based index to 1-based for our mapping.
                p_k = self.get_action_probability(k + 1)

                # Check if the selected action is successful.
                if np.random.random() < p_k:
                    first_success = True  # Mark that a success has been found.

                    # Save the probability of the successful action.
                    successful_prob = p_k

                    # Add reward for this successful trial.
                    episode_return += self.reward_value
            else:
                # Once a successful action is found, continue exploiting it.
                if np.random.random() < successful_prob:
                    episode_return += self.reward_value

        return episode_return

    def calculate_theoretical_return(self) -> float:
        """
        Compute the theoretical expected return using probability theory.

        For each trial, the function estimates the probability of achieving a success at that trial,
        then calculates the expected reward from that trial (accounting for future rewards if the success
        occurs, which triggers exploitation of that action).

        Returns:
            float: The theoretical expected total return for the episode.
        """
        total_return = 0.0  # Accumulator for the total expected return

        # Loop over each trial using 1-based indexing.
        for k in range(1, self.num_trials + 1):
            # Calculate q_k: the number of complete cycles (each cycle covers all actions)
            q_k = (k - 1) // self.num_actions

            # Determine the current trial's position in the cycle (remainder).
            r_k = k % self.num_actions

            # Depending on the remainder, determine the success probability p_k for this trial.
            # Also set adjustment factors (a_1, a_2, a_4) that help calculate the probability
            # that no success occurred in previous trials.
            if r_k == 0:
                p_k = self.action_probabilities[3]  # p4
                a_1, a_2, a_4 = 0, 0, -1
            elif r_k == 1:
                p_k = self.action_probabilities[0]  # p1
                a_1, a_2, a_4 = 0, 0, 0
            elif r_k == 2:
                p_k = self.action_probabilities[1]  # p2
                a_1, a_2, a_4 = 1, 0, 0
            else:  # r_k == 3
                p_k = self.action_probabilities[2]  # p3
                a_1, a_2, a_4 = 1, 1, 0

            # Expected reward if success occurs at trial k:
            # 1 reward at trial k plus rewards for all remaining trials using the successful action.
            expected_reward_k = 1 + (self.num_trials - k) * p_k

            # Extract individual action probabilities for clarity.
            p1, p2, p3, p4 = self.action_probabilities

            # Compute the probability that no success was achieved in all previous trials,
            # using the adjustment factors for each action.
            prob_factors = (
                (1 - p1) ** (q_k + a_1)
                * (1 - p2) ** (q_k + a_2)
                * (1 - p3) ** q_k
                * (1 - p4) ** (q_k + a_4)
            )

            # The expected return contribution from trial k is the probability that the first success
            # occurs exactly at trial k times the expected reward if that success happens.
            return_k = prob_factors * p_k * expected_reward_k

            # Accumulate the expected return.
            total_return += return_k

        return total_return

    def calculate_regret(self, actual_return: float) -> float:
        """
        Compute the regret relative to the optimal strategy.

        Regret is defined as the difference between the optimal achievable return (if the best action was
        used in every trial) and the actual return achieved by the agent.

        Args:
            actual_return (float): The return obtained by the agent.

        Returns:
            float: The regret value.
        """
        return self.optimal_return - actual_return

    def run_multiple_episodes(self, num_episodes: int) -> tuple[float, float]:
        """
        Execute a specified number of episodes and compute the average return and regret.

        Args:
            num_episodes (int): Number of episodes to simulate.

        Returns:
            tuple[float, float]: A tuple containing:
                - average_return: Mean reward obtained over the episodes.
                - average_regret: Regret computed from the average return.
        """
        total_return = 0.0  # Accumulate total return across episodes

        # Run each episode and sum the returns.
        for _ in range(num_episodes):
            episode_return = self.run_episode()
            total_return += episode_return

        # Compute the average return.
        average_return = total_return / num_episodes
        # Compute the corresponding average regret.
        average_regret = self.calculate_regret(average_return)

        return average_return, average_regret


if __name__ == "__main__":
    # Initialize the exploitation-only agent with 4 actions, 100 trials per episode,
    # and the given success probabilities for each action.
    agent = ExploitationOnlyAgent(
        num_actions=4,
        num_trials=100,
        action_probabilities=[0.5, 0.7, 0.3, 0.4],
        reward_value=1.0,
    )

    # Run a single episode and print its return and regret.
    single_return = agent.run_episode()
    single_regret = agent.calculate_regret(single_return)
    print(f"Single Episode - Return: {single_return:.2f}, Regret: {single_regret:.2f}")

    # Calculate and print the theoretical expected return and corresponding regret.
    theo_return = agent.calculate_theoretical_return()
    theo_regret = agent.calculate_regret(theo_return)
    print(f"\nTheoretical values:")
    print(f"Return: {theo_return:.2f}")
    print(f"Regret: {theo_regret:.2f}")

    # Run multiple episodes (e.g., 10,000) and compute average performance metrics.
    num_episodes = 10000
    avg_return, avg_regret = agent.run_multiple_episodes(num_episodes=num_episodes)
    print(f"\nAverage over {num_episodes} episodes:")
    print(f"Expected Return: {avg_return:.2f} (theoretical: {theo_return:.2f})")
    print(f"Average Regret: {avg_regret:.2f} (theoretical: {theo_regret:.2f})")
