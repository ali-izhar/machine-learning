{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8916a7",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Regularization is a widespread technique in machine learning, which is used to control the complexity of the machine learning model and thereby improve its generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4e290",
   "metadata": {},
   "source": [
    "## What is Regularization?\n",
    "Regularization is a technique to prevent overfitting by penalizing complex models. The idea is to add a penalty term to the cost function of the model, such that it becomes dependent on two factors:\n",
    "\n",
    "$$\\text{Cost}(h) = \\text{Training Error}(h) + \\lambda \\text{Complexity}(h)$$\n",
    "\n",
    "$\\lambda$ is a hyperparameter (called the regularization coefficient) that controls the tradeoff between the bias and the variance. Higher $\\lambda$ will induce a larger penalty on the complexity of the model, and thus will lead to simpler models with higher error on the training set but with smaller variance.\n",
    "\n",
    "The complexity of the model can be measured in a variety of ways. For example, in models that consist of a vector of parameters (weights) $w$, such as linear regression or neural networks, we use the size of the parameters (the norm of the vector $w$) as a measure for the modelâ€™s complexity. In such models, there are two common types of regularization, depending on the norm of the vector $w$ that we are using:\n",
    "\n",
    "1. **L1 regularization.** In this case, we use the $L1$ norm of the vector $w$, i.e., the sum of the absolute values of the weights. For example, in linear regression, if we have $m$ features in our data set, then the model will have $m$ parameters (weights) plus a bias term, thus we can write the $L1$ norm of $w$ as:\n",
    "\n",
    "$$||W||_1 = |w_0| + |w_1| + \\cdots + |w_m|$$\n",
    "\n",
    "2. **L2 regularization.** In this case, we use the $L2$ norm of the vector $w$ (squared), i.e., the sum of the squares of the weights:\n",
    "\n",
    "$$||w||_2 ^2 = w_0 ^2 + w_1 ^2 + \\cdots + w_m ^2 $$\n",
    "\n",
    "In general, $L1$ regularization is a stronger form of regularization than $L2.$ In $L1$ regularization, the rate at which the weights drop to 0 is constant (since the gradient of $|w_j|$ is 1), while in $L2$ regularization, the rate becomes slower as the weights approach 0 (since the gradient of $w_j^2$ is $2w_j$). Hence, $L1$ is more likely to zero out some of the weights, effectively removing their associated features from the model.\n",
    "\n",
    "Normally, the bias (intercept) $w_0$ is not regularized, since penalizing the model based on the intercept value can have a dramatic effect on the resulting model. For example, in linear regression, changing $w_0$ shifts the regression hyperplane closer or farther from the origin along the dimension of the target variable $y$, while setting $w_0$ to exactly 0 forces the hyperplane to go through the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e556d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c165bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
