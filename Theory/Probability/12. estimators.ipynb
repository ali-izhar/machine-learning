{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e424182",
   "metadata": {},
   "source": [
    "# 12. Estimators, Properties of Point Estimators and Methods of Estimation\n",
    "<hr>\n",
    "\n",
    "An **estimator** is a rule, often expressed as a formula, that tells us how to calculate the value of an estimate based on the measurements contained in a sample.\n",
    "\n",
    "Let $\\hat{\\theta}$ be a point estimate for a parameter $\\theta$. Then $\\hat{\\theta}$ is an unbiased estimator if $E(\\hat{\\theta})=\\theta$. If $E(\\hat{\\theta}) \\neq \\theta$, then $\\hat{\\theta}$ is said to be biased.\n",
    "\n",
    "The **bias** of a point estimator $\\hat{\\theta}$ is given by:\n",
    "\n",
    "$$B(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$$\n",
    "\n",
    "- If $B(\\hat{\\theta}) < \\theta$, then $\\hat{\\theta}$ tends to underestimate $\\theta$.\n",
    "- If $B(\\hat{\\theta}) > \\theta$, then $\\hat{\\theta}$ tends to overestimate $\\theta$.\n",
    "\n",
    "The **mean square error** of a point estimator $\\hat{\\theta}$ is:\n",
    "\n",
    "$$\\text{MSE} = E\\left[ (\\hat{\\theta} - \\theta)^2 \\right] = V(\\hat{\\theta}) + \\left[ B(\\hat{\\theta}) \\right]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a3271",
   "metadata": {},
   "source": [
    "## 12.1 Estimating Variance\n",
    "<hr>\n",
    "\n",
    "Sample variance could be estimated as:\n",
    "\n",
    "$$s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$$\n",
    "\n",
    "OR...\n",
    "\n",
    "$$s'^2 = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$$\n",
    "\n",
    "Which one is an unbiased estimator?\n",
    "\n",
    "*Note that there is no assumption on the shape of the distribution and $E(Y_i)=\\mu$ and $V(Y_i)=\\sigma^2$.*\n",
    "\n",
    "$$s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\sum_{i=1}^n (Y_i - \\bar{Y})^2 = \\sum_{i=1}^n Y_i^2 - n\\bar{Y}$$\n",
    "\n",
    "$$E\\left[ \\sum_{i=1}^n Y_i^2 - n\\bar{Y} \\right] = \\sum_{i=1}^n E(Y_i^2) - n E(\\bar{Y}^2)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{align}\n",
    "E(Y_i)^2 &= V(Y_i) + [E(Y_i)]^2 = \\sigma^2 + \\mu^2 \\\\\n",
    "E(\\bar{Y}^2) &= V(\\bar{Y}) + [E(\\bar{Y})]^2 = \\frac{\\sigma^2}{n} + \\mu^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "$$E\\left[ \\sum_{i=1}^n Y_i^2 - n\\bar{Y} \\right] = \\sum_{i=1}^n E(Y_i^2) - n E(\\bar{Y}^2) = n(\\sigma^2 + \\mu^2) - n \\left( \\frac{\\sigma^2}{n} + \\mu^2 \\right) = (n-1) \\sigma^2 $$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$E(s^2) = E\\left[ \\frac{1}{n-1} (n-1) \\sigma^2 \\right] = E(\\sigma^2) = \\sigma^2$$\n",
    "\n",
    "$$E(s'^2) = E\\left[ \\frac{1}{n} (n-1) \\sigma^2 \\right] = E(\\sigma^2 - \\frac{\\sigma^2}{n}) = \\sigma^2 - \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "Hence, $s^2$ is an unbiased estimator of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1568f",
   "metadata": {},
   "source": [
    "## 12.2 The Method of Moments (MOM)\n",
    "<hr>\n",
    "\n",
    "One of the oldest methods for deriving point estimators is the method of moments. Recall that the $k$th moment of a random variable, taken about the origin, is:\n",
    "\n",
    "$$u'_k = E(Y^k)$$\n",
    "\n",
    "The corresponding $k$th sample moment is the average:\n",
    "\n",
    "$$m'_k = \\frac{1}{n} \\sum_{i=1}^n Y_i^k$$\n",
    "\n",
    "Sample moments should provide good estimates of the corresponding population moments. That is, $m'_k$ should be a good estimator of $\\mu'_k$. Then, because the population moments are functions of the population parameters, we can equate corresponding population and sample moments and solve for the desired estimators.\n",
    "\n",
    "***Choose as estimates those values of the parameters that are solutions of the equations $Î¼'_k=m'_k$, for $k=1,2,\\cdots,t$ where $t$ is the number of parameters to be estimated.***\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** Let $Y_1, Y_2, \\cdots, Y_n$ denote a random sample from the pdf:\n",
    "\n",
    "$$f(y \\mid \\theta) = \\begin{cases}\n",
    "(\\theta+1)y^\\theta & 0<y<1; \\theta>-1 \\\\\n",
    "0 & \\text{elsewhere}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Find an estimator for $\\theta$ by the method of moments. Show that the estimator is consistent.\n",
    "\n",
    "Since there is onlyh one parameter $\\theta$, we find the first moment for $t=1$ as:\n",
    "\n",
    "$$\\mu'_1 = E(Y) \\quad \\quad m'_1 = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}$$\n",
    "\n",
    "Setting $\\mu'_1$ equal to $m'_1$:\n",
    "\n",
    "$$E(Y) = \\bar{Y}$$\n",
    "\n",
    "Finding $E(Y)$ as:\n",
    "\n",
    "$$E(Y) = \\int_{0}^1 y(\\theta+1)y^\\theta dy = (\\theta + 1) \\int_{0}^1 y^{\\theta+1} dy = \\frac{\\theta+1}{\\theta+2}$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\frac{\\theta+1}{\\theta+2} = \\bar{Y}$$\n",
    "\n",
    "Solving for $\\theta$:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{MOM}} = \\frac{2 \\bar{Y} - 1}{1 - \\bar{Y}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** If $Y_1,Y_2, \\cdots ,Y_n$ denote a random sample from the normal distribution with known mean $\\mu=0$ and unknown variance $\\sigma^2$, find the method-of-moments estimator of $\\sigma^2$.\n",
    "\n",
    "$$Y_i \\text{(iid)} \\sim N(0,\\sigma^2)$$\n",
    "\n",
    "Since one parameter $\\mu=0$ is already known, we need to find only one equation. For a two-parameter random variable such as normal, we need to find the second moment, and set it equal to the sample second moment.\n",
    "\n",
    "$$\\mu'_2 = E(Y^2), \\quad \\quad m'_2 = \\frac{1}{n} \\sum_{i=1}^n Y_i^2$$\n",
    "\n",
    "$$E(Y^2) = V(Y) + [E(Y)]^2 = \\sigma^2 + 0^2 = \\sigma^2$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\hat{\\sigma}^2_{\\text{MOM}} = m'_2 = \\frac{1}{n} \\sum_{i=1}^n Y_i^2$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_ {\\text{MOM}} &= \\bar{Y} & \\text{(exponential)} \\\\\n",
    "\\hat{p}_ {\\text{MOM}} &= \\frac{1}{\\bar{Y}} & \\text{(geometric)} \\\\\n",
    "\\hat{\\lambda}_ {\\text{MOM}} &= \\bar{Y} & \\text{(poisson)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "For a gamma random variable, since it is a two-parameter random variable, we need two equations and the 2nd moment.\n",
    "\n",
    "$$E(Y)=\\alpha \\beta, \\quad V(Y)=\\alpha \\beta^2, \\quad E(Y^2)=\\alpha \\beta^2 + (\\alpha \\beta)^2$$\n",
    "\n",
    "$$\\mu'_1 = m'_1 \\quad \\rightarrow E(Y)=\\bar{Y} \\quad \\rightarrow \\alpha \\beta = \\bar{Y} \\quad \\rightarrow \\hat{\\alpha}_{\\text{MOM}} = \\frac{\\bar{Y}}{\\beta}$$\n",
    "\n",
    "$$\\mu'_2 = m'_2 \\quad \\rightarrow \\alpha \\beta^2 + (\\alpha \\beta)^2 = \\frac{1}{n} \\sum_{i=1}^n Y_i^2$$\n",
    "\n",
    "Plugging in the value of $\\hat{\\alpha}_{\\text{MOM}}$ and simplifying:\n",
    "\n",
    "$$\\hat{\\beta}_{\\text{MOM}} = \\frac{\\sum_{i=1}^n Y_i^2 - \\bar{Y}^2}{n \\bar{Y}}$$\n",
    "\n",
    "Plugging this value back into $\\hat{\\alpha}_{\\text{MOM}}$:\n",
    "\n",
    "$$\\hat{\\alpha}_{\\text{MOM}} = \\frac{n \\bar{Y}^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cc1e9",
   "metadata": {},
   "source": [
    "## 12.3 Maximum Likelihood Estimation (MLE)\n",
    "<hr>\n",
    "\n",
    "The method of maximum likelihood selects as estimate(s) of the parameter(s) those which maximize the likelihood of the observed sample.\n",
    "\n",
    "What is a likelihood function?\n",
    "\n",
    "- $y_1,y_2, \\cdots ,y_n$ are sample observations taken on random variables $Y_1,Y_2, \\cdots ,Y_n$\n",
    "- The distribution of $Y_i$ depends on a parameter $\\theta$\n",
    "- Notation: $L(y_1,y_2,\\cdots,y_n \\mid \\theta)$ represents the likelihood of the sample given $\\theta$\n",
    "\n",
    "\\begin{align}\n",
    "L(y_1, y_2, \\cdots, y_n \\mid \\theta) &= P(y_1, \\cdots, y_n \\mid \\theta) = \\prod_{i=1}^n P(y_i \\mid \\theta) & \\text{(discrete)} \\\\\n",
    "L(y_1, y_2, \\cdots, y_n \\mid \\theta) &= f(y_1, \\cdots, y_n \\mid \\theta) = \\prod_{i=1}^n f(y_i \\mid \\theta) & \\text{(continuous)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** Let $Y_1, Y_2, \\cdots, Y_n$ denote a random sample from the pdf:\n",
    "\n",
    "$$f(y \\mid \\theta) = \\begin{cases}\n",
    "(\\theta+1)y^\\theta & 0<y<1; \\theta>-1 \\\\\n",
    "0 & \\text{elsewhere}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Question:** Find the MLE for $\\theta$.\n",
    "\n",
    "\\begin{align}\n",
    "L(y_1, \\cdots, y_n \\mid \\theta) &= \\prod_{i=1}^n f(y_i \\mid \\theta) = \\prod_{i=1}^n \\left[ (\\theta+1)y^\\theta \\right] = (\\theta+1)^n (y_1 y_2 \\cdots y_n)^\\theta \\\\\n",
    "\\\\\n",
    "\\mathcal{L}(y_1, \\cdots, y_n \\mid \\theta) &= \\ln \\left[ (\\theta+1)^n (y_1 y_2 \\cdots y_n)^\\theta \\right] = \\ln{(\\theta+1)^n} + \\ln{(y_1 y_2 \\cdots y_n)^\\theta} = n \\ln{(\\theta+1)} + \\theta \\ln{(y_1 y_2 \\cdots y_n)} = n \\ln{(\\theta+1)} + \\theta \\sum_{i=1}^n \\ln{(y_i)} \\\\ \n",
    "\\end{align}\n",
    "\n",
    "*Taking the derivative of the log-likelihood and setting it equal to zero (maximization):*\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d \\mathcal{L}}{d\\theta} &= n \\left( \\frac{1}{\\theta+1} \\right) + \\sum_{i=1}^n \\ln{(y_i)} = 0 \\\\\n",
    "n \\left( \\frac{1}{\\theta+1} \\right) &= - \\sum_{i=1}^n \\ln{(y_i)} \\\\\n",
    "\\theta+1 &= -\\frac{n}{\\sum_{i=1}^n \\ln{(y_i)}} \\\\\n",
    "\\\\\n",
    "\\hat{\\theta}_ {\\text{MLE}} &= -\\frac{n}{\\sum_{i=1}^n \\ln{(y_i)}} - 1\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
