{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0843f45a",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Forward propagation computes the output of a neural network given inputs. Traditionally, it's done sequentially, layer by layer, which isn't suitable for parallel processing. To enable parallelization, we use matrix multiplication, termed as matrix forward propagation.\n",
    "\n",
    "To compute forward propagation for layer $l$, we need the following:\n",
    "\n",
    "- Inputs\n",
    "    - Input vector $a^{[l-1]}$\n",
    "    - Weight matrix $W^{[l]}$\n",
    "    - Bias vector $b^{[l]}$\n",
    "    - Activation function $g^{[l]}$\n",
    "- Outputs\n",
    "    - Output vector $a^{[l]}$\n",
    "    - Cache $z^{[l]}$\n",
    "\n",
    "The output of layer $l$ is computed as follows:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "When we compute forward propagation, we cache the intermediate values of $z^{[l]}$ and $a^{[l]}$ to use them in backward propagation.\n",
    "\n",
    "## Backward Propagation\n",
    "Backward propagation computes the gradient of the loss function concerning the network's weights and biases. To compute backward propagation for layer $l$, we need the following:\n",
    "\n",
    "- Inputs\n",
    "    - $da^{[l]}$\n",
    "- Outputs\n",
    "    - $da^{[l-1]}$\n",
    "    - $dW^{[l]}$\n",
    "    - $db^{[l]}$\n",
    "\n",
    "To compute the gradient of the loss function concerning the weights and biases of layer $l$, we use the following formulas:\n",
    "\n",
    "$$dz^{[l]} = da^{[l]} * g'^{[l]}(z^{[l]})$$\n",
    "\n",
    "$$dW^{[l]} = dz^{[l]}a^{[l-1]T}$$\n",
    "\n",
    "$$db^{[l]} = \\sum_{i=1}^{m}dz^{[l_i]}$$\n",
    "\n",
    "$$da^{[l-1]} = W^{[l]T}dz^{[l]}$$\n",
    "\n",
    "## Derivations\n",
    "Computing forward propagation is straightforward. However, computing backward propagation is a bit tricky. We need to derive the gradient of the loss function concerning the weights and biases of each layer. To do so, we use the chain rule.\n",
    "\n",
    "### Forward Propagation Equations\n",
    "1. $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$\n",
    "2. $a^{[l]} = g^{[l]}(z^{[l]})$\n",
    "\n",
    "### Backward Propagation Equations\n",
    "#### Derivation of $z^{[l]}$ :\n",
    "To compute the gradient of the loss function $L$ with respect to $z^{[l]}$, we use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$$\n",
    "\n",
    "In the above equation, we can compute $\\frac{\\partial L}{\\partial a^{[l]}}$ from the previous layer. However, we need to compute $\\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$ which is the derivative of the activation function $g^{[l]}$. Therefore:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} g'^{[l]}(z^{[l]}) = da^{[l]} * g'^{[l]}(z^{[l]})$$\n",
    "\n",
    "#### Derivation of $W^{[l]}$ :\n",
    "To compute the gradient of the loss function $L$ with respect to $W^{[l]}$, we use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial W^{[l]}}$$\n",
    "\n",
    "In the above equation, we can compute $\\frac{\\partial L}{\\partial z^{[l]}}$ from the previous layer. However, we need to compute $\\frac{\\partial z^{[l]}}{\\partial W^{[l]}}$ which is the derivative of the forward propagation equation $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$. Therefore:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}} a^{[l-1]T} = dz^{[l]}a^{[l-1]T}$$\n",
    "\n",
    "#### Derivation of $b^{[l]}$ :\n",
    "To compute the gradient of the loss function $L$ with respect to $b^{[l]}$, we use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial b^{[l]}}$$\n",
    "\n",
    "In the above equation, we can compute $\\frac{\\partial L}{\\partial z^{[l]}}$ from the previous layer. However, we need to compute $\\frac{\\partial z^{[l]}}{\\partial b^{[l]}}$ which is the derivative of the forward propagation equation $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$. Therefore:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial b^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}} = dz^{[l]}$$\n",
    "\n",
    "#### Derivation of $a^{[l-1]}$ :\n",
    "To compute the gradient of the loss function $L$ with respect to $a^{[l-1]}$, we use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^{[l-1]}} = \\frac{\\partial L}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial a^{[l-1]}}$$\n",
    "\n",
    "In the above equation, we can compute $\\frac{\\partial L}{\\partial z^{[l]}}$ from the previous layer. However, we need to compute $\\frac{\\partial z^{[l]}}{\\partial a^{[l-1]}}$ which is the derivative of the forward propagation equation $z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$. Therefore:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^{[l-1]}} = \\frac{\\partial L}{\\partial z^{[l]}} \\frac{\\partial z^{[l]}}{\\partial a^{[l-1]}} = \\frac{\\partial L}{\\partial z^{[l]}} W^{[l]T} = da^{[l]} W^{[l]T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff42877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
