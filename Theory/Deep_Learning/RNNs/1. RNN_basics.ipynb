{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ec8d3c",
   "metadata": {},
   "source": [
    "# 1 - Sequence Models\n",
    "\n",
    "Sequence models are a special form of neural networks that take their input as a sequence of tokens. They are often applied in ML tasks such as speech recognition, Natural Language Processing or bioinformatics (like processing DNA sequences).\n",
    "\n",
    "Previously seen models processed some sort of input (e.g. images) which exhibited following properties:\n",
    "- It was uniform (e.g. an image of a certain size)\n",
    "- It was processed as a whole (i.e. an image was not partially processed)\n",
    "- It was often multidimensional (e.g. an image with 3 color channels yields a matrix of $H \\times W \\times 3$)\n",
    "\n",
    "Sequence models are a bit different in that they require their input to be a sequence of tokens. Examples for such sequences could be:\n",
    "\n",
    "- audio data (sequence of sounds)\n",
    "- text (sequence of words)\n",
    "- video (sequence of images)\n",
    "- $\\cdots$\n",
    "\n",
    "The length of the individual input elements (i.e. their number of tokens) does not need to be of the same length, neither for training nor prediction. These input tokens are processed one after the other, whereas at each time step a certain token is processed. Processing can be stopped at any point. A form of sequence models are Recurrent Neural Networks (RNN) which are often used to process speech data (e.g. speech recognition, machine translation), generative models (e.g. generating music) or NLP (e.g. sentiment analysis, named entity recognition (NER), etc.).\n",
    "\n",
    "The notation for an input sequence $x$ of length $T_x$ or an output sequence $y$ of length $T_y$ is as follows (note the new notation with chevrons around the indices to enumerate the tokens):\n",
    "\n",
    "$$x = x^{<1>}, x^{<2>}, \\cdots, x^{<t>}, \\cdots, x^{<T_x>}$$\n",
    "\n",
    "$$y = y^{<1>}, y^{<2>}, \\cdots, y^{<t>}, \\cdots, y^{<T_y>}$$\n",
    "\n",
    "As stated above, the input and the output sequence don't need to be of the same length $\\left( T_x^{(i)} \\neq T_y^{(i)} \\right)$. Also the length of the individual training samples can vary $\\left( T_y^{(i)} \\neq T_y^{(j)} \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df92cc",
   "metadata": {},
   "source": [
    "# 2 - Recurrent Neural Networks\n",
    "\n",
    "The previously seen approach of a NN with an input layer, several hidden layers and an output layer is not feasible for the following reasons:\n",
    "\n",
    "- input and output can have different lengths for each sample (e.g. sentences with different numbers of words)\n",
    "- the samples don't share common features (e.g. in NER, where the named entity can be at any position in the sentence)\n",
    "\n",
    "Because RNNs process their input token-by-token, they don't suffer from these disadvantages. A simple RNN only has one layer through which the tokens pass during training/processing. However, the result of this processing has an influence on the processing of the next token. Consider the following sample architecture of a simple RNN:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/rnn.png\" width=800>\n",
    "    <caption><center><font color=\"purple\">Example of an RNN</font></center></caption>\n",
    "</div>\n",
    "\n",
    "A side effect of this kind of processing is that an RNN requires far less parameters to be optimized than e.g. a ConvNet would to do the same task. This especially comes in handy for sentence processing where each word (token) can be a vector of dimension e.g. $10,000 \\times 1$ (one-hot encoding from a vocabulary of 10,000 words).\n",
    "\n",
    "## 2.1 - Unidirectional RNN\n",
    "\n",
    "As seen in the picture above the RNN processes each token $x^{<t>}$ individually from left to right, one after the other. In each step $t$, the RNN tries to predict the output $\\hat{y}^{<t>}$ from the input token $x^{<t>}$ and the previous activation $a^{<tâˆ’1>}$. To determine the influence of the activation and the input token and the two weight matrices $W_{aa}$ and $W_{ax}$ are used. There is also a matrix $W_{ya}$ that governs the output predictions. Those matrices are the same for each step, i.e. they are shared for a single training instance. This way the layer is recursively used to process the sequence. A single input token can therefore not only directly influence the output at a given time step, but also indirectly the output of subsequent steps (thus the term recurrent). Vice versa a single prediction at time step $t$ not only depends on a single input token, but on several previously seen tokens (we will see how to expand this so that the following tokens are taken into consideration in bidirectional RNNs).\n",
    "\n",
    "## 2.2 - Forward propagation\n",
    "\n",
    "The activation $a^{<t>}$ and prediction $\\hat{y}^{<t>}$ for a single time step $t$ can be calculated as follows (for the first token the zero vector is often used as the previous activation):\n",
    "\n",
    "$$a^{<t>} = g_1 \\left( W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a \\right)$$\n",
    "\n",
    "$$\\hat{y}^{<t>} = g_2 \\left( W_{ya} a^{<t>} + b_y \\right) \\tag{1}$$\n",
    "    \n",
    "Note that the activation functions $g_1$ and $g_2$ can be different. The activation function to calaculate the next activation $(g_1)$ is often $\\text{Tanh}$ or $\\text{ReLU}$. The activation function to predict the next output $(g_2)$ is often the $\\text{Sigmoid}$ function for binary classification or else $\\text{Softmax}$. The notation of the weight matrices is by convention as that the first index denotes the output quantity and the second index the input quantity. $W_{ax}$ for example means <i>\"use the weights in $W$ to compute some output $a$ from input $x$\".</i>\n",
    "    \n",
    "This calculation can further be simplified by concatenating the matrices $W_{aa}$ and $W_{ax}$ into a single matrix $W_a$ and stacking:\n",
    "    \n",
    "$$W_a = \\left[ W_{aa} \\mid W_{ax} \\right]$$\n",
    "    \n",
    "$$\\left[ a^{<t-1>}, x^{<t>} \\right] = \\left[ \\matrix{a^{<t-1>} \\\\ x^{<t>}} \\right]$$\n",
    "    \n",
    "The simplified formula to calculate forward propagation is then:\n",
    "\n",
    "$$a^{<t>} = g_1 \\left( W_a \\left[ a^{<t-1>}, x^{<t>} \\right] + b_a \\right)$$\n",
    "    \n",
    "$$\\hat{y}^{<t>} = g_2 \\left( W_{y} a^{<t>} + b_y \\right)$$\n",
    "    \n",
    "Note that the formula to calculate $\\hat{y}$ only changed in the subscripts used for the weight matrix. This simplified notation is equivalent to (1) but only uses one weight matrix instead of two.\n",
    "    \n",
    "## 2.2 - Backpropagation\n",
    "\n",
    "Because the input is read sequentially and the RNN computes a prediction in each step, the output is a sequence of predictions. The loss function for backprop for a single time step $t$ could be:\n",
    "\n",
    "$$\\mathcal{L} \\left( \\hat{y}^{<t>}, y^{<t>} \\right) = -y^{<t>} \\log{\\hat{y}^{<t>}} - \\left(1-y^{<t>}\\right) \\log{\\left( 1- \\hat{y}^{<t>} \\right)}\\tag{2}$$\n",
    "\n",
    "The formula to compute the overall cost for a sequence of $T_x$ predictions is therefore:\n",
    "\n",
    "$$\\mathcal{L} \\left(\\hat{y}, y\\right) = \\sum_{t=1}^{T_y} \\mathcal{L}^{<t>} \\left(\\hat{y}^{<t>}, y^{<t>} \\right)\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305323f",
   "metadata": {},
   "source": [
    "# 3 - RNN architectures\n",
    "\n",
    "There are different types of network architectures for RNN in terms of how the length of the input relates to the length of the output. A RNN can take a sequence of several tokens as an input and only produce a single value as an output. Such an architecture is called **many-to-one** and is used for tasks like **sentiment analysis** where the RNN e.g. tries to predict a movie rating based on a textual description of the critics.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/many-to-one.png\" width=350>\n",
    "    <caption><center><font color=\"purple\"><b><u>Sentiment Analysis:</u></b> Predict the sentiment (y = 0/1) from a sequence of text</font></center></caption>\n",
    "</div>\n",
    "\n",
    "The opposite is also possible: A RNN can take only a single value as input and produce a sequence as an output by re-using the previous outputs to make the next prediction. Such an architecture is called **one-to-many.** It could be used for example in a RNN that **generates music** by taking a genre as an input and generates a sequence of notes as an output.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/one-to-many.png\" width=350>\n",
    "    <caption><center><font color=\"purple\"><b><u>Music Generation:</u></b> Generate a sequence of music from given genre</font></center></caption>\n",
    "</div>\n",
    "\n",
    "There is theoretically also a one-to-one architecture. However, such an architecture is rarely encountered since it essentially corresponds to a standard NN.\n",
    "\n",
    "Finally, there are networks which take an input sequence of length $T_x$ and produce an output of length $T_y$. This is called a **many-to-many** architecture. In the above example, the length of the input was equal to the length of the output. However, input and output sequences need not to be of the same length. This property is especially important for tasks like **machine translation** where the translated text might be longer or shorter than the original text.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/many-to-many.png\" width=350>\n",
    "    <caption><center><font color=\"purple\"><b><u>Machine Translation:</u></b> Translate input sequence into output sequence</font></center></caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 3.1 - Encoder-Decoder Networks\n",
    "\n",
    "Models with a many-to-one architecture might be implemented as encoder-decoder models. This is perhaps the most commonly used framework for sequence modelling with neural networks. Like the name suggests, an Encoder-Decoder model consists of two RNNs.\n",
    "\n",
    "- The encoder maps the input sequence $X$ to a hidden representation $H$ of the same length as the input. \n",
    "- The decoder then consumes this hidden representation to produce $Y$, i.e. make a prediction.\n",
    "\n",
    "$$H = encode(X)$$\n",
    "\n",
    "$$Y = p(Y \\mid X) = decode(H)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/encoder-decoder.png\" width=550>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d45f1",
   "metadata": {},
   "source": [
    "# 4 - Language Model and Sequence generation\n",
    "\n",
    "RNN can be used for NLP tasks, e.g. in speech recognition to calculate for words that sound the same (homophones) the probability for each writing variant. Such tasks usually require large corpora of text which is tokenized. A token can be a word, a sentence or also just a single character. The most common words could then be kept in a dictionary and vectorized using one-hot encoding. Those word vectors could then be used to represent sentences as a matrix of word vectors. A special vector for the unknown word $(<unk>)$ could be defined for words in a sentece that is not in the dictionary plus an $<EOS>$ vector to indicate the end of a sentence.\n",
    "\n",
    "The RNN can then calculate in each step the probabilities for each word appearing in the given context using softmax. This means if the dictionary contains the 10,000 most common words, the prediction $\\hat{y}$ would be a vector of dimensions $(10,000 \\times 1)$ containing the probabilities for each word. This probabaility is calculated using Bayesian probability given the already seen previous words:\n",
    "\n",
    "$$\\hat{y}^{<t>} = P\\left( x^{<t>} \\mid x^{<t-1>}, x^{<t-2>}, \\cdots, x^{<1>} \\right)$$\n",
    "\n",
    "This output vector indicates the probability distribution over all words given a sequence of $t$ words. Predictions can be made until the $<EOS>$ token is processed or until some number of words have been processed. Such a network could be trained with the loss function (2) and the cost function (3) to predict the next word for a given sequence of words. This also works on character level where the next character is predicted to form a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b009020",
   "metadata": {},
   "source": [
    "# 5 - Vanishing Gradients in RNN\n",
    "\n",
    "Vanishing Gradients are also a problem for RNNs. This is especially relevant for language models because sentences can have relationships between words spanning over a lot of words. Consider the following sequence of tokens representing the sentence <i>\"The cat, which already ate a lot of food, which was delicious, was full.\"</i>\n",
    "\n",
    "```\n",
    "<the> <cat> <which> <already> <ate> <a> <lot> <of> <food> <which> <was> <delicious> <was> <full> <EOS>\n",
    "```\n",
    "\n",
    "OR:\n",
    "\n",
    "```\n",
    "<the> <cats> <which> <already> <ate> <a> <lot> <of> <food> <which> <was> <delicious> <were> <full> <EOS>\n",
    "```\n",
    "\n",
    "Note that the token \\<was> affects the token \\<cat>. However, since there are a lot of tokens in between, the RNN will have a hard time predicting the token \\<was> correctly. To capture **long-range dependencies** between words, the RNN would need to be very deep, which increases the risk of vanishing or exploding gradients. Exploding gradients can relatively easily be solved by using gradient clipping where gradients are clipped to some arbitrary maximal value. Vanishing gradients are harder to deal with and require the use of gated recurrent units (GRU) to memorize words for long range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493df207",
   "metadata": {},
   "source": [
    "# 6 - Gated Recurrent Unit (GRU)\n",
    "\n",
    "Gated Recurrent Units (GRU) are a modification for the hidden layeres in an RNN that help in mitigating the problem of vanishing gradients. GRU are cells in an RNN that have memory which serves as an additional input to make a prediction. To better understand how GRU memory cells work, consider the following image depicting how a normal RNN cell works:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/rnn-cell.png\" width=600>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 6.1 - Simple GRU\n",
    "\n",
    "GRU units have a memory cell $c^{<t>}$ to \"remember\" e.g. that the token $<cat>$ was singular for later time steps. Note that for GRU cells $c^{<t>}=a^{<t>}$ but we still use the variable $c$ for consistency reasons, because in another type of cell (the LSTM-cell), we use the same symbol. In each time step a value $\\tilde{c}$ is calculated as a candidate to replace the existing content of the memory cell $c$. This candidate uses an activation function (e.g. tanh), its own trainable parameter matrix $W_c$ and a separate bias $b_c$.\n",
    "\n",
    "$$\\tilde{c}^{<t>} = \\tanh{\\left( W_c \\left[c^{<t-1>}, x^{<t>} \\right] + b_c\\right)}\\tag{4}$$\n",
    "\n",
    "After calculating the candidate $\\tilde{c}^{<t>}$, we use an **update-gate** $\\Gamma_u$ to decide whether we should update the cell with this value or keep the old value. The value for $\\Gamma_u$  can be calculated using another trainable parameter matrix $W_u$ and bias $b_u$. Because Sigmoid is used as the activation function, the values for $\\Gamma_u$ are always between 0 and 1 (for simplification you can also think of $\\Gamma_u$ to be either exactly 0 or exactly 1).\n",
    "\n",
    "$$\\Gamma_u = \\sigma \\left( W_u \\left[ c^{<t-1>}, x^{<t>} \\right] + b_u \\right)\\tag{5}$$\n",
    "\n",
    "This gate is the key component of a GRU because it \"decides\" when to update the memory cell. Combining equations (4) and (5) gives us the following formula to calculate the value of the memory cell in each time step:\n",
    "\n",
    "$$c^{<t>} = \\Gamma_u \\cdot \\tilde{c}^{<t>} + (1-\\Gamma_u) \\cdot c^{<t-1>} \\tag{6}$$\n",
    "\n",
    "Note that the dimensions of $c^{<t>}, \\tilde{c}^{<t>}$, and $\\Gamma_u$ correspond to the number of units in the hidden layer. The following picture illustrates the calculations inside a GRU cell. The black box stands for the calculations in formula (6).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/gru-cell.png\" width=550>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 6.2 - Full GRU\n",
    "The above explanations described a simplified version of a GRU with only one gate $\\Gamma_u$ to decide whether to update the cell value or not. Because the memory-cell $c$ is a vector with several components, you can think of it as a series of bits, whereas each bit remembers one specific feature about the already seen words (e.g. one bit for the fact that $<cat>$ was singular, another bit to remember that the middle sentence was about food etc...). Full GRUs however usually have an additional parameter $\\Gamma_r$ that describes the relevance of individual features, which again uses its own parameter matrix $W_r$ and bias $b_r$ to be trained:\n",
    "\n",
    "$$\\Gamma_r = \\sigma \\left( W_r \\left[ c^{<t-1>}, x^{<t>} \\right] + b_r \\right) \\tag{7}$$\n",
    "\n",
    "In short, GRU cells allow a RNN to remember things by using a memory cell which is updated depending on an update-gate $\\Gamma_u$. In researach, the symbols used to denote the memory cell $c$, the candidate $\\tilde{c}$ and the two gates $\\Gamma_u$ and $\\Gamma_r$ are sometimes different. The following table contains all the parameters of a full GRU cell including a description and how to calculate them:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/full_gru.png\" width=900>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbb153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a60f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
