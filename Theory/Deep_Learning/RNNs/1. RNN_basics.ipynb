{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ec8d3c",
   "metadata": {},
   "source": [
    "# 1 - Sequence Models\n",
    "<hr>\n",
    "\n",
    "Sequence models are a special form of neural networks that take their input as a sequence of tokens. They are often applied in ML tasks such as speech recognition, Natural Language Processing or bioinformatics (like processing DNA sequences).\n",
    "\n",
    "Previously seen models processed some sort of input (e.g. images) which exhibited following properties:\n",
    "- It was uniform (e.g. an image of a certain size)\n",
    "- It was processed as a whole (i.e. an image was not partially processed)\n",
    "- It was often multidimensional (e.g. an image with 3 color channels yields a matrix of $H \\times W \\times 3$)\n",
    "\n",
    "Sequence models are a bit different in that they require their input to be a sequence of tokens. Examples for such sequences could be:\n",
    "\n",
    "- audio data (sequence of sounds)\n",
    "- text (sequence of words)\n",
    "- video (sequence of images)\n",
    "- $\\cdots$\n",
    "\n",
    "The length of the individual input elements (i.e. their number of tokens) does not need to be of the same length, neither for training nor prediction. These input tokens are processed one after the other, whereas at each time step a certain token is processed. Processing can be stopped at any point. A form of sequence models are Recurrent Neural Networks (RNN) which are often used to process speech data (e.g. speech recognition, machine translation), generative models (e.g. generating music) or NLP (e.g. sentiment analysis, named entity recognition (NER), etc.).\n",
    "\n",
    "The notation for an input sequence $x$ of length $T_x$ or an output sequence $y$ of length $T_y$ is as follows (note the new notation with chevrons around the indices to enumerate the tokens):\n",
    "\n",
    "$$x = x^{<1>}, x^{<2>}, \\cdots, x^{<t>}, \\cdots, x^{<T_x>}$$\n",
    "\n",
    "$$y = y^{<1>}, y^{<2>}, \\cdots, y^{<t>}, \\cdots, y^{<T_y>}$$\n",
    "\n",
    "Example: <i>\"The cat ate the food\"</i>\n",
    "\n",
    "```\n",
    "x = <The> <cat> <ate> <the> <food> <EOS>\n",
    "```\n",
    "\n",
    "$$T_x = \\text{len}(x) = 5$$\n",
    "\n",
    "$$x^{<T_x>} = \\text{food}$$ \n",
    "\n",
    "Where each token is one-hot encoded. For instance, if the size of the full dictionary (or vocabulary) is 10,000 and the token \"ate\" appears at position 371, then \"ate\" is one-hot encoded in a 10,000-dimenstional vector as:\n",
    "\n",
    "$$<ate> = \\left[ \\matrix{\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0}\n",
    "\\right]_{\\ (10000, 1)}\n",
    "$$\n",
    "\n",
    "As stated above, the input and the output sequence don't need to be of the same length $\\left( T_x^{(i)} \\neq T_y^{(i)} \\right)$. Also the length of the individual training samples can vary $\\left( T_y^{(i)} \\neq T_y^{(j)} \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df92cc",
   "metadata": {},
   "source": [
    "# 2 - Recurrent Neural Networks\n",
    "<hr>\n",
    "\n",
    "Previously seen NNs with an input layer, several hidden layers and an output layer are not feasible for sequential data for the following reasons:\n",
    "\n",
    "- Input and output can have different lengths for each sample (e.g. sentences with different numbers of words). A conventional NN always has the sampe input length so it'll not work for sequential data where input length could be different per sample.\n",
    "- The samples don't share common features. Ideally, we'd like the model to share features to remember that \"Harry\" is a name no matter where in the same or different sentence it appears.\n",
    "\n",
    "Because RNNs process their input token-by-token, they don't suffer from these disadvantages. A simple RNN only has one layer through which the tokens pass during training/processing. However, the result of this processing has an influence on the processing of the next token. Consider the following sample architecture of a simple RNN:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/rnn.png\" width=800>\n",
    "    <caption><center><font color=\"purple\">To predict the 3rd word $\\left(\\hat{y}^{<3>}\\right)$ in a sentence, the model processes the current input $x^{<3>}$ as well as previously predicted words $a^{<2>}$</font></center></caption>\n",
    "</div>\n",
    "\n",
    "## 2.1 - Unidirectional RNN\n",
    "\n",
    "The RNN processes each token $x^{<t>}$ individually from left to right, one after the other. At each time step $t$, the RNN tries to predict the output $\\hat{y}^{<t>}$ from the input token $x^{<t>}$ and the previous activation $a^{<tâˆ’1>}$. To determine the influence of the activation and the input token, the two weight matrices $W_{aa}$ and $W_{ax}$ are used. There is also a matrix $W_{ya}$ that governs the output predictions. Those matrices are the same at each time step, i.e. they are shared for a single training instance. This way the layer is recursively used to process the sequence. A single input token can therefore not only directly influence the output at a given time step, but also indirectly the output of subsequent steps (thus the term recurrent). Vice versa - a single prediction at time step $t$ not only depends on a single input token at time step $t$, but on several previously seen tokens.\n",
    "\n",
    "## 2.2 - Forward propagation\n",
    "\n",
    "The activation $a^{<t>}$ and prediction $\\hat{y}^{<t>}$ for a single time step $t$ can be calculated as follows (for the first input token, the zero vector is often used as the previous activation):\n",
    "\n",
    "$$a^{<t>} = g_1 \\left( W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a \\right)$$\n",
    "\n",
    "$$\\hat{y}^{<t>} = g_2 \\left( W_{ya} a^{<t>} + b_y \\right) \\tag{1}$$\n",
    "    \n",
    "Note that the activation functions $g_1$ and $g_2$ can be different. The activation function to calaculate the next activation $(g_1)$ is often $\\text{Tanh}$ or $\\text{ReLU}$. The activation function to predict the next output $(g_2)$ is often the $\\text{Sigmoid}$ function for binary classification or else $\\text{Softmax}$. The notation of the weight matrices is by convention as that the first index denotes the output quantity and the second index the input quantity. $W_{ax}$ for example means <i>\"use the weights in $W$ to compute some output $a$ from input $x$\".</i>\n",
    "    \n",
    "This calculation can further be simplified by concatenating the matrices $W_{aa}$ and $W_{ax}$ into a single matrix $W_a$ by stacking:\n",
    "    \n",
    "$$W_a = \\left[ W_{aa} \\mid W_{ax} \\right]$$\n",
    "    \n",
    "$$\\left[ a^{<t-1>}, x^{<t>} \\right] = \\left[ \\matrix{a^{<t-1>} \\\\ x^{<t>}} \\right]$$\n",
    "    \n",
    "The simplified formula to calculate forward propagation is then:\n",
    "\n",
    "$$a^{<t>} = g_1 \\left( W_a \\left[ a^{<t-1>}, x^{<t>} \\right] + b_a \\right)$$\n",
    "    \n",
    "$$\\hat{y}^{<t>} = g_2 \\left( W_{y} a^{<t>} + b_y \\right)$$\n",
    "    \n",
    "Note that the formula to calculate $\\hat{y}$ only changed in the subscripts used for the weight matrix. This simplified notation is equivalent to (1) but only uses one weight matrix instead of two.\n",
    "    \n",
    "## 2.2 - Backpropagation\n",
    "\n",
    "Because the input is read sequentially and the RNN computes a prediction at each step $t$, the output is a sequence of predictions. The loss function for backprop for a single time step $t$ could be:\n",
    "\n",
    "$$\\mathcal{L} \\left( \\hat{y}^{<t>}, y^{<t>} \\right) = -y^{<t>} \\log{\\hat{y}^{<t>}} - \\left(1-y^{<t>}\\right) \\log{\\left( 1- \\hat{y}^{<t>} \\right)}\\tag{2}$$\n",
    "\n",
    "The formula to compute the overall cost for a sequence of $T_x$ predictions is therefore:\n",
    "\n",
    "$$\\mathcal{L} \\left(\\hat{y}, y\\right) = \\sum_{t=1}^{T_y} \\mathcal{L}^{<t>} \\left(\\hat{y}^{<t>}, y^{<t>} \\right)\\tag{3}$$\n",
    "\n",
    "This algorithm is also called **backpropagation through time (BPTT)** since we're backpropagating through the different time steps $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305323f",
   "metadata": {},
   "source": [
    "# 3 - RNN architectures\n",
    "<hr>\n",
    "\n",
    "There are different types of network architectures for RNN in terms of how the length of the input relates to the length of the output. A RNN can take a sequence of several tokens as an input and only produce a single value as an output. Such an architecture is called **many-to-one** and is used for tasks like **sentiment analysis** where the RNN, for instance, tries to predict a movie rating based on a textual description of the critics.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/many-to-one.png\" width=350>\n",
    "    <caption><center><font color=\"purple\"><b><u>Sentiment Analysis:</u></b> Predict the sentiment (y = 0/1) from a sequence of text</font></center></caption>\n",
    "</div>\n",
    "\n",
    "A RNN can also take only a single value as input and produce a sequence as an output by re-using the previous outputs to make the next prediction. Such an architecture is called **one-to-many.** It could be used in a RNN that **generates music** by taking a genre as an input and generateing a sequence of notes as an output.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/one-to-many.png\" width=350>\n",
    "    <caption><center><font color=\"purple\"><b><u>Music Generation:</u></b> Generate a sequence of music from given genre</font></center></caption>\n",
    "</div>\n",
    "\n",
    "There is theoretically also a **one-to-one** architecture. However, such an architecture is rarely encountered since it essentially corresponds to a standard NN.\n",
    "\n",
    "Finally, there are networks that take as input a sequence of length $T_x$ and produce an output of length $T_y$. This is called a **many-to-many** architecture. In the above example, the length of the input was equal to the length of the output. However, input and output sequences need not to be of the same length. This property is especially important for tasks like **machine translation** where the translated text might be longer or shorter than the original text.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/many-to-many.png\" width=350>\n",
    "    <caption><center><font color=\"purple\"><b><u>Machine Translation:</u></b> Translate input sequence into output sequence</font></center></caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 3.1 - Encoder-Decoder Networks\n",
    "\n",
    "Models with a many-to-many architecture might be implemented as encoder-decoder models. This is perhaps the most commonly used framework for sequence modelling with neural networks. Like the name suggests, an Encoder-Decoder model consists of two RNNs.\n",
    "\n",
    "- The encoder maps the input sequence $X$ to a hidden representation $H$ of the same length as the input. \n",
    "- The decoder then consumes this hidden representation to produce $Y$, i.e. make a prediction.\n",
    "\n",
    "$$H = encode(X)$$\n",
    "\n",
    "$$Y = p(Y \\mid X) = decode(H)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/encoder-decoder.png\" width=550>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d45f1",
   "metadata": {},
   "source": [
    "# 4 - Language Model and Sequence Generation\n",
    "<hr>\n",
    "\n",
    "RNNs can be used for NLP tasks, e.g. in speech recognition to calculate the probability of a word occuring in a sentence - especially if two words said in the speed sound the same (homophones) like \"pair\" and \"pear\". Such tasks usually require large corpora of text which is tokenized. A token can be a word, a sentence or also just a single character. The most common words could then be kept in a dictionary and vectorized using one-hot encoding. Those word vectors could then be used to represent sentences as a matrix of word vectors. A special vector for the unknown word $(<unk>)$ could be defined for words in a sentece that are not in the dictionary plus an $<EOS>$ vector to indicate the end of a sentence.\n",
    "\n",
    "The RNN can then calculate, at each step $t$, the probabilities for each word appearing in the given context using softmax. This means if the dictionary contains the 10,000 most common words, the prediction $\\hat{y}$ would be a vector of dimensions $(10,000 \\times 1)$ containing the probabilities for each word. This probabaility is calculated using Bayesian probability given the already seen previous words:\n",
    "\n",
    "$$\\hat{y}^{<t>} = P\\left( x^{<t>} \\mid x^{<t-1>}, x^{<t-2>}, \\cdots, x^{<1>} \\right)$$\n",
    "\n",
    "This output vector indicates the probability distribution over all words given a sequence of $t$ words. Predictions can be made until the $<EOS>$ token is processed or until some number of words have been processed. Such a network could be trained with the loss function (2) and the cost function (3) to predict the next word for a given sequence of words. This also works on character level where the vocabulary consists of individual characters from A-Z rather than words. One advantage of it is that we no longer have to worry about \\<known> tokens since we're working on a character-level; but it is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b009020",
   "metadata": {},
   "source": [
    "# 5 - Vanishing Gradients in RNN\n",
    "<hr>\n",
    "\n",
    "Vanishing Gradients are also a problem for RNNs. This is especially relevant for language models because sentences can have relationships between words spanning over a lot of words. Consider the following sequence of tokens representing the sentence <i>\"The cat, which already ate a lot of food which was delicious, was full.\"</i>\n",
    "\n",
    "\\<the> <font color=\"red\">\\<cat\\></font> \\<which> \\<already> \\<ate> \\<a> \\<lot> \\<of> \\<food> \\<which> \\<was> \\<delicious> <font color=\"red\">\\<was\\></font> \\<full> \\<EOS>\n",
    "\n",
    "OR:\n",
    "\n",
    "\\<the> <font color=\"red\">\\<cats\\></font> \\<which> \\<already> \\<ate> \\<a> \\<lot> \\<of> \\<food> \\<which> \\<was> \\<delicious> <font color=\"red\">\\<were\\></font> \\<full> \\<EOS>\n",
    "\n",
    "Note that the token \\<was> affects the token \\<cat>. However, since there are a lot of tokens in between, the RNN will have a hard time predicting the token \\<was> correctly. To capture **long-range dependencies** between words, the RNN would need to be very deep, which increases the risk of vanishing or exploding gradients. Exploding gradients can relatively easily be solved by using gradient clipping where gradients are clipped to some arbitrary maximal value. Vanishing gradients are harder to deal with and require the use of gated recurrent units (GRU) to memorize words for long range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493df207",
   "metadata": {},
   "source": [
    "# 6 - Gated Recurrent Unit (GRU)\n",
    "<hr>\n",
    "\n",
    "Gated Recurrent Units (GRU) are a modification for the hidden layers in an RNN that help in mitigating the problem of vanishing gradients. GRUs are cells in an RNN that have memory which serves as an additional input to make a prediction. To better understand how GRU memory cells work, consider the following image depicting how a normal RNN cell works:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/rnn_step_forward.png\" width=600>\n",
    "    <caption><center><font color=\"purple\">GRU without memory cell</font></center></caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 6.1 - Simple GRU\n",
    "\n",
    "GRU units have a memory cell $c^{<t>}$ to \"remember\" for instance that the token $<cat>$ was singular for later time steps. Note that for GRU cells $c^{<t>}=a^{<t>}$ but we still use the variable $c$ for consistency because in another type of cell (the LSTM-cell), we use the same symbol where $c^{<t>} \\neq a^{<t>}$. In each time step a value $\\tilde{c}$ is calculated as a candidate to replace the existing content of the memory cell $c$. This candidate uses an activation function (e.g. tanh), its own trainable parameter matrix $W_c$ and a separate bias $b_c$.\n",
    "\n",
    "$$\\tilde{c}^{<t>} = \\tanh{\\left( W_c \\left[c^{<t-1>}, x^{<t>} \\right] + b_c\\right)}\\tag{4}$$\n",
    "\n",
    "After calculating the candidate $\\tilde{c}^{<t>}$, we use an **update-gate** $\\Gamma_u$ to decide whether we should update the cell with this value or keep the old value. The value for $\\Gamma_u$  can be calculated using another trainable parameter matrix $W_u$ and bias $b_u$. Because Sigmoid is used as the activation function, the values for $\\Gamma_u$ are always between 0 and 1 (for simplification you can also think of $\\Gamma_u$ to be either exactly 0 or exactly 1).\n",
    "\n",
    "$$\\Gamma_u = \\sigma \\left( W_u \\left[ c^{<t-1>}, x^{<t>} \\right] + b_u \\right)\\tag{5}$$\n",
    "\n",
    "This gate is the key component of a GRU because it \"decides\" when to update the memory cell. Combining equations (4) and (5) gives us the following formula to calculate the value of the memory cell in each time step:\n",
    "\n",
    "$$c^{<t>} = \\Gamma_u \\cdot \\tilde{c}^{<t>} + (1-\\Gamma_u) \\cdot c^{<t-1>} \\tag{6}$$\n",
    "\n",
    "Note that the dimensions of $c^{<t>}, \\tilde{c}^{<t>}$, and $\\Gamma_u$ correspond to the number of units in the hidden layer. The following picture illustrates the calculations inside a GRU cell. The black box stands for the calculations in formula (6).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/gru-cell.png\" width=450>\n",
    "    <caption><center><font color=\"purple\">GRU with memory cell</font></center></caption>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 6.2 - Full GRU\n",
    "\n",
    "The above explanations described a simplified version of a GRU with only one gate $\\Gamma_u$ to decide whether to update the cell value or not. Because the memory-cell $c$ is a vector with several components, you can think of it as a series of bits, whereas each bit remembers one specific feature about the already seen words (e.g. one bit for the fact that $<cat>$ was singular, another bit to remember that the middle sentence was about food etc...). Full GRUs however usually have an additional parameter $\\Gamma_r$ that describes the relevance of individual features, which again uses its own parameter matrix $W_r$ and bias $b_r$ to be trained:\n",
    "\n",
    "$$\\Gamma_r = \\sigma \\left( W_r \\left[ c^{<t-1>}, x^{<t>} \\right] + b_r \\right) \\tag{7}$$\n",
    "\n",
    "In short, GRU cells allow a RNN to remember things by using a memory cell which is updated depending on an update-gate $\\Gamma_u$. In researach, the symbols used to denote the memory cell $c$, the candidate $\\tilde{c}$ and the two gates $\\Gamma_u$ and $\\Gamma_r$ are sometimes different. The following table contains all the parameters of a full GRU cell including a description and how to calculate them:\n",
    "\n",
    "| Symbol | alternative | Description | Calculation |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| $$\\tilde{c}^{<t>}$$ | $$\\tilde{h}$$ | candidate to replace the memory cell value | $$\\tilde{c}^{<t>} = \\tanh{\\left( W_c \\left[c^{<t-1>}, x^{<t>} \\right] + b_c\\right)}$$ |\n",
    "| $$\\Gamma_u$$ | $$u$$ | Update-Gate to control whether to update the memory cell or not | $$\\Gamma_u = \\sigma \\left( W_u \\left[ c^{<t-1>}, x^{<t>} \\right] + b_u \\right)$$ |\n",
    "| $$\\Gamma_r$$ | $$r$$ | Relevance-Gate to control the relevance of the memory cell values for the candidate | $$\\Gamma_r = \\sigma \\left( W_r \\left[ c^{<t-1>}, x^{<t>} \\right] + b_r \\right)$$ | \n",
    "| $$c^{<t>}$$ | $$h$$ | new memory cell value at time step t | $$c^{<t>} = \\Gamma_u \\cdot \\tilde{c}^{<t>} + (1-\\Gamma_u) \\cdot c^{<t-1>}$$ | \n",
    "| $$a^{<t>}$$ | - | new activation value | $$a^{<t>} = c^{<t>}$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac38431",
   "metadata": {},
   "source": [
    "# 7 - Long Short Term Memory (LSTM)\n",
    "<hr>\n",
    "\n",
    "An advanced alternative to GRU are Long Short Term Memory (LSTM) cells. LSTM cells can be considered a more general and more powerful version of GRU cells. Such cells also use a memory cell $c$ to remember something. However, the update of this cell is slightly different from GRU cells.\n",
    "\n",
    "In contrast to GRU cells, the memory cell does not correspond to the activation value anymore, so for LSTM-cells $c^{<t>} \\neq a^{<t>}$. It also does not use a relevance gate $\\Gamma_r$ anymore but rather a **forget-gate** $\\Gamma_f$ that governs whether to forget the current cell value or not. Finally, there is a third parameter $\\Gamma_o$ to act as **output-gate** and is used to scale the update memory cell value to calculate the activation value for the next iteration.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/LSTM.png\" width=800>\n",
    "    <caption><center><font color=\"purple\">LSTM cell with additional forget and ouput gates</font></center></caption>\n",
    "</div>\n",
    "\n",
    "Several of such LSTM cells can be combined to form an LSTM-network. There are variations for the LSTM cell implementation, such as making the gate parameters $\\Gamma_u, \\Gamma_f$ and $\\Gamma_o$ not only depend on the previous activation value $a^{<tâˆ’1>}$ and the current input token $x^{<t>}$, but also on the previous memory cell valu $c^{<tâˆ’1>}$. The update for the update gate is then $\\Gamma_u=\\sigma\\left( W_u \\left[a^{<tâˆ’1>}, x^{<t>}, c^{<tâˆ’1>} \\right] + b_u \\right)$. This is called a **peephole connection.**\n",
    "\n",
    "The following table summarizes the different parameters and how to calculate them.\n",
    "\n",
    "| Symbol | alternative | Description | Calculation |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| $$\\tilde{c}^{<t>}$$ | $$\\tilde{h}$$ | candidate to replace the memory cell value | $$\\tilde{c}^{<t>} = \\tanh{\\left( W_c \\left[a^{<t-1>}, x^{<t>} \\right] + b_c\\right)}$$ |\n",
    "| $$\\Gamma_u$$ | $$u$$ | Update-Gate to control whether to update the memory cell or not | $$\\Gamma_u = \\sigma \\left( W_u \\left[ a^{<t-1>}, x^{<t>} \\right] + b_u \\right)$$ |\n",
    "| $$\\Gamma_f$$ | $$u$$ | Forget-Gate to control influence of current memory cell value for the new value | $$\\Gamma_f = \\sigma\\left( W_f \\left[ a^{<t-1>}, x^{<t>} \\right] + b_f \\right)$$ |\n",
    "| $$\\Gamma_o$$ | $$u$$ | Output-Gate to control influence of current memory cell value for the new value | $$\\Gamma_o = \\sigma \\left( W_o \\left[ a^{<t-1>}, x^{<t>} \\right] + b_o \\right)$$ | \n",
    "| $$c^{<t>}$$ | $$h$$ | new memory cell value at time step t | $$c^{<t>} = \\Gamma_u \\cdot \\tilde{c}^{<t>} + \\Gamma_f \\cdot c^{<t-1>}$$ | \n",
    "| $$a^{<t>}$$ | - | new activation value | $$a^{<t>} = \\Gamma_o \\cdot \\text{tanh}\\left( c^{<t>} \\right)$$ |\n",
    "\n",
    "\n",
    "## 7.1 - GRU vs. LSTM\n",
    "\n",
    "There is not an universal rule when to use GRU- or LSTM-cells. GRU cells represent a simpler model, hence they are more suitable to build a bigger RNN model because they are computationally more efficient and the RNN will scale faster. On the other hand the LSTM-cells are more powerful and more flexible, but they also require more training data. As a rule-of-thumb, practitioners will often use LSTM cells as they're more powerful and have longer richer than GRU cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5477336",
   "metadata": {},
   "source": [
    "# 8 - Bidirectional RNN (BRNN)\n",
    "<hr>\n",
    "\n",
    "Unidirectional RNNs only consider already seen tokens at a time step $t$ to make a prediction. In contrast, bidirectional RNN (BRNN) also take subsequent tokens into account. This is helpful for NER when trying to predict whether the word \"Teddy\" is part of a name in the following two sencences:\n",
    "\n",
    "\\<he> \\<said> <font color=\"red\">\\<teddy\\></font> \\<bears> \\<are> \\<on> \\<sale> \\<EOS>\n",
    "\n",
    "\\<he> \\<said> <font color=\"red\">\\<teddy\\></font> \\<roosevelt> \\<was> \\<a> \\<US> \\<president> \\<EOS>\n",
    "    \n",
    "Just by looking at the previously seen words it is not clear at time step $t=3$ whether $<teddy>$ is part of a name or not. To do that we need the information of the following tokens. \n",
    "    \n",
    "A BRNN can do this using an additional layer. During forward propagation the activation values $\\overrightarrow{a}$ are computed as seen above from the input tokens and the previous activation values using an RNN cell (normal RNN cell, GRU or LSTM). The second part of forward propagation calculates the values $\\overleftarrow{a}$ from left to right using the additional layer. The following picture illustrates this. Note that the arrows in blue and green only indicate the order in which the tokens are evaluated. It does not indicate backpropagation.\n",
    "    \n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/brnn.png\" width=650>\n",
    "    <caption><center><font color=\"purple\">BRNN activation flow diagram</font></center></caption>\n",
    "</div> \n",
    "    \n",
    "After a single pass of forward propagation,a prediction at time step $t$ an be made by stacking the activations of both directions and calculating the prediction value as follows:\n",
    "    \n",
    "$$\\hat{y}^{<t>} = g \\left( W_y \\left[ \\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>} \\right] + b_y \\right)$$\n",
    "    \n",
    "The advantage of BRNN is that it allows to take into account words from both directions when making a prediction, which makes it a good fit for many language-related applications like machine translation. On the downside, because tokens from both directions are considered, the whole sequence needs to be processed before a prediction can be made. This makes it unsuitable for tasks like real-time speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca5d7d",
   "metadata": {},
   "source": [
    "# 9 - Deep RNNs\n",
    "<hr>\n",
    "\n",
    "The RNNs we have seen so far consisted actually of only one layer (with the exception of the BRNN which used an additional layer for the reverse direction). We can however stack several of those layers on top of each other to get a Deep RNN. In such a network, the results from one layer are passed on to the next layer in each time step $t$:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/deep-rnn.png\" width=550>\n",
    "    <caption><center><font color=\"purple\">RNN units vertically stacked to create a deep RNN</font></center></caption>\n",
    "</div>\n",
    "\n",
    "The activation $a^{[l]<t>}$ for layer $l$ at time step $t$ can be calculated as follows:\n",
    "\n",
    "$$a^{[l]<t>} = g\\left( W_a^{[l]} + \\left[ a^{[l]<t-1>}, a^{[l-1]<t>} \\right] + b_a^{[l]} \\right)$$\n",
    "\n",
    "Deep-RNNs can become computationally very expensive quickly, therefore they usually do not contain as many stacked layers as we would expect in a conventional Deep-NN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
