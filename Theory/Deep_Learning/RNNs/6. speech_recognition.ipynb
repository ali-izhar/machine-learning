{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0169e8e2",
   "metadata": {},
   "source": [
    "# Speech Recognition\n",
    "<hr>\n",
    "\n",
    "The problem in speech recognition is that there is usually much more input than output data. Take for example the sentence \"The quick brown fox jumps over the lazy dog.\" which consists of 35 characters. An audio clip of a recording of this sentence which is 10s long and was recorded with a sampling rate of 100Hz (100 samples per second) however has $10 \\times 100 = 1,000$ input samples! The samples of an audio clip can be visualized using a spectrogram.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/spectrogram.png\" width=500>\n",
    "    <center><caption><font color=\"purple\">Example of a spectrogram</font></caption></center>\n",
    "</div>\n",
    "\n",
    "Traditional approaches to get the transcription for a piece of audio involved aligning individual sounds (phonemes) with the audio signal. For this, a phonetic transcription of the text using the International Phonetic alphabet (IPA) was needed. The alignment process then involved detecting individual phonemes in the audio signal and matching them up with the phonetic transcription. To do this, Hidden Markov Models (HMM) were often used. This method was state of the art for a long time, however it required an exact phonetic transcription of the speech and was prone to features of the audio signal (e.g. sampling rate, background noise, number of speakers), the text being spoken (intonation, pronunciation, language, speaking rate) or the speaker itself (pitch of the voice, gender, accent).\n",
    "\n",
    "A more recent approach for speech recognition is a technique called **Connectionist Temporal Classification (CTC).** In contrast to HMM, CTC does not need an exact phonetic transcription of the speech audio (i.e. it is alignment-free). The CTC method allows for directly transforming an input signal using an RNN. This is constrasting to the HMM approach where the transcript first had to be mapped to a phonetic translation and the audio signal was then mapped to the individual phonemes. The whole process allows the RNN to output a sequence of characters that is much shorter than the sequence of input tokens.\n",
    "\n",
    "The underlying principle of CTC is that the input (i.e. spectrograms) are each mapped not only to a single character, but to all characters of the alphabet at the same time (with different probabilities). This may output in a sequence of characters like this:\n",
    "\n",
    "```\n",
    "ttt_h_eee______ ______q____uuu_i______cccc____k ...\n",
    "```\n",
    "\n",
    "Where we'd then merge characters not separated by the blank \"\\_\" character.\n",
    "\n",
    "```\n",
    "the quick ...\n",
    "```\n",
    "\n",
    "This process usually generates a whole bunch of possible output sequences, which can then be further reduced by passing it through a CTC-cost function, which collapses repeated characters. By doing this, we get a set of possible transcriptions, which can then be evaluated by means of a language model, which yields the most likely sequence to form the transcript for the audio.\n",
    "\n",
    "The main advantage of CTC is that it not only outpeforms all previous models but also that it does not require an intermediate phonetic transcription (i.e. it is an end-to-end model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7538c",
   "metadata": {},
   "source": [
    "## Trigger word detection\n",
    "<hr>\n",
    "\n",
    "A special application of speech recognition is trigger word detection, where the focus lies on detecting certain words inside an audio signal to trigger some action. Such systems are widely used in mobile phones or home speakers to wake up the device and make it listen to further instructions like \"Alexa\", \"Hey Siri\", etc.\n",
    "\n",
    "To train such a system the label for the signal can be simplified by marking it as 0 for time slots where the trigger word is not being said an 1 right after the trigger word was said. Usually a row of ones are used to prevent the amount of zeros being overly large and also because the end of the trigger word might not be easy to define exactly.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/trigger-word-detection.png\" width=700>\n",
    "    <center><caption><font color=\"purple\">Trigger word detection</font></caption></center>\n",
    "</div>\n",
    "\n",
    "The labelling of input data in trigger word detection can also be illustrated by visualizing the audio clip's spectrogram together with the $y$-labels. The following figure contains the spectrogram of an audio clip containing the words \"innocent\" and \"baby\" as well as the activation word \"activate\".\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/trigger-word-spectrogram.png\" width=500>\n",
    "    <center><caption><font color=\"purple\">Trigger word spectrogram</font></caption></center>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
