{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52933e67",
   "metadata": {},
   "source": [
    "# Attention Models\n",
    "<hr>\n",
    "\n",
    "So far, the task of machine translation has only been exemplified with sequence models following an encoder-decoder architecture where one RNN \"reads\" a sentence and encodes it as a vector and another RNN makes the translation. This works well for comparatively short sentences. However for longer sentences, the performance (measured by Bleu score) will decrease: translating a very long sentence chunk by chunk (like a human would), it is difficult to make an RNN memorize the whole sentece because it is processed all in one go.\n",
    "\n",
    "A modification to the encoder-decoder architecture are **attention models (AM).** AM processes a sentence similarly to how a human would by splitting it up into several chunks (contexts) of equal size and translating each chunk separately. This is especially useful in tasks with real-time requirements like speech recognition or simultaneous translation where you usually don't want to wait for the whole input to be available before making a prediction. For each context $c$ the model computes the amount of attention it should pay to each word. The output for this chunk serves as input for the next chunk.\n",
    "\n",
    "As an example, consider the chunk \"Jane visite l'Afrique en septembre ...\" from a much longer French sentence. This chunk of tokens $x^{<t'>}$ is being processed by a bidirectional RNN which acts as an encoder-network by encoding the chunk as set of features $a^{<t'>} = \\left( \\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>} \\right)$ (one feature per word). Note that $t'$ denotes the time step for the current chunk whereas $t$ denotes the time step over the whole sequence.\n",
    "\n",
    "Those features are then weighted by weights $\\alpha^{<t,t'>}$ which must sum up to 1. Those weights indicate how much attention the model should pay to the specific feature (therefore the term attention model). The weighted features are then summed up to form a context $c^{<t>}$. A different context is calculated for each time step $t$ with different weights. All the contexts are then processed by an unidirectional decoder-RNN which makes the actual predictions $\\hat{y}^{<t>}$.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/attention-model.png\" width=800>\n",
    "</div>\n",
    "\n",
    "The attention weights can be learned as follows:\n",
    "\n",
    "$$\\alpha ^{<t,t'>} = \\frac{exp\\left( e^{<t,t'>}\\right)}{\\sum_{t'=1}^{T_x} exp\\left( e^{<t,t'>}\\right) }$$\n",
    "\n",
    "Note that the above formula only makes the attention weights sum up to 1. The actual attention weights are in the parameter $e$, which is a trainable parameter that is learned by the decoder network, which can be trained by a very small neural network itself:\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/attention-model-e.png\" width=300>\n",
    "</div>\n",
    "\n",
    "The figure below shows an example for an attention model as well as the calculation of the attention weights inside a single step.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/attention-model-step.png\">\n",
    "</div>\n",
    "\n",
    "\n",
    "The magnitude of the different attentions during processing can further be visualized:\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/attention-model-weight-visualized.png\" width=400>\n",
    "</div>\n",
    "\n",
    "The advantage of an attention model is that it does not process individual words one by one, but rather pays different degrees of attention to different parts of a sentence during processing. This makes them a good fit for tasks like machine translation or image captioning. On the downside the model takes quadratic time to train because for $T_x$ input tokens and $T_y$ output tokens, the number of trainable parameters are $T_x \\times T_y$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
