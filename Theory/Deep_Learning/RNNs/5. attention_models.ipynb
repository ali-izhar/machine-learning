{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52933e67",
   "metadata": {},
   "source": [
    "# Attention models\n",
    "<hr>\n",
    "\n",
    "So far, the task of machine translation has only been exemplified with sequence models following an encoder-decoder architecture where one RNN \"reads\" a sentence and encodes it as a vector and another RNN makes the translation. This works well for comparatively short sentences. However for longer sentences, the performance (measured by Bleu score) will decrease: translating a very long sentence chunk by chunk (like a human would), it is difficult to make an RNN memorize the whole sentece because it is processed all in one go.\n",
    "\n",
    "A modification to the encoder-decoder architecture are **attention models (AM).** AM processes a sentence similarly to how a human would by splitting it up into several chunks (contexts) of equal size and translating each chunk separately. This is especially useful in tasks with real-time requirements like speech recognition or simultaneous translation where you usually don't want to wait for the whole input to be available before making a prediction. For each context $c$ the model computes the amount of attention it should pay to each word. The output for this chunk serves as input for the next chunk.\n",
    "\n",
    "As an example, consider the chunk \"Jane visite l'Afrique en septembre ...\" from a much longer French sentence. This chunk of tokens $x^{<t'>}$ is being processed by a bidirectional RNN which acts as an encoder-network by encoding the chunk as set of features $a^{<t'>} = \\left( \\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>} \\right)$ (one feature per word). Note that $t'$ denotes the time step for the current chunk whereas $t$ denotes the time step over the whole sequence.\n",
    "\n",
    "Those features are then weighted by weights $\\alpha^{<t,t'>}$ which must sum up to 1. Those weights indicate how much attention the model should pay to the specific feature (therefore the term attention model). The weighted features are then summed up to form a context $c^{<t>}$. A different context is calculated for each time step $t$ with different weights. All the contexts are then processed by an unidirectional decoder-RNN which makes the actual predictions $\\hat{y}^{<t>}$.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/attention-model.png\" width=800>\n",
    "</div>\n",
    "\n",
    "The attention weights can be learned as follows:\n",
    "\n",
    "$$a^{<t,t'>} = \\frac{exp\\left( e^{<t,t'>}\\right)}{\\sum_{t'=1}^{T_x} exp\\left( e^{<t,t'>}\\right) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a027a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
