{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeefe44b",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Models\n",
    "<hr>\n",
    "\n",
    "We have learned that RNNs with a many-to-many architecture (with encode-decoder networks as a special form) take a sequence as an input and also produce a sequence as output. Such models are called **Sequence-to-Sequence (S2S) models.** Such networks are traditionally used in machine translation where the input consists of sentences, which are transformed by an encoder network to serve as the input for the decoder network which does the actual translation. The output is then again a sequence, namely the translated sentence. The same process works likewise for other data. You could for instance take an image as input and have an RNN try to produce a sentence that states what is on the picture (image captioning). The encoder network in this case could then be a conventional CNN whereas the last layer will contain the image as a vector. This vector is then served to an RNN which acts as the decoder network to make predictions. Assuming you have enough already captioned images as training data an RNN could then learn how to produce captions for yet unseen images.\n",
    "\n",
    "## S2S in Machine Translation\n",
    "\n",
    "There are certain similarities between S2S-models and the previously seen language-models where we had an RNN produce a sequence of words based on the probability of previously seen words in the sequence:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/language-model.png\" width=300>\n",
    "    <center><caption><font color=\"purple\"><b>Figure 1:</b> Structure of a language model</font></caption></center>\n",
    "</div>\n",
    "\n",
    "In such a setting, the output was generated by producing a somewhat random sequence of words. However, that is not what we want in machine translation. Here we usually want the most likely sentence that corresponds to the best translation for a given input sentence. This is a key difference between language models as seen before and machine translation models. So in contrast to the model above, a machine translation model does not start with the zero vector as the first token, but rather takes a whole sentence and encodes it using the encoder part of the network.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/machine-translation.png\" width=450>\n",
    "    <center><caption><font color=\"purple\"><b>Figure 2:</b> Structure of a machine translation model</font></caption></center>\n",
    "</div>\n",
    "\n",
    "In that respect one can think of a machine translation model as kind of **conditional language model** that calculates the probability of a translated sentence given the input sentence in the original language: $P$( translation $\\mid$ input language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e71202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb323a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde2b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
