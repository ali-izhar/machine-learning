{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd24cf82",
   "metadata": {},
   "source": [
    "# 1 - Semantic Segmentation\n",
    "\n",
    "The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. Because we're predicting for every pixel in the image, this task is commonly referred to as dense prediction.\n",
    "\n",
    "Note that unlike the previous tasks, the expected output in semantic segmentation are not just labels and bounding box parameters. The output itself is a high resolution image (typically of the same size as input image) in which each pixel is classified to a particular class. Thus it is a pixel level image classification.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/instance_seg.png\" width=800>\n",
    "</div>\n",
    "\n",
    "To generate this pixel-level output image, semantic segmentation models often have a U-shape architecture:\n",
    "- First sample down to capture context and high-level features\n",
    "- Then sample back up to the original input resolution using upsampling and ***transpose convolutions*** in order to output a dense prediction with the same dimensions.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/segmen_example.png\" width=800>\n",
    "    <caption><center><font color=\"purple\">Down-sampling (first half) and then Up-sampling (latter half) using Transpose Convolution for Semantic Segmentation</font></center></caption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c0219",
   "metadata": {},
   "source": [
    "## 1.1 - Transpose Convolution\n",
    "\n",
    "A transpose convolution (also called a deconvolution) upsamples a smaller input to produce a larger output, opposite to a normal convolution which downsamples the input.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/normal_vs_tranpose.png\" width=700>\n",
    "    <caption><center><font color=\"purple\">Normal vs. Transpose Convolution</font></center></caption>\n",
    "</div>\n",
    "\n",
    "In a normal convolution, we convolve a filter over the input image which results in a smaller output dimension. In a transpose convolution, we connect the filter to the output image - then back-propagate to determine what input image would produce this output. This results in a larger input dimension\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/transpose_conv.png\" width=800>\n",
    "    <caption><center><font color=\"purple\">Applying 2x2 filter to an input image 2x2 to obtain 3x3 output image</font></center></caption>\n",
    "</div>\n",
    "\n",
    "For a different set of configuations:\n",
    "- Input image: $2 \\times 2$\n",
    "- Filter size: $3 \\times 3$\n",
    "- Padding: $p=1$\n",
    "- Stride: $s=2$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/transpose_conv1.png\" width=800>\n",
    "    <caption><center><font color=\"purple\">Applying 3x3 filter to an input image 2x2 to obtain 4x4 output image</font></center></caption>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820ef7e",
   "metadata": {},
   "source": [
    "## 1.2 - Skip Connections\n",
    "\n",
    "The U-Net architecture for semantic segmentation consists of two main parts:\n",
    "\n",
    "1. An encoder similar to a normal CNN that compresses the input image into a smaller representation, capturing high-level context but losing spatial details.\n",
    "2. A decoder that upsamples the representation back to the input dimensions using transpose convolutions. This allows dense prediction at the pixel level.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/unet-skip-conn.png\" width=800>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4019b65",
   "metadata": {},
   "source": [
    "# 2. U-Net Architecture\n",
    "\n",
    "The U-Net architecture consists of two main pathways:\n",
    "\n",
    "**Encoder (contracting path):**\n",
    "- Typical CNN layers to compress the input image into feature representations, capturing higher-level context but losing spatial details. Uses convolutions and max pooling.\n",
    "\n",
    "\n",
    "**Decoder (expanding path):**\n",
    "- Transpose convolutions to upsample features back to original input resolution so dense predictions can be made at pixel level.\n",
    "\n",
    "The key innovation in U-Net is the use of skip connections to bypass information from the encoder layers directly to the decoder layers at the same spatial scale. This helps the decoder recover spatial details lost during downsampling.\n",
    "\n",
    "So the decoder combines:\n",
    "- High-level context from the encoder path\n",
    "- Fine-grained localization from the skip connections\n",
    "\n",
    "This provides sufficient information to accurately classify pixels for semantic segmentation. The overall architecture looks like a U shape, with encoder and decoder paths bridged by skip connections, enabling precise localization while leveraging context.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/unet.png\" width=900>\n",
    "</div>\n",
    "\n",
    "**Left Half**\n",
    "- Apply normal convolutions to reduce height and width but increase depth.\n",
    "\n",
    "**Right Half**\n",
    "1. Apply transpose convolution to reduce depth (same heigh and width). Using skip connection, copy over the activations from the encode part.\n",
    "2. Apply transpose convolution to reduce depth, but this time increase height and width and also copy over the activations from the encoder part.\n",
    "3. At the top layer, after applying the last transpose convolution, the dimensions are the same as the input.\n",
    "4. Lastly, apply a $1 \\times 1$ convolution to output a $(h_{\\text{in}}, w_{\\text{in}}, n_{\\text{classes}})$ volume. In the image above, since we're interested in segmenting 3 classes; if the input image was $19 \\times 19$, the output volume would be $19 \\times 19 \\times 3$ where 3 is the desired classes (not the depth or number of channels)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
