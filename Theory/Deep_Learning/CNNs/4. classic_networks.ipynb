{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792203f3",
   "metadata": {},
   "source": [
    "# Classic Networks\n",
    "\n",
    "In this notebook, we'll learn about some of the classic CNN architectures.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797bf97",
   "metadata": {},
   "source": [
    "## LeNet-5 (1998)\n",
    "\n",
    "The goal for this model was to identify handwritten digits in a $32 \\times 32 \\times 1$ gray image.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/lenet5.png\">\n",
    "    <caption><font color=\"red\">Parameters: 60k</font></caption>\n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "- **Initial Input:** The input is an image of size 32x32x1 (e.g., grayscale handwritten digits).\n",
    "\n",
    "\n",
    "- **First Convolutional Layer:**\n",
    "    - Uses 6 filters of size 5x5, with a stride of 1.\n",
    "    - The absence of padding reduces the image size to 28x28.\n",
    "    - The output dimension becomes 28x28x6.\n",
    "- **First Pooling Layer:**\n",
    "    - Applies average pooling (though modern implementations might prefer max pooling).\n",
    "    - Uses a 2x2 window with a stride of 2, halving the dimensions to 14x14x6.\n",
    "\n",
    "\n",
    "- **Second Convolutional Layer:**\n",
    "    - Employs 16 filters of size 5x5, again without padding.\n",
    "    - This reduces the dimensions to 10x10x16.\n",
    "- **Second Pooling Layer:**\n",
    "    - Similar to the first pooling layer, further reducing the dimensions to 5x5x16.\n",
    "\n",
    "\n",
    "- **Flattening:**\n",
    "    - The output from the previous layer is flattened, resulting in a vector of size 400 (5x5x16).\n",
    "- **Fully Connected Layers:**\n",
    "    - The network includes fully connected layers, with the first having 120 neurons, and the second having 84 neurons.\n",
    "- **Output Layer:**\n",
    "    - The final layer originally used a different classification approach, but modern implementations would use a softmax layer for 10-way classification (digits 0-9).\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Characteristics**\n",
    "- LeNet-5 has approximately 60,000 parameters, which is small compared to modern networks.\n",
    "- The network depth leads to a decrease in spatial dimensions (height and width) and an increase in the number of channels through the layers.\n",
    "- The architecture typically follows a pattern of convolutional layers followed by pooling layers, and then fully connected layers leading to the output.\n",
    "\n",
    "**Historical Context**\n",
    "- The original LeNet-5 used sigmoid and tanh activation functions instead of ReLU.\n",
    "- Some unique wiring in the convolutional layers was used due to computational limitations at the time.\n",
    "- The original LeNet-5 applied a non-linearity (sigmoid) after the pooling layers, which is uncommon in modern architectures.\n",
    "\n",
    "[LeCun et al., 1998. Gradient-based learning applied to document recognition]\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd54297",
   "metadata": {},
   "source": [
    "## AlexNet (2012)\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/alexnet.png\">\n",
    "    <caption><font color=\"red\">Parameters: 60M</font></caption>\n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "- **Input Dimensions:** AlexNet processes images of size 227x227x3 (although the original paper mentioned 224x224x3, the architecture works with 227x227x3).\n",
    "\n",
    "\n",
    "- **First Convolutional Layer:**\n",
    "    - Uses 96 filters, each of size 11x11, with a stride of 4.\n",
    "    - The output dimension becomes 55x55x96 due to the large stride.\n",
    "- **First Max Pooling Layer:**\n",
    "    - Applies max pooling with a 3x3 filter and a stride of 2.\n",
    "    - Reduces the dimension to 27x27x96.\n",
    "\n",
    "\n",
    "- **Second Convolutional Layer:**\n",
    "    - Employs a 5x5 convolution with same padding, increasing the number of filters to 256.\n",
    "    - Maintains the dimension at 27x27x256.\n",
    "- **Second Max Pooling Layer:**\n",
    "    - Further applies max pooling, reducing the dimensions to 13x13x256.\n",
    "\n",
    "\n",
    "- **Additional Convolutional Layers:**\n",
    "    - Several more convolutional layers are added, using 3x3 filters with same padding.\n",
    "    - Increases the number of filters first to 384, then again to 256.\n",
    "    - A final max pooling layer reduces the dimensions to 6x6x256.\n",
    "\n",
    "\n",
    "- **Flattening:**\n",
    "    - Flattens the output to prepare for fully connected layers, resulting in 9216 features (6x6x256).\n",
    "- **Fully Connected Layers:**\n",
    "    - Includes multiple fully connected layers, the final layer using softmax activation for a 1000-way classification (reflecting the 1000 different classes in the ImageNet dataset).\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Notable Characteristics**\n",
    "- AlexNet is significantly larger than LeNet-5, with approximately 60 million parameters.\n",
    "- It was one of the first successful applications of deep learning in computer vision, particularly in the ImageNet challenge.\n",
    "- Uses ReLU (Rectified Linear Unit) activation functions, which was a key factor in its performance.\n",
    "\n",
    "**Special Features**\n",
    "- Trained on two GPUs due to hardware limitations at the time.\n",
    "- Included Local Response Normalization (LRN), a technique not commonly used in modern architectures.\n",
    "\n",
    "**Historical Impact**\n",
    "- AlexNet significantly influenced the computer vision community, showcasing the effectiveness of deep learning in this field.\n",
    "- It paved the way for further advances in deep learning across various applications, not just in computer vision.\n",
    "\n",
    "[Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks]\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e8c0e",
   "metadata": {},
   "source": [
    "## VGG-16 (2015)\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/vgg16.png\">\n",
    "    <caption><font color=\"red\">Parameters: ~ 138M</font></caption>\n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "- **Initial Input:** The input is an image of size 224x224x3.\n",
    "\n",
    "\n",
    "- **Convolutional Layers:**\n",
    "    - The first two layers use 64 filters of size 3x3, with same padding and a stride of 1, maintaining the dimension at 224x224x64.\n",
    "    - This pattern of using 3x3 filters with a stride of 1 and same padding is consistent throughout the network.\n",
    "- **Max Pooling Layers:**\n",
    "    - Follows each set of convolutional layers.\n",
    "    - Uses a 2x2 window with a stride of 2, halving the dimensions each time.\n",
    "    - First pooling layer reduces the dimension to 112x112x64.\n",
    "\n",
    "\n",
    "- **Increasing Filter Depth:**\n",
    "    - After each pooling layer, the number of filters doubles $(64 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512)$.\n",
    "    - The network includes multiple sets of convolutional layers before each pooling layer, with the number of filters increasing as the network goes deeper.\n",
    "\n",
    "- **Fully Connected Layers:**\n",
    "    - After the final pooling layer, the network flattens the output and passes it through three fully connected layers.\n",
    "    - The first two fully connected layers have 4096 units each.\n",
    "    - The final layer uses a softmax activation function for classification into 1000 classes.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Network Size:**\n",
    "- VGG-16 is a large network with approximately 138 million parameters.\n",
    "- The '16' in VGG-16 refers to the number of layers with weights (convolutional and fully connected layers).\n",
    "\n",
    "\n",
    "**Design Uniformity:**\n",
    "- VGG-16 is known for its uniform architecture, making it easy to understand and modify.\n",
    "- The network architecture systematically <font color='red'>doubles the number of filters while halving the height and width of the feature maps.</font> For instance:\n",
    "    - Filters: $(64 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512)$\n",
    "    - Feature Maps: $(224 \\rightarrow 112 \\rightarrow 56 \\rightarrow 28 \\rightarrow 14 \\rightarrow 7)$\n",
    "\n",
    "\n",
    "**VGG-19 Variant:**\n",
    "- There is a larger variant known as VGG-19, but VGG-16 is more commonly used due to its comparable performance and slightly smaller size.\n",
    "    \n",
    "\n",
    "**Historical Significance:**\n",
    "- VGG-16, developed by Karen Simonyan and Andrew Zisserman, is one of the most influential architectures in deep learning, particularly for its simplicity and uniformity.\n",
    "- It was one of the key architectures that highlighted the effectiveness of deep convolutional networks in image recognition tasks.\n",
    "\n",
    "[Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition]\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
