{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cda955",
   "metadata": {},
   "source": [
    "# Inception Network\n",
    "<hr>\n",
    "\n",
    "**Multiple Filters in One Layer:** Instead of deciding between different filter sizes (like 1x1, 3x3, or 5x5) or pooling layers for a CNN, the Inception network proposes using all of them simultaneously within the same layer, known as the **Inception module.**\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/inception.png\">\n",
    "</div>\n",
    "\n",
    "\n",
    "**Naive Version of Inception Module:**\n",
    "Consider, for example, the following inception module that takes an input volume of 28x28x192 and applies various filters and pooling in parallel.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/inception1.png\" width=600>\n",
    "</div>\n",
    "\n",
    "\n",
    "Each path uses different filter sizes or pooling, and their outputs are concatenated, resulting in an output volume of 28x28x256.\n",
    "\n",
    "- **Computational Cost Problem**\n",
    "    - A naive Inception module can be computationally expensive, especially with larger filter sizes like 5x5.\n",
    "    - For example, using a 5x5 filter with 32 channels on a 28x28x192 input results in 28x28x32 volume. The total number of multiplications are:\n",
    "        - Each output position is the result of applying the 5x5x192 filter to the input volume. \n",
    "        - For a total of 28x28x32 output volume, the number of multiplications are: $(5 \\times 5 \\times 192) \\times (28 \\times 28 \\times 32) \\approx 120M$ multiplications.\n",
    "\n",
    "\n",
    "- **Bottleneck Layer to Reduce Cost**\n",
    "    - To mitigate computational costs, 1 x 1 convolutions, known as bottleneck layers, are used before larger convolutions.\n",
    "    - These layers reduce the depth of the input volume before applying the expensive 5x5 convolutions, dramatically cutting down the number of multiplications needed.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/bottleneck.png\" width=700>\n",
    "</div>\n",
    "\n",
    "\n",
    "- **Reduced Computational Cost with 1 x 1 Convolutions**\n",
    "    - The bottleneck architecture starts with a 1 x 1 convolution that reduces the depth from 192 to 16.\n",
    "    - A subsequent 5 x 5 convolution produces the desired 28x28x32 output.\n",
    "    - Multiplications needed are:\n",
    "        - For the bottleneck layer, there are $(1 \\times 1 \\times 192) \\times (28 \\times 28 \\times 16) \\approx 2.4M$ multiplications.\n",
    "        - For the inception module, there are $(5 \\times 5 \\times 16) \\times (28 \\times 28 \\times 32) \\approx 10M$ multiplications.\n",
    "        - In total, there are $\\approx 12.4M$ multiplications, a $\\frac{1}{10}$ drop from the previouly 120M.\n",
    "\n",
    "\n",
    "- **Efficiency and Performance**\n",
    "    - The reduced representation, surprisingly, does not significantly affect network performance.\n",
    "    - The inception module allows for a deep and wide architecture without an excessive computational burden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12920c97",
   "metadata": {},
   "source": [
    "# Inception Module\n",
    "<hr>\n",
    "\n",
    "An inception module, continuining from the previous example, could look like this:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/inception_module.png\" width=700>\n",
    "    <caption><font color=\"red\"><u>An Inception Module</u></font></caption>\n",
    "</div>\n",
    "\n",
    "Where channel concatenation is the concatenation of all channels $(64 + 128 + 32 + 32) = 256$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2d9b3",
   "metadata": {},
   "source": [
    "# GoogLeNet \n",
    "<hr>\n",
    "\n",
    "**Inception Network (GoogleNet)**\n",
    "- GoogleNet consists of a sequence of concatenated Inception modules.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/inception_network.png\">\n",
    "    <caption><font color=\"red\"><u>Inception Network (GoogLeNet)</u></font></caption>\n",
    "</div>\n",
    "\n",
    "**Model Architecture**\n",
    "- The full model is built from multiple Inception modules stacked together.\n",
    "- Max-Pooling layers are occasionally inserted before Inception modules to reduce the dimensionality of the input data.\n",
    "\n",
    "\n",
    "**Softmax Branches**\n",
    "- The network includes three softmax branches at various depths. These auxiliary classifiers provide intermediate supervision, pushing the network towards the correct output earlier during training.\n",
    "- Softmax0 and Softmax1, the classifiers in the intermediate layers, also impart a regularization effect, which can improve generalization and prevent overfitting.\n",
    "\n",
    "\n",
    "**Evolution of Inception Networks**\n",
    "- Following the original Inception module, several iterations and improvements have been made, resulting in Inception v2, v3, and v4.\n",
    "- These newer versions incorporate advanced techniques like batch normalization, factorized convolutions, and label smoothing for enhanced performance.\n",
    "\n",
    "\n",
    "**Combination with ResNet**\n",
    "- There are also architectures that combine the concepts of the Inception module with Residual Networks (ResNets), leveraging the strengths of both designs.\n",
    "\n",
    "- The architecture's name is inspired by the film \"Inception\" reflecting the network's depth within depth structure. The meme was actually referenced in the original [paper](https://arxiv.org/pdf/1409.4842v1.pdf).\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/meme.jpg\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
