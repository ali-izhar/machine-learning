{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbc6f1c",
   "metadata": {},
   "source": [
    "# Residual Networks (ResNets)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- Very deep neural networks are hard to train due to vanishing and exploding gradients. ResNet addresses this problem.\n",
    "- Skip (or shortcut) connections allow activations from one layer to be fed directly to a later layer, bypassing one or more intermediate layers. This is the core idea behind ResNets.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/skip_connection.jpg\" width=500>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/resnet_paths.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Residual Block:**\n",
    "- The fundamental unit of ResNet.\n",
    "- In a traditional sequential network, an activation $a^{[l]}$ goes through transformations to become $a^{[l+2]}$.\n",
    "- In a residual block, the original activation $a^{[l]}$ is added directly to the output of the layer(s) before applying the final activation function, like ReLU. This is $a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$, where $g$ is the ReLU function.\n",
    "\n",
    "**Network Architecture:**\n",
    "- ResNets are composed of stacked residual blocks.\n",
    "- Each block has two or more layers, with skip connections adding the input of the block to its output.\n",
    "\n",
    "**Effectiveness in Training Deep Networks:**\n",
    "- Allows training of very deep networks, over 100 layers, by mitigating the vanishing/exploding gradient problems.\n",
    "- Empirically, ResNets continue to benefit from increased depth, showing decreased training error with more layers.\n",
    "\n",
    "**Comparison to Plain Networks:**\n",
    "- In plain deep networks (without skip connections), increasing depth can lead to increased training error after a certain point.\n",
    "- ResNets, however, demonstrate continued improvement in training performance with added depth.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/plain_vs_resnet.png\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/plain_vs_resnet1.png\" width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c8ff5",
   "metadata": {},
   "source": [
    "## Why do ResNets Work?\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/resnet_identity.png\" width=500>\n",
    "</div>\n",
    "\n",
    "If we apply ReLU, then we know that $a^{[l]} \\geq 0$.\n",
    "\n",
    "\\begin{align}\n",
    "a^{[l+2]} &= g\\left( z^{[l+2]} + a^{[l]} \\right) & (\\text{by skip connection}) \\\\\n",
    "&= g\\left( W^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]} \\right) \\\\\n",
    "&= g\\left(a^{[l]} \\right) & (\\text{if} \\ W^{[l+1]} = b^{[l+1]} = 0) \\\\\n",
    "&= a^{[l]} \\\\\n",
    "\\end{align}\n",
    "\n",
    "- One of the main reasons ResNets work well is their ability to easily learn the identity function. In a ResNet, when the optimal function for a layer is close to the identity function, the network can easily push the weights towards zero, effectively making the layer approximate an identity mapping.\n",
    "- This means that adding extra layers doesn't necessarily hurt performance because these layers can effectively become \"no-op\" operations if needed, ensuring that the network's performance doesn't degrade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683bbe2",
   "metadata": {},
   "source": [
    "## Spectrum of Depth\n",
    "<hr>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/depth_spectrum.png\" width=700>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Identity Block**\n",
    "- The conv is followed by a batch norm `BN` before `ReLU`. Dimensions here are same.\n",
    "- This skip is over 2 layers. The skip connection can jump $n$ connections where $n>2$.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/identity_block.png\" width=800>\n",
    "</div>\n",
    "\n",
    "**Convolution Block**\n",
    "- The conv can be bottleneck $1 \\times 1$ conv.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"media/conv_block.png\" width=800>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
