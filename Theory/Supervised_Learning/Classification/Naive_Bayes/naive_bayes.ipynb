{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365684e8",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "The Naive Bayes classifiers are a family of probabilistic classifiers that are based on applying Bayes' theorem with naive assumption on independence between the features.\n",
    "\n",
    "These classifiers are extremely fast both in training and prediction, and they are also highly scalable and interpretable. Despite their oversimplified assumptions, they often work well on complex real-world problems, especially in text classification tasks such as spam filtering and sentiment analysis, where their naive assumption largely holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42977b24",
   "metadata": {},
   "source": [
    "## Background: Bayes’ Theorem\n",
    "Bayes' theorem (or Bayes' rule) is an important theorem in probability that allows us to compute the conditional probability of an event, based on prior knowledge of conditions that are related to that event.\n",
    "\n",
    "Mathematically, the theorem states that for any events $A$ and $B$:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
    "\n",
    "- $P(A \\mid B)$ is the **posterior probability** of $A$ given $B$, i.e., the probability of event $A$ occurring given that $B$ has occurred.\n",
    "- $P(B \\mid A)$ is the **likelihood** of $B$ given $A$, i.e., the probability of event $B$ occurring given that $A$ has occurred.\n",
    "- $P(A)$ is the **prior probability** of $A$, i.e., the probability of $A$ without any prior conditions.\n",
    "- $P(B)$ is the **marginal probability** of $B$, i.e., the probability of $B$ without any prior conditions.\n",
    "\n",
    "Bayes’ theorem is particularly useful for inferring causes from their effects, since it is often easier to discern the probability of an effect given the presence or absence of its cause, rather than the other way around. For example, it is much easier to estimate the probability that a patient with meningitis will suffer from a headache, than the other way around (since many other diseases can cause an headache). In such cases, we can apply Bayes’ rule as follows:\n",
    "\n",
    "$$P(\\text{cause} \\mid \\text{effect}) = \\frac{P(\\text{effect} \\mid \\text{cause}) P(\\text{cause})}{P(\\text{effect})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac0d4c",
   "metadata": {},
   "source": [
    "### Example\n",
    "It is known that approximately $25\\%$ of patients with lung cancer suffer from a chest pain. Assume that the incidence rate of lung cancer is $50$ per $100,000$ individuals, and the incidence rate of chest pain is $1,500$ per $100,000$ individuals worldwide. What is the probability that a patient with chest pain has a lung cancer?\n",
    "\n",
    "Let’s write the given inputs in the form of probabilities. Let $L$ be the event of having lung cancer, and $C$ the event of having a chest pain. From the data we have we know that:\n",
    "\n",
    "$$P(C \\mid L) = 0.25$$\n",
    "\n",
    "$$P(L) = \\frac{50}{100,000} = 0.0005$$\n",
    "\n",
    "$$P(C) = \\frac{1,500}{100,000} = 0.015$$\n",
    "\n",
    "Using Bayes' rule, the posterior probability of having a lung cancer given a chest pain is:\n",
    "\n",
    "$$P(L \\mid C) = \\frac{P(C \\mid L) P(L)}{P(C)} = \\frac{0.25 \\times 0.0005}{0.015} = 0.00833$$\n",
    "\n",
    "i.e., there is only a $0.833\\%$ chance that the patient has a lung cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b59028",
   "metadata": {},
   "source": [
    "## The Naive Bayes Model\n",
    "The Naive Bayes models are probabilistic classifiers, i.e., they not only assign a class label to a given sample, but they also provide an estimate of the probability that it belongs to that class. For example, a Naive Bayes model can predict that a given email has $80\\%$ chance of being a spam and $20\\%$ chance of being a ham.\n",
    "\n",
    "Given a sample $(x, y)$, a Naive Bayes classifier computes its probability of belonging to class $k$ (i.e., $y = k$) using Bayes' rule:\n",
    "\n",
    "$$P(y = k \\mid x) = \\frac{P(x \\mid y = k) P(y = k)}{P(x)}$$\n",
    "\n",
    "The probabilities on the right side of the equation are estimated from the training set. First, the **class prior probability** $P(y = k)$ can be estimated from the relative frequency of class $k$ across the training samples:\n",
    "\n",
    "$$P(y = k) = \\frac{n_k}{n}$$\n",
    "\n",
    "where $n_k$ is the number of samples that belong to class $k$, and $n$ is the total number of samples in the training set.\n",
    "\n",
    "Second, the marginal probability $P(x)$ can be computed by summing the terms appearing in the numerator of the Bayes' rule over all the classes:\n",
    "\n",
    "$$P(x) = \\sum_{k=1}^K P(x \\mid y = k) P(y = k)$$\n",
    "\n",
    "Since the marginal probability does not depend on the class, it is not necessary to compute it if we are only interested in assigning a hard label to the sample (without providing probability estimates).\n",
    "\n",
    "Lastly, we need to estimate the likelihood of the features given the class, i.e., $P(x \\mid y = k)$. The main issue with estimating these probabilities is that there are too many of them, and we may not have enough data in the training set to estimate them all.\n",
    "\n",
    "For example, imagine that $x$ consists of $m$ binary features, e.g., each feature represents if a given word appears in the text document or not. In this case, in order to model $P(x \\mid y)$, we will have to estimate $2^m$ conditional probabilities from the training set for every class (one for each possible combination of $x_1, \\cdots, x_m$), hence $2^m K$ probabilities in total for $K$ training samples. In most cases, we will not have enough samples in the training set to estimate all these probabilities, and even if we had it would have taken us an exponential time to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1c874",
   "metadata": {},
   "source": [
    "## The Naive Bayes Assumption\n",
    "In order to reduce the number of parameters that need to be estimated, the Naive Bayes model makes the following assumption: \n",
    "\n",
    "> **the features are independent of each other given the class variable.**\n",
    "\n",
    "This assumption allows us to write the probability $P(x \\mid y = k)$ as a product of the conditional probabilities of each individual feature given the class:\n",
    "\n",
    "$$P(x \\mid y=k) = P(x_1 \\mid y=k) \\dot P(x_2 \\mid y=k) \\cdot P(x_m \\mid y=k) = \\prod_{j=1}^m P(x_j \\mid y=k)$$\n",
    "\n",
    "For example, in a spam filtering task, the Naive Bayes assumption means that words such as \"rich\" and \"prince\" contribute independently to the prediction if the email is spam or not, regardless of any possible correlation between these words.\n",
    "\n",
    "The naive assumption largely holds in application domains such as text classification and recommendation systems, where the features are generally independent of each other.\n",
    "\n",
    "The naive Bayes assumption reduces significantly the number of parameters that need to be estimated from the data.\n",
    "\n",
    "**Without the Naive Bayes Assumption:**\n",
    "- Each feature can take 2 values (0 or 1)\n",
    "- We need to estimate the probabilities of all possible combinations of these features.\n",
    "- For $m$ features, there are $2^m$ combinations.\n",
    "- Thus, we need to estimate $2^m$ parameters.\n",
    "    \n",
    "**With the Naive Bayes Assumption:**\n",
    "- We assume that each feature is independent of the others, given the class.\n",
    "- We only need to estimate the probability of each feature independently, which is $P(x_i \\mid y=k)$ for each $i$.\n",
    "- For $m$ features, this is $m$ probabilities.\n",
    "- Thus, we only need to estimate $m$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66853e7",
   "metadata": {},
   "source": [
    "## MAP (Maximum A-Posteriori)\n",
    "Based on the naive Bayes assumption, we can now write the class posterior probability as follows:\n",
    "\n",
    "$$P(y = k \\mid x) = \\frac{P(y=k) \\prod_{j=1}^m P(x_j \\mid y=k)}{P(x)}$$\n",
    "\n",
    "If we are only interested in assigning a class label to the given sample (and we do not care about the probabilities), we can ignore the denominator $P(x)$, and use the following classification rule:\n",
    "\n",
    "$$\\hat{y} = \\text{argmax}_{k} P(y=k) \\prod_{j=1}^m P(x_j \\mid y=k)$$\n",
    "\n",
    "This is called a **MAP (Maximum A-Posteriori)** decision rule, since it chooses the hypothesis that maximizes the posterior probability.\n",
    "\n",
    "Naive Bayes will make the correct MAP decision as long as the correct class is predicted as more probable than the other classes, even when the probability estimates are inaccurate. This provides some robustness to the model against the deficiencies in the underlying naive independence assumption.\n",
    "\n",
    "Note that if we assume that all the priors $P(y)$ are equally likely (e.g., when we don’t have any prior information on which hypothesis is more likely), then the MAP decision rule is equivalent to the **MLE (maximum likelihood estimation)** decision rule, which chooses the model that maximizes the likelihood of the data given the model $P(x \\mid y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239de36",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "We are now left with the task of estimating the conditional probabilities $P(x_j \\mid y = k)$ for each feature $j$ and for each class $k$. This estimation depends both on the type of the feature (e.g., discrete or continuous) and the probability distribution we assume that it has.\n",
    "\n",
    "The assumptions on the distribution of the features are called **event models.** Each event model leads to a different type of Naive Bayes classifier. In the following sections we will discuss the different event models and how to estimate the model parameters in each one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673b20b",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes\n",
    "In the Bernoulli event model, the features $x_j$ are modeled as independent binary variables with a **Bernoulli distribution,** i.e., each feature $x_j$ has a probability $p_j$ of appearing in the given sample and $1 − p_j$ for being absent from that sample. For example, in a text classification task, each feature $x_j$ may represent the occurrence or absence of the $j$-th word from the vocabulary in the text. \n",
    "\n",
    "In the Bernoulli event model, the probability $P(x_j \\mid y = k)$ is estimated from the frequency in which feature $j$ appears in samples of class $k$:\n",
    "\n",
    "$$P(x_j = 1 \\mid y=k) = \\frac{n_{jk}}{n_k}$$\n",
    "\n",
    "where $n_{jk}$ is the number of samples in class $k$ in which feature $x_j$ appears, and $n_k$ is the total number of samples in class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd7728",
   "metadata": {},
   "source": [
    "## Categorical Naive Bayes\n",
    "The categorical event model is an extension of the Bernoulli event model to $V$ categories (instead of only two). In this model, we assume that each feature is a categorical (discrete) variable that can take one of $V$ possible categories with some probability $p_i$, where the sum of all the probabilities is $1$.\n",
    "\n",
    "In this event model, we need to estimate the probability $P(x_j = v \\mid y = k)$ for every feature $x_j$ and every category $v$. Similar to the previous model, we estimate this probability as the frequency in which feature $j$ gets the value $v$ in samples of class $k$:\n",
    "\n",
    "$$P(x_j=v \\mid y=k) = \\frac{n_{jvk}}{n_k}$$\n",
    "\n",
    "where $n_{jvk}$ is the number of samples in class $k$ where feature $x_j$ gets the value $v$, and $n_k$ is the total number of samples in class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb1393",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Example: Customer Purchase Prediction\n",
    "Assume that we have the following table with data on the past purchases of customers in a store:\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/customer_data.png\" width=600>\n",
    "</div>\n",
    "\n",
    "Each row in the table contains the age of the customer, whether they are a student or not, their level of income, their credit rating and whether or not they have purchased the product.\n",
    "\n",
    "A new customer with the following properties arrives at the store:\n",
    "\n",
    "**<Age = Young, Student = Yes, Income = Low, Credit = Excellent>**\n",
    "\n",
    "You need to predict whether this customer will buy the product or not.\n",
    "\n",
    "We first compute the class prior probabilities by counting the number of rows that have Buys = Yes (6 out of 10) and the number of rows that have Buys = No (4 out of 10):\n",
    "\n",
    "$$P(\\text{Buys=Yes}) = \\frac{6}{10} = 0.6$$\n",
    "\n",
    "$$P(\\text{Buys=N}) = \\frac{4}{10} = 0.4$$\n",
    "\n",
    "Then, we compute the likelihood of the features in each class:\n",
    "\n",
    "$$P(\\text{Age=Young} \\mid \\text{Buys=Yes}) = \\frac{2}{6} = 0.333$$\n",
    "\n",
    "$$P(\\text{Age=Young} \\mid \\text{Buys=No}) = \\frac{2}{4} = 0.5$$\n",
    "\n",
    "$$P(\\text{Student=Yes} \\mid \\text{Buys=Yes}) = \\frac{4}{6} = 0.667$$\n",
    "\n",
    "$$P(\\text{Student=Yes} \\mid \\text{Buys=No}) = \\frac{2}{4} = 0.5$$\n",
    "\n",
    "$$P(\\text{Income=Low} \\mid \\text{Buys=Yes}) = \\frac{1}{6} = 0.167$$\n",
    "\n",
    "$$P(\\text{Income=Low} \\mid \\text{Buys=No}) = \\frac{2}{4} = 0.5$$\n",
    "\n",
    "$$P(\\text{Credit=Excellent} \\mid \\text{Buys=Yes}) = \\frac{4}{6} = 0.667$$\n",
    "\n",
    "$$P(\\text{Credit=Excellent} \\mid \\text{Buys=No}) = \\frac{1}{4} = 0.25$$\n",
    "\n",
    "Therefore, the class posterior probabilities are:\n",
    "\n",
    "$$P(\\text{Buys=Yes} \\mid x) = P(\\text{Yes}) \\cdot P(\\text{Young} \\mid \\text{Yes}) \\cdot P(\\text{Student} \\mid \\text{Yes}) \\cdot P(\\text{Low} \\mid \\text{Yes}) \\cdot P(\\text{Excellent} \\mid \\text{Yes}) = 0.6 \\cdot 0.333 \\cdot 0.667 \\cdot 0.667 \\alpha = 0.0148\\alpha$$\n",
    "\n",
    "$$P(\\text{Buys=No} \\mid x) = P(\\text{No}) \\cdot P(\\text{Young} \\mid \\text{No}) \\cdot P(\\text{Student} \\mid \\text{No}) \\cdot P(\\text{Low} \\mid \\text{No}) \\cdot P(\\text{Excellent} \\mid \\text{No}) = 0.4 \\cdot 0.5 \\cdot 0.5 \\cdot 0.5 \\alpha = 0.25\\alpha$$\n",
    "\n",
    "$\\alpha$ is the normalization factor $(\\alpha = \\frac{1}{P(x)})$\n",
    "\n",
    "Since $P(\\text{Buys=Yes} \\mid x) > P(\\text{Buys=No} \\mid x)$, our prediction is that the customer will buy the product.\n",
    "\n",
    "If we want to get the actual probability that the customer will buy the product, we can first find the normalization factor using the fact that the two posterior probabilities must sum to 1:\n",
    "\n",
    "$$0.0148 \\alpha + 0.0125 \\alpha = 1 \\rightarrow \\frac{1}{0.0148 + 0.0125} = 36.63$$\n",
    "\n",
    "Then, we can plug it in the posterior probability for $\\text{Buy=Yes}$:\n",
    "\n",
    "$$P(\\text{Buys=Yes} \\mid x) = 0.0148 \\cdot 36.63 = 0.5421$$\n",
    "\n",
    "The probability that the customer will buy the product is $54.21\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27a922",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "In the multinomial event model, we assume that the data set has only one categorical feature $x$ that can take one of $m$ categories, and each feature vector $(x_1, \\cdots, x_m)$ is a histogram, where $x_j$ counts the number of times $x$ had the value $j$ in that particular instance.\n",
    "\n",
    "This event model is particularly useful when working with text documents, where $m$ is the number of words in the vocabulary, and each feature $x_j$ represents the number of times the $j$-th word from the vocabulary appears in the document. This representation is called the **bag-of-words** model:\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/bag_of_words.png\" width=600>\n",
    "</div>\n",
    "\n",
    "In this event model, we estimate the probability $P(x=v \\mid y=k)$ as the frequency in which feature $x$ gets the value $v$ in samples of class $k$:\n",
    "\n",
    "$$P(x=v \\mid y=k) = \\frac{n_{vk}}{n_k}$$\n",
    "\n",
    "where $n_{vk}$ is the number of samples in class $k$ in which $x$ gets the value $v$, and $n_k$ is the total number of samples in class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fd1be",
   "metadata": {},
   "source": [
    "### Example: Spam Filter\n",
    "We would like to build a spam filter based on a training set with 100 emails: 80 of them are non-spam (ham) and 20 of them are spam. The counts of the words in each type of email are given in the following tables:\n",
    "\n",
    "<div style=\"align:center\">\n",
    "    <img src=\"media/spam_ham.png\" width=600>\n",
    "</div>\n",
    "\n",
    "A new email has arrived with the text \"rich friend need money\". Is this a ham or a spam?\n",
    "\n",
    "Let’s first compute the class prior probabilities:\n",
    "\n",
    "$$P(\\text{Ham}) = \\frac{80}{100} = 0.8$$\n",
    "\n",
    "$$P(\\text{Spam}) = \\frac{20}{100} = 0.2$$\n",
    "\n",
    "Next, we estimate the likelihoods of the words in each type of email. The total number of words in the ham emails is: $80 + 5 + 5 + 20 + 50 + 40 = 200$, and the total number of words in the spam emails is: $20 + 20 + 30 + 25 + 25 = 120$. Therefore, the likelihoods of the words are:\n",
    "\n",
    "$$P(\\text{rich} \\mid \\text{Ham}) = \\frac{5}{200} = 0.025$$\n",
    "\n",
    "$$P(\\text{rich} \\mid \\text{Spam}) = \\frac{30}{120} = 0.25$$\n",
    "\n",
    "$$P(\\text{friend} \\mid \\text{Ham}) = \\frac{80}{200} = 0.4$$\n",
    "\n",
    "$$P(\\text{friend} \\mid \\text{Spam}) = \\frac{20}{120} = 0.167$$\n",
    "\n",
    "$$P(\\text{need} \\mid \\text{Ham}) = \\frac{50}{200} = 0.25$$\n",
    "\n",
    "$$P(\\text{need} \\mid \\text{Spam}) = \\frac{25}{120} = 0.208$$\n",
    "\n",
    "$$P(\\text{money} \\mid \\text{Ham}) = \\frac{20}{200} = 0.1$$\n",
    "\n",
    "$$P(\\text{money} \\mid \\text{Spam}) = \\frac{25}{120} = 0.208$$\n",
    "\n",
    "Consequently, the class posterior probabilities are:\n",
    "\n",
    "$$P(\\text{Ham} \\mid x) = P(\\text{Ham}) \\cdot P(\\text{rich} \\mid \\text{Ham}) \\cdot P(\\text{friend} \\mid \\text{Ham}) \\cdot P(\\text{need} \\mid \\text{Ham}) \\cdot P(\\text{money} \\mid \\text{Ham}) = 0.8 \\cdot 0.025 \\cdot 0.4 \\cdot 0.25 \\cdot 0.1\\alpha = 0.0002\\alpha$$\n",
    "\n",
    "$$P(\\text{Spam} \\mid x) = P(\\text{Spam}) \\cdot P(\\text{rich} \\mid \\text{Spam}) \\cdot P(\\text{friend} \\mid \\text{Spam}) \\cdot P(\\text{need} \\mid \\text{Spam}) \\cdot P(\\text{money} \\mid \\text{Spam}) = 0.2 \\cdot 0.25 \\cdot 0.167 \\cdot 0.208 \\cdot 0.208\\alpha = 0.00036\\alpha$$\n",
    "\n",
    "Therefore, our prediction is that the email is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c58e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e8e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Mining (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
